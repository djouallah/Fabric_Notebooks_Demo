{"cells":[{"cell_type":"code","source":["!pip install -q duckdb    --upgrade\n","!pip install    obstore   --upgrade\n","import sys\n","sys.exit(0)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"1f8cc864-f8a9-4f8d-a88c-9fa1d0826c5d","normalized_state":"finished","queued_time":"2025-09-26T14:41:40.561376Z","session_start_time":null,"execution_start_time":"2025-09-26T14:41:40.562258Z","execution_finish_time":"2025-09-26T14:41:47.9290982Z","parent_msg_id":"4f6111c0-81fa-4950-99b9-1ddc5dbb36b5"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: obstore in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (0.8.2)\nRequirement already satisfied: typing-extensions in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from obstore) (4.14.0)\nsys.exit called with value 0. The interpreter will be restarted.\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"609bd2a6-aabc-477d-ab56-0ddbc987825c"},{"cell_type":"code","source":["import duckdb\n","duckdb.sql(\" force install delta from core_nightly\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"1f8cc864-f8a9-4f8d-a88c-9fa1d0826c5d","normalized_state":"finished","queued_time":"2025-09-26T14:41:40.6347563Z","session_start_time":null,"execution_start_time":"2025-09-26T14:41:47.9303133Z","execution_finish_time":"2025-09-26T14:41:51.1710713Z","parent_msg_id":"ce4ebadf-33d8-4075-bbac-dc4ce7415485"}},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"c285bbfa-6c2f-4a8b-b972-05cfdfe18cc4"},{"cell_type":"code","source":["ws                    = 'largedata'\n","lh                    = 'simple'\n","schema                = 'test'\n","compaction_threshold  =  150\n","sql_folder            = 'https://github.com/djouallah/Fabric_Notebooks_Demo/raw/refs/heads/main/orchestration/new/'\n","Nbr_files_to_download =  60"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"1f8cc864-f8a9-4f8d-a88c-9fa1d0826c5d","normalized_state":"finished","queued_time":"2025-09-26T14:41:40.7064967Z","session_start_time":null,"execution_start_time":"2025-09-26T14:41:51.17232Z","execution_finish_time":"2025-09-26T14:41:51.6168863Z","parent_msg_id":"16bcd498-d021-433f-82eb-b56f1a6e9786"}},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"tags":["parameters"]},"id":"06d4db22-5b31-439b-b326-642378ccc6e6"},{"cell_type":"code","source":["import duckdb\n","import requests\n","import os\n","import sys\n","import importlib.util\n","from deltalake import DeltaTable, write_deltalake\n","from typing import List, Tuple, Union, Any, Optional, Callable, Dict\n","from string import Template\n","\n","\n","class Tasksql:\n","    \"\"\"\n","    Simplified Lakehouse task runner supporting:\n","      - ('script_name', (args,))          → runs script_name.py → script_name(*args)\n","      - ('table_name', 'mode', {params})  → runs table_name.sql with params, writes to Delta\n","    \"\"\"\n","\n","    def __init__(self, workspace: str, lakehouse_name: str, schema: str, sql_folder: str, compaction_threshold: int = 10):\n","        self.workspace = workspace\n","        self.lakehouse_name = lakehouse_name\n","        self.schema = schema\n","        self.sql_folder = sql_folder.strip()\n","        self.compaction_threshold = compaction_threshold\n","        self.table_base_url = f'abfss://{self.workspace}@onelake.dfs.fabric.microsoft.com/{self.lakehouse_name}.Lakehouse/Tables/'\n","        self.con = duckdb.connect()\n","        self._attach_lakehouse()\n","\n","    @classmethod\n","    def connect(cls, workspace: str, lakehouse_name: str, schema: str, sql_folder: str, compaction_threshold: int = 10):\n","        print(\"Connecting to Lakehouse...\")\n","        return cls(workspace, lakehouse_name, schema, sql_folder.strip(), compaction_threshold)\n","\n","    def _get_storage_token(self):\n","        return os.environ.get(\"AZURE_STORAGE_TOKEN\", \"PLACEHOLDER_TOKEN_TOKEN_NOT_AVAILABLE\")\n","\n","    def _create_onelake_secret(self):\n","        token = self._get_storage_token()\n","        if token != \"PLACEHOLDER_TOKEN_TOKEN_NOT_AVAILABLE\":\n","            self.con.sql(f\"CREATE OR REPLACE SECRET onelake (TYPE AZURE, PROVIDER ACCESS_TOKEN, ACCESS_TOKEN '{token}')\")\n","        else:\n","            print(\"Please login to Azure CLI\")\n","            self.con.sql(\"CREATE OR REPLACE PERSISTENT SECRET onelake (TYPE azure, PROVIDER credential_chain, CHAIN 'cli', ACCOUNT_NAME 'onelake')\")\n","\n","    def _attach_lakehouse(self):\n","        self._create_onelake_secret()\n","        try:\n","            list_tables_query = f\"\"\"\n","                SELECT DISTINCT(split_part(file, '_delta_log', 1)) as tables\n","                FROM glob (\"abfss://{self.workspace}@onelake.dfs.fabric.microsoft.com/{self.lakehouse_name}.Lakehouse/Tables/*/*/_delta_log/*.json\")\n","            \"\"\"\n","            list_tables_df = self.con.sql(list_tables_query).df()\n","            list_tables = list_tables_df['tables'].tolist() if not list_tables_df.empty else []\n","\n","            if not list_tables:\n","                print(f\"No Delta tables found in {self.lakehouse_name}.Lakehouse/Tables.\")\n","                return\n","\n","            print(f\"Found {len(list_tables)} Delta tables. Attaching as views...\")\n","\n","            for table_path in list_tables:\n","                parts = table_path.strip(\"/\").split(\"/\")\n","                if len(parts) >= 2:\n","                    potential_schema = parts[-2]\n","                    table = parts[-1]\n","                    if potential_schema == self.schema:\n","                        try:\n","                            self.con.sql(f\"\"\"\n","                                CREATE OR REPLACE VIEW {table}\n","                                AS SELECT * FROM delta_scan('{self.table_base_url}{self.schema}/{table}');\n","                            \"\"\")\n","                        except Exception as e:\n","                            print(f\"Error creating view for table {table}: {e}\")\n","            print(\"\\nAttached tables (views) in DuckDB:\")\n","            self.con.sql(\"SELECT name FROM (SHOW ALL TABLES) WHERE database='memory'\").show()\n","        except Exception as e:\n","            print(f\"Error attaching lakehouse: {e}\")\n","\n","\n","\n","    def _read_sql_file(self, table_name: str, params: Optional[Dict] = None) -> Optional[str]:\n","        is_url = self.sql_folder.startswith(\"http\")\n","        if is_url:\n","            url = f\"{self.sql_folder.rstrip('/')}/{table_name}.sql\".strip()\n","            try:\n","                resp = requests.get(url)\n","                resp.raise_for_status()\n","                content = resp.text\n","            except Exception as e:\n","                print(f\"Failed to fetch SQL from {url}: {e}\")\n","                return None\n","        else:\n","            path = os.path.join(self.sql_folder, f\"{table_name}.sql\")\n","            try:\n","                with open(path, 'r') as f:\n","                    content = f.read()\n","            except Exception as e:\n","                print(f\"Failed to read SQL file {path}: {e}\")\n","                return None\n","\n","        if not content.strip():\n","            print(f\"SQL file is empty: {table_name}.sql\")\n","            return None\n","\n","        # Merge system + user params\n","        full_params = {\n","            'ws': self.workspace,\n","            'lh': self.lakehouse_name,\n","            'schema': self.schema\n","        }\n","        if params:\n","            full_params.update(params)\n","\n","        # Use string.Template ($ws, ${run_date}) — safe with DuckDB {}\n","        try:\n","            template = Template(content)\n","            content = template.substitute(full_params)\n","        except KeyError as e:\n","            print(f\"Missing parameter in SQL file: ${e}\")\n","            return None\n","        except Exception as e:\n","            print(f\"Error during SQL template substitution: {e}\")\n","            return None\n","\n","        return content\n","\n","    def _load_py_function(self, name: str) -> Optional[Callable]:\n","        is_url = self.sql_folder.startswith(\"http\")\n","        try:\n","            if is_url:\n","                url = f\"{self.sql_folder.rstrip('/')}/{name}.py\".strip()\n","                resp = requests.get(url)\n","                resp.raise_for_status()\n","                code = resp.text\n","                namespace = {}\n","                exec(code, namespace)\n","                func = namespace.get(name)\n","                return func if callable(func) else None\n","            else:\n","                path = os.path.join(self.sql_folder, f\"{name}.py\")\n","                if not os.path.isfile(path):\n","                    print(f\"Python file not found: {path}\")\n","                    return None\n","                spec = importlib.util.spec_from_file_location(name, path)\n","                mod = importlib.util.module_from_spec(spec)\n","                spec.loader.exec_module(mod)\n","                func = getattr(mod, name, None)\n","                return func if callable(func) else None\n","        except Exception as e:\n","            print(f\"Error loading Python function '{name}': {e}\")\n","            return None\n","\n","    def _run_py_task(self, name: str, args: tuple) -> int:\n","        func = self._load_py_function(name)\n","        if not func:\n","            return 0\n","        try:\n","            print(f\"Running Python task: {name}{args}\")\n","            \n","            print(f\"✅ Python task '{name}' completed.\")\n","            return func(*args)\n","        except Exception as e:\n","            print(f\"❌ Error in Python task '{name}': {e}\")\n","            return 0\n","\n","    def _write_delta(self, sql: str, table: str, mode: str):\n","        path = f\"{self.table_base_url}{self.schema}/{table}\"\n","        df = self.con.sql(sql).arrow()\n","        write_deltalake(\n","            path, df, mode=mode,\n","            max_rows_per_file=8_000_000,\n","            max_rows_per_group=8_000_000,\n","            min_rows_per_group=8_000_000,\n","            engine='pyarrow'\n","        )\n","        # Refresh view\n","        self.con.sql(f\"CREATE OR REPLACE VIEW {table} AS SELECT * FROM delta_scan('{path}')\")\n","\n","    def _run_sql_task(self, table: str, mode: str, params: Optional[Dict] = None) -> int:\n","        allowed_modes = {'overwrite', 'append', 'ignore'}\n","        if mode not in allowed_modes:\n","            print(f\"Invalid mode '{mode}'. Use: {allowed_modes}\")\n","            return 0\n","\n","        sql = self._read_sql_file(table, params)\n","        if sql is None:\n","            return 0\n","\n","        path = f\"{self.table_base_url}{self.schema}/{table}\"\n","        try:\n","            if mode == 'overwrite':\n","                self.con.sql(f\"DROP VIEW IF EXISTS {table}\")\n","                self._write_delta(sql, table, mode)\n","                dt = DeltaTable(path)\n","                dt.vacuum(retention_hours=0, dry_run=False, enforce_retention_duration=False)\n","                dt.cleanup_metadata()\n","            elif mode == 'append':\n","                self._write_delta(sql, table, mode)\n","                dt = DeltaTable(path)\n","                if len(dt.files()) > self.compaction_threshold:\n","                    print(f\"Compacting {table} (files: {len(dt.files())})\")\n","                    dt.optimize.compact()\n","                    dt.vacuum(dry_run=False)\n","                    dt.cleanup_metadata()\n","            elif mode == 'ignore':\n","                try:\n","                    DeltaTable(path)\n","                except:\n","                    print(f\"{table} doesn't exist. Creating in overwrite mode.\")\n","                    self.con.sql(f\"DROP VIEW IF EXISTS {table}\")\n","                    self._write_delta(sql, table, 'overwrite')\n","                    dt = DeltaTable(path)\n","                    dt.vacuum(dry_run=False)\n","                    dt.cleanup_metadata()\n","            print(f\"✅ SQL task '{table}' ({mode}) completed.\")\n","            return 1\n","        except Exception as e:\n","            print(f\"❌ Error in SQL task '{table}': {e}\")\n","            return 0\n","\n","    def run_task_sequences(self, tasks: List[Union[Tuple[str, tuple], Tuple[str, str, Dict]]]) -> bool:\n","        \"\"\"\n","        Run tasks with simple syntax:\n","          - ('download', (url_list, path_list, depth))\n","          - ('staging_table', 'overwrite', {'run_date': '2024-06-01'})\n","        \"\"\"\n","        for i, task in enumerate(tasks):\n","            print(f\"\\n--- Running Task {i+1}: {task[0]} ---\")\n","            name = task[0]\n","\n","            if len(task) == 2:\n","                # Python task: ('name', (args,))\n","                args = task[1]\n","                if not isinstance(args, (tuple, list)):\n","                    args = (args,)\n","                result = self._run_py_task(name, tuple(args))\n","            elif len(task) == 3:\n","                # SQL write task: ('table', 'mode', {params})\n","                mode, params = task[1], task[2]\n","                if not isinstance(params, dict):\n","                    print(f\"❌ Expected dict as 3rd item in SQL task, got {type(params)}\")\n","                    return False\n","                result = self._run_sql_task(name, mode, params)\n","            else:\n","                print(f\"❌ Invalid task format: {task}\")\n","                return False\n","\n","            if result != 1:\n","                print(f\"❌ Task {i+1} failed. Stopping.\")\n","                return False\n","\n","        print(\"\\n✅ All tasks completed successfully.\")\n","        return True\n","\n","    def get_connection(self):\n","        return self.con\n","\n","    def close(self):\n","        if self.con:\n","            self.con.close()\n","            print(\"DuckDB connection closed.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"1f8cc864-f8a9-4f8d-a88c-9fa1d0826c5d","normalized_state":"finished","queued_time":"2025-09-26T14:41:40.7939768Z","session_start_time":null,"execution_start_time":"2025-09-26T14:41:51.6182268Z","execution_finish_time":"2025-09-26T14:41:53.852274Z","parent_msg_id":"f457b1fe-9893-4d94-a1d4-99465ab8f979"}},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"jupyter":{"source_hidden":true}},"id":"02e7908e-78cb-4a78-92fa-7e40d60a7185"},{"cell_type":"code","source":["%%time\n","con = Tasksql.connect( ws,lh,schema, sql_folder, compaction_threshold  )"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"1f8cc864-f8a9-4f8d-a88c-9fa1d0826c5d","normalized_state":"finished","queued_time":"2025-09-26T14:41:40.8783689Z","session_start_time":null,"execution_start_time":"2025-09-26T14:41:53.8534912Z","execution_finish_time":"2025-09-26T14:41:57.1493908Z","parent_msg_id":"60f187f5-72be-45e1-8f6f-1f48a82f01fd"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Connecting to Lakehouse...\nFound 8 Delta tables. Attaching as views...\n\nAttached tables (views) in DuckDB:\n┌─────────────┐\n│    name     │\n│   varchar   │\n├─────────────┤\n│ calendar    │\n│ duid        │\n│ mstdatetime │\n│ price       │\n│ price_today │\n│ scada       │\n│ scada_today │\n│ summary     │\n└─────────────┘\n\nCPU times: user 1.64 s, sys: 66.6 ms, total: 1.71 s\nWall time: 2.77 s\n"]}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"f02dab12-9c27-4be1-b4f4-4fca2d598dd0"},{"cell_type":"code","source":["%%time\n","con.run_task_sequences([\n","                        ('download_files', ([\"http://nemweb.com.au/Reports/Current/DispatchIS_Reports/\"],[\"Reports/Current/DispatchIS_Reports/\"], Nbr_files_to_download, ws,lh,6)),\n","                        ('price_today','append',{'ws': ws,'lh':lh}),\n","                        ('download_files', ([\"http://nemweb.com.au/Reports/Current/Dispatch_SCADA/\" ],[\"Reports/Current/Dispatch_SCADA/\"], Nbr_files_to_download, ws,lh,6)),\n","                        ('scada_today','append',{'ws': ws,'lh':lh}),\n","                        ('duid','ignore',{'ws': ws,'lh':lh}),\n","                        ('summary', 'append',{})\n","                        \n","                    ])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"1f8cc864-f8a9-4f8d-a88c-9fa1d0826c5d","normalized_state":"finished","queued_time":"2025-09-26T14:41:41.031722Z","session_start_time":null,"execution_start_time":"2025-09-26T14:41:57.1506207Z","execution_finish_time":"2025-09-26T14:42:18.8161843Z","parent_msg_id":"e07415f5-9168-4df1-952a-e2bac4885ea0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n--- Running Task 1: download_files ---\nRunning Python task: download_files(['http://nemweb.com.au/Reports/Current/DispatchIS_Reports/'], ['Reports/Current/DispatchIS_Reports/'], 60, 'largedata', 'simple', 6)\n✅ Python task 'download_files' completed.\nFailed to download PUBLIC_DISPATCHIS_202509261920_0000000482307340.zip: HTTP 403\nUpdated log Reports/Current/DispatchIS_Reports/download_log.csv with 59 new entries\nhttp://nemweb.com.au/Reports/Current/DispatchIS_Reports/ - 59 files extracted and uploaded\n\n--- Running Task 2: price_today ---\n✅ SQL task 'price_today' (append) completed.\n\n--- Running Task 3: download_files ---\nRunning Python task: download_files(['http://nemweb.com.au/Reports/Current/Dispatch_SCADA/'], ['Reports/Current/Dispatch_SCADA/'], 60, 'largedata', 'simple', 6)\n✅ Python task 'download_files' completed.\nUpdated log Reports/Current/Dispatch_SCADA/download_log.csv with 60 new entries\nhttp://nemweb.com.au/Reports/Current/Dispatch_SCADA/ - 60 files extracted and uploaded\n\n--- Running Task 4: scada_today ---\n✅ SQL task 'scada_today' (append) completed.\n\n--- Running Task 5: duid ---\n✅ SQL task 'duid' (ignore) completed.\n\n--- Running Task 6: summary ---\n✅ SQL task 'summary' (append) completed.\n\n✅ All tasks completed successfully.\nCPU times: user 3.85 s, sys: 196 ms, total: 4.05 s\nWall time: 21.1 s\n"]},{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"True"},"metadata":{}}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"16b93f49-6390-4afd-a62d-96f0646609e2"},{"cell_type":"code","source":["%%time\n","con.run_task_sequences([\n","                        ('download_files', ([\"https://nemweb.com.au/Reports/Current/Daily_Reports/\"],[\"Reports/Current/Daily_Reports/\"],Nbr_files_to_download,ws,lh,6)),\n","                        ('price','append',{'ws': ws,'lh':lh}),\n","                        ('scada','append',{'ws': ws,'lh':lh}),\n","                        ('download_excel',(\"raw/\", ws,lh)),\n","                        ('duid','overwrite',{'ws': ws,'lh':lh}),\n","                        ('calendar','ignore',{}),\n","                        ('mstdatetime','ignore',{}),\n","                        ('summary','overwrite',{})\n","                     ])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"1f8cc864-f8a9-4f8d-a88c-9fa1d0826c5d","normalized_state":"finished","queued_time":"2025-09-26T14:41:41.1384898Z","session_start_time":null,"execution_start_time":"2025-09-26T14:42:18.8173998Z","execution_finish_time":"2025-09-26T14:42:20.2837844Z","parent_msg_id":"3393759c-e4e5-4250-992c-7a38c987d813"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n--- Running Task 1: download_files ---\nRunning Python task: download_files(['https://nemweb.com.au/Reports/Current/Daily_Reports/'], ['Reports/Current/Daily_Reports/'], 60, 'largedata', 'simple', 6)\n✅ Python task 'download_files' completed.\nhttps://nemweb.com.au/Reports/Current/Daily_Reports/ - 0 files extracted (all 60 files already downloaded)\n❌ Task 1 failed. Stopping.\nCPU times: user 63.2 ms, sys: 1.79 ms, total: 65 ms\nWall time: 972 ms\n"]},{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"False"},"metadata":{}}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"3348a4c5-068e-4300-a046-dd1f1da97b28"}],"metadata":{"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"name":"jupyter","language":"Jupyter","display_name":"Jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"720000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}