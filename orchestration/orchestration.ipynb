{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bcc8b01-2ba1-48d3-b94d-58f89e38b54a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "sql_folder           = 'https://github.com/djouallah/Fabric_Notebooks_Demo/raw/refs/heads/main/orchestration/'\n",
    "workspace            = 'processing'\n",
    "LH                   = 'test'\n",
    "schema               = 'new'\n",
    "compaction_threshold = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79497de8-fada-4c44-bbeb-942b98ce5f68",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**<mark>Import Packages</mark>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1856589-8657-446e-9b58-f0f4847192d5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4.91 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from   deltalake     import  write_deltalake,DeltaTable\n",
    "from   datetime      import datetime, time\n",
    "from   zoneinfo      import ZoneInfo\n",
    "import duckdb\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bde8a-91d3-48b3-9328-532463b9d67f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**<mark>Authentication</mark>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f371dc0-6413-4c91-a93f-fc2b655098a7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from   azure.identity import DefaultAzureCredential\n",
    "    import os\n",
    "    os.environ['azure_storage_token'.upper()] = DefaultAzureCredential().get_token(\"https://storage.azure.com/.default\").token\n",
    "except: \n",
    "    os.environ['azure_storage_token'] = notebookutils.credentials.getToken('storage') \n",
    "    print(\"you are in Fabric notebook\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2161785-c8ec-41d1-ac36-e01332dd43aa",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**<mark>Function to Run SQL Queries</mark>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68aaf852-93e3-4ce5-bf77-67c7696fff0f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.23 s\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "table_base_url = f'abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{LH}.Lakehouse/Tables/'\n",
    "con = duckdb.connect()   \n",
    "con.sql(f\"\"\" CREATE or replace SECRET onelake ( TYPE AZURE, PROVIDER ACCESS_TOKEN, ACCESS_TOKEN '{os.getenv('azure_storage_token')}')   \"\"\")  \n",
    "def run_sql(list_files):\n",
    "    successful_runs = 0\n",
    "    for x in list_files:\n",
    "        try:\n",
    "            file_path = f'{sql_folder}/{x}.sql'\n",
    "            is_github = sql_folder.startswith(\"http\")\n",
    "            if is_github:\n",
    "                sql_content = requests.get(file_path).text  \n",
    "            else: \n",
    "                with open(file_path, 'r') as file:\n",
    "                    sql_content = file.read()\n",
    "\n",
    "            match = re.search(r\"-- materialized:\\s*\\((.*?)\\)\", sql_content)\n",
    "\n",
    "            if match:\n",
    "                materialized_content = match.group(1)\n",
    "                parts = [part.strip() for part in materialized_content.split(',')]\n",
    "\n",
    "                if len(parts) >= 2:\n",
    "                    table_name = parts[0]\n",
    "                    mode       = parts[1]\n",
    "                    try:\n",
    "                        write_delta(sql_content,table_name,schema, mode)\n",
    "                        con.sql(f\"\"\" create or replace view {table_name} as select * from delta_scan('{table_base_url}{schema}/{table_name}') \"\"\")\n",
    "                        successful_runs = successful_runs +1\n",
    "                    except:\n",
    "                        print(f\"Data not updated in {x}\")\n",
    "                else:\n",
    "                    try:\n",
    "                            con.sql(sql_content)\n",
    "                            print(f\"Data updated in {x}\")\n",
    "                            successful_runs = successful_runs +1\n",
    "                    except:\n",
    "                            print(f\"Data not updated in {x}\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")      \n",
    "    return successful_runs\n",
    "def write_delta(sql_content,tbl,schema, mode):\n",
    "            tbl_path = table_base_url + schema + '/' +tbl\n",
    "            RG=8_000_000\n",
    "            df = con.sql(sql_content).arrow()\n",
    "            r = df.num_rows\n",
    "            write_deltalake(\n",
    "            tbl_path,\n",
    "            df,\n",
    "            mode=mode,\n",
    "            max_rows_per_file = RG ,\n",
    "            max_rows_per_group = RG,\n",
    "            min_rows_per_group = RG,\n",
    "            engine ='pyarrow'\n",
    "            )\n",
    "            print(f'table {tbl} has {r} new rows, Delta mode {mode}')\n",
    "            if mode =='append':\n",
    "                dt = DeltaTable(tbl_path)\n",
    "                if len(dt.files()) > compaction_threshold:\n",
    "                    dt.optimize.compact()\n",
    "                    dt.vacuum(retention_hours=7, dry_run=False, enforce_retention_duration=False)\n",
    "                    dt.cleanup_metadata()\n",
    "                    print(\"compaction completed\")\n",
    "            elif mode =='overwrite':\n",
    "                dt = DeltaTable(tbl_path)\n",
    "                dt.vacuum(retention_hours=7, dry_run=False, enforce_retention_duration=False)\n",
    "                dt.cleanup_metadata()\n",
    "                print(\"vaccum completed\")\n",
    "            return 'done'\n",
    "def duckdb_attach_lakehouse():   \n",
    "    list_tables = con.sql(f\"\"\" SELECT  distinct(split_part(file, '_delta_log', 1)) as tables\n",
    "                    FROM glob (\"abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{LH}.Lakehouse/Tables/*/*/_delta_log/*.json\")\n",
    "                     \"\"\").df()['tables'].tolist()\n",
    "    for table_path in list_tables:\n",
    "            parts = table_path.strip(\"/\").split(\"/\")\n",
    "            table = parts[-1]\n",
    "            try:\n",
    "                con.sql(f\"\"\"CREATE OR REPLACE view {table}\n",
    "                 AS select * FROM delta_scan('abfss://{workspace}@onelake.dfs.fabric.microsoft.com/{LH}.Lakehouse/Tables/{schema}/{table}');\"\"\")\n",
    "            except:\n",
    "                pass\n",
    "    con.sql(\"\"\" select name ,column_names from (show all tables) where database='memory' \"\"\").show(max_width=120)\n",
    "    return 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63004775-1037-4fe5-823f-853397e268a8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌─────────────┬────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│    name     │                                              column_names                                              │\n",
      "│   varchar   │                                               varchar[]                                                │\n",
      "├─────────────┼────────────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ calendar    │ [date, year, month]                                                                                    │\n",
      "│ duid        │ [DUID, Region, FuelSourceDescriptor, Participant, State]                                               │\n",
      "│ price       │ [UNIT, REGIONID, VERSION, RUNNO, INTERVENTION, RRP, EEP, ROP, APCFLAG, MARKETSUSPENDEDFLAG, TOTALDEM…  │\n",
      "│ price_today │ [REGIONID, RUNNO, DISPATCHINTERVAL, INTERVENTION, RRP, EEP, ROP, APCFLAG, MARKETSUSPENDEDFLAG, RAISE…  │\n",
      "│ scada       │ [UNIT, DUID, VERSION, RUNNO, INTERVENTION, DISPATCHMODE, AGCSTATUS, INITIALMW, TOTALCLEARED, RAMPDOW…  │\n",
      "│ scada_today │ [DUID, INITIALMW, INTERVENTION, SETTLEMENTDATE, date, file, PRIORITY, YEAR]                            │\n",
      "│ summary     │ [date, time, cutoff, DUID, mw, price]                                                                  │\n",
      "└─────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "CPU times: total: 4.75 s\n",
      "Wall time: 51.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "duckdb_attach_lakehouse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e191c0b-2090-4208-8527-4525120d210c",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table scada and price exists, will check in the morning if new files arrived\n",
      "incremental updates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca09a5af48e948c8886ab86d304dcc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not updated in scada_today\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e410ce250248c7988637a7f6344a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not updated in price_today\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e938283c8fcc4bda822bf52abebc92e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "now_brisbane = datetime.now(ZoneInfo(\"Australia/Brisbane\")).time()\n",
    "start = time(4, 0)   \n",
    "end   = time(5, 30) \n",
    "t = 0  \n",
    "table_exists = con.sql(\"\"\" select count(*) from (show all tables) where database='memory' and name in ('scada','price','duid') \"\"\").fetchone()[0]\n",
    "if table_exists == 3: \n",
    "    print(\"table scada and price exists, will check in the morning if new files arrived\")\n",
    "    if start <= now_brisbane <= end:\n",
    "        print(\"check if nightly load has arrived\")\n",
    "        t = run_sql(['price','scada'])\n",
    "        print(f\"\"\" Nbr of tables changed {t}:  if both tables change do backup\"\"\")\n",
    "else:\n",
    "    print(\"table does not exists, load everything\")\n",
    "    run_sql(['price','scada'])\n",
    "    t=2\n",
    "if t == 2 :\n",
    "    run_sql(['calendar','duid','mstdatetime','summary_backfill'])\n",
    "else :\n",
    "    print(\"incremental updates\")\n",
    "    run_sql(['scada_today','price_today','summary_incremental'])"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
