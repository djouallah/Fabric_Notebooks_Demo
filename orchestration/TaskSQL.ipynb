{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7908e-78cb-4a78-92fa-7e40d60a7185",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import requests\n",
    "from deltalake import DeltaTable, write_deltalake\n",
    "from typing import List, Tuple, Union, Any\n",
    "from notebookutils.common import configs\n",
    "configs.tokenCacheEnabled = False\n",
    "\n",
    "class Tasksql:\n",
    "    \"\"\"\n",
    "    A class to manage interactions with a Lakehouse using DuckDB and Delta Lake,\n",
    "    using a connect() class method for initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, workspace: str, lakehouse_name: str, schema: str, sql_folder: str, compaction_threshold: int = 10):\n",
    "        \"\"\"\n",
    "        Initializes the Tasksql connector instance.\n",
    "        This constructor is intended to be called by the class method `connect`.\n",
    "\n",
    "        Args:\n",
    "            workspace (str): The name of the workspace.\n",
    "            lakehouse_name (str): The name of the Lakehouse (LH).\n",
    "            schema (str): The schema within the Lakehouse (e.g., 'dbo').\n",
    "            sql_folder (str): The path or URL to the folder containing SQL files.\n",
    "            compaction_threshold (int): The number of files in a Delta table\n",
    "                                        before compaction is triggered (default is 10).\n",
    "        \"\"\"\n",
    "        self.workspace = workspace\n",
    "        self.lakehouse_name = lakehouse_name\n",
    "        self.schema = schema\n",
    "        self.sql_folder = sql_folder\n",
    "        self.compaction_threshold = compaction_threshold\n",
    "        self.table_base_url = f'abfss://{self.workspace}@onelake.dfs.fabric.microsoft.com/{self.lakehouse_name}.Lakehouse/Tables/'\n",
    "        self.con = duckdb.connect()\n",
    "        self._attach_lakehouse() # Automatically attach lakehouse on initialization\n",
    "\n",
    "    @classmethod\n",
    "    def connect(cls, workspace: str, lakehouse_name: str, schema: str, sql_folder: str, compaction_threshold: int = 10):\n",
    "        \"\"\"\n",
    "        Class method to create and initialize a Tasksql connector instance.\n",
    "\n",
    "        Args:\n",
    "            workspace (str): The name of the workspace.\n",
    "            lakehouse_name (str): The name of the Lakehouse (LH).\n",
    "            schema (str): The schema within the Lakehouse (e.g., 'dbo').\n",
    "            sql_folder (str): The path or URL to the folder containing SQL files.\n",
    "            compaction_threshold (int): The number of files in a Delta table\n",
    "                                        before compaction is triggered (default is 10).\n",
    "\n",
    "        Returns:\n",
    "            Tasksql: An initialized instance of the Tasksql class.\n",
    "        \"\"\"\n",
    "        print(\"Connecting to Lakehouse...\")\n",
    "        # Create an instance of the class using the provided arguments\n",
    "        return cls(workspace, lakehouse_name, schema, sql_folder, compaction_threshold)\n",
    "\n",
    "\n",
    "    def _get_storage_token(self):\n",
    "        \"\"\"\n",
    "        Retrieves the storage access token using notebookutils.\n",
    "        Assumes notebookutils is available in the execution environment.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Replace with the actual way to get the token in your environment\n",
    "            # This is a placeholder based on your original code\n",
    "            # If not in Fabric/Synapse notebooks, you'll need another method\n",
    "            return notebookutils.credentials.getToken('storage')\n",
    "        except NameError:\n",
    "            # print(\"Warning: notebookutils not found. Cannot get storage token.\") # Reduced verbosity\n",
    "            # print(\"Please ensure this code is run in an environment where notebookutils is available.\") # Reduced verbosity\n",
    "            return \"PLACEHOLDER_TOKEN_TOKEN_NOT_AVAILABLE\"\n",
    "\n",
    "\n",
    "    def _create_onelake_secret(self):\n",
    "        \"\"\"\n",
    "        Creates or replaces the OneLake secret in DuckDB.\n",
    "        \"\"\"\n",
    "        token = self._get_storage_token()\n",
    "        if token != \"PLACEHOLDER_TOKEN_TOKEN_NOT_AVAILABLE\":\n",
    "             self.con.sql(f\"\"\"\n",
    "                CREATE or replace SECRET onelake (\n",
    "                    TYPE AZURE,\n",
    "                    PROVIDER ACCESS_TOKEN,\n",
    "                    ACCESS_TOKEN '{token}'\n",
    "                )\n",
    "            \"\"\")\n",
    "             # print(\"OneLake secret created/replaced.\") # Reduced verbosity\n",
    "        else:\n",
    "            print(\"Skipping OneLake secret creation due to missing storage token.\")\n",
    "\n",
    "\n",
    "    def _attach_lakehouse(self):\n",
    "        \"\"\"\n",
    "        Attaches the Lakehouse tables as views in the DuckDB connection.\n",
    "        \"\"\"\n",
    "        self._create_onelake_secret()\n",
    "        try:\n",
    "            # Query to find delta table paths in the Lakehouse\n",
    "            list_tables_query = f\"\"\"\n",
    "                SELECT DISTINCT(split_part(file, '_delta_log', 1)) as tables\n",
    "                FROM glob (\"abfss://{self.workspace}@onelake.dfs.fabric.microsoft.com/{self.lakehouse_name}.Lakehouse/Tables/*/*/_delta_log/*.json\")\n",
    "            \"\"\"\n",
    "            list_tables_df = self.con.sql(list_tables_query).df()\n",
    "            list_tables = list_tables_df['tables'].tolist() if not list_tables_df.empty else []\n",
    "\n",
    "            if not list_tables:\n",
    "                print(f\"No Delta tables found in {self.lakehouse_name}.Lakehouse/Tables.\")\n",
    "                return 'No tables found'\n",
    "\n",
    "            print(f\"Found {len(list_tables)} Delta tables. Attaching as views...\")\n",
    "\n",
    "            for table_path in list_tables:\n",
    "                parts = table_path.strip(\"/\").split(\"/\")\n",
    "                # Assuming path is like .../Lakehouse/Tables/schema/table_name\n",
    "                if len(parts) >= 2:\n",
    "                     # Extract schema and table name from the path\n",
    "                    potential_schema = parts[-2]\n",
    "                    table = parts[-1]\n",
    "\n",
    "                    # Only create view if the schema matches the configured schema for this instance\n",
    "                    if potential_schema == self.schema:\n",
    "                        try:\n",
    "                            view_query = f\"\"\"\n",
    "                                CREATE OR REPLACE view {table}\n",
    "                                AS select * FROM delta_scan('{self.table_base_url}{self.schema}/{table}');\n",
    "                            \"\"\"\n",
    "                            self.con.sql(view_query)\n",
    "                            # print(f\"View created/replaced for table: {table}\") # Reduced verbosity\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error creating/replacing view for table {table}: {e}\")\n",
    "                            # Continue to the next table even if one fails\n",
    "                            pass\n",
    "                    else:\n",
    "                        # print(f\"Skipping table '{table}' in schema '{potential_schema}' as it doesn't match the configured schema '{self.schema}'.\") # Reduced verbosity\n",
    "                        pass\n",
    "                else:\n",
    "                    print(f\"Skipping invalid table path format: {table_path}\")\n",
    "\n",
    "\n",
    "            print(\"\\nAttached tables (views) in DuckDB:\")\n",
    "            # Show the created views in the 'memory' database (DuckDB's default)\n",
    "            self.con.sql(\"\"\"\n",
    "                SELECT name, column_names\n",
    "                FROM (show all tables)\n",
    "                WHERE database='memory'\n",
    "            \"\"\").show(max_width=130)\n",
    "\n",
    "            return 'success'\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during lakehouse attachment: {e}\")\n",
    "            return 'error'\n",
    "\n",
    "\n",
    "    def _read_sql_file(self, table_name: str) -> str | None:\n",
    "        \"\"\"\n",
    "        Reads the SQL content from a local file or a GitHub URL.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): The name of the table, used to construct the file name.\n",
    "\n",
    "        Returns:\n",
    "            str | None: The content of the SQL file, or None if an error occurs.\n",
    "        \"\"\"\n",
    "        file_path = f'{self.sql_folder}/{table_name}.sql'\n",
    "        is_github = self.sql_folder.startswith(\"http\")\n",
    "        sql_content = None\n",
    "\n",
    "        try:\n",
    "            if is_github:\n",
    "                # print(f\"Fetching SQL from URL: {file_path}\") # Reduced verbosity\n",
    "                response = requests.get(file_path)\n",
    "                response.raise_for_status() # Raise an exception for bad status codes\n",
    "                sql_content = response.text\n",
    "            else:\n",
    "                # print(f\"Reading SQL from local file: {file_path}\") # Reduced verbosity\n",
    "                with open(file_path, 'r') as file:\n",
    "                    sql_content = file.read()\n",
    "\n",
    "            if not sql_content or not sql_content.strip():\n",
    "                print(f\"Error: SQL content is empty or could not be read from '{file_path}'.\")\n",
    "                return None\n",
    "\n",
    "            return sql_content\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: The file '{file_path}' was not found.\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching SQL from URL {file_path}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while reading SQL file: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def _write_delta(self, sql_content: str, table_name: str, mode: str):\n",
    "        \"\"\"\n",
    "        Writes data to a Delta table based on SQL query results.\n",
    "\n",
    "        Args:\n",
    "            sql_content (str): The SQL query to get the data.\n",
    "            table_name (str): The name of the target Delta table.\n",
    "            mode (str): The write mode ('overwrite', 'append', 'ignore').\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the write operation fails.\n",
    "        \"\"\"\n",
    "        tbl_path = f\"{self.table_base_url}{self.schema}/{table_name}\"\n",
    "        # Recommended Row Group size for Delta Lake\n",
    "        RG = 8_000_000\n",
    "\n",
    "        try:\n",
    "            # Execute the SQL query and get results as a PyArrow RecordBatch\n",
    "            df = self.con.sql(sql_content).record_batch()\n",
    "\n",
    "            # Write the RecordBatch to the Delta Lake table\n",
    "            write_deltalake(\n",
    "                tbl_path,\n",
    "                df,\n",
    "                mode=mode,\n",
    "                max_rows_per_file=RG,\n",
    "                max_rows_per_group=RG,\n",
    "                min_rows_per_group=RG,\n",
    "                engine='pyarrow' # Specify pyarrow engine\n",
    "            )\n",
    "            # print(f'Table {table_name} updated, Delta mode {mode}') # Reduced verbosity\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing to delta table {table_name} in mode {mode}: {e}\")\n",
    "            raise # Re-raise the exception after printing\n",
    "\n",
    "    def _run_sql_internal(self, table_name: str | None = None, mode: str | None = None, sql_statement: str | None = None) -> int:\n",
    "        \"\"\"\n",
    "        Internal helper method to execute SQL and optionally write to Delta Lake.\n",
    "\n",
    "        Args:\n",
    "            table_name (str, optional): The name of the table. Required if sql_statement is None\n",
    "                                        and mode is not None. Used to construct the SQL file path\n",
    "                                        or the target Delta table path.\n",
    "            mode (str, optional): The mode of operation for Delta Lake writes.\n",
    "                                 Allowed values are None, 'overwrite', 'append', 'ignore'.\n",
    "                                 Defaults to None (direct SQL execution).\n",
    "            sql_statement (str, optional): A direct SQL statement string to execute.\n",
    "                                           If provided, the method will use this string\n",
    "                                           instead of reading from a file.\n",
    "\n",
    "        Returns:\n",
    "            int: 1 if the SQL run was successful, 0 otherwise.\n",
    "        \"\"\"\n",
    "        successful_runs = 0\n",
    "\n",
    "        sql_content = None\n",
    "\n",
    "        if sql_statement is not None:\n",
    "            sql_content = sql_statement\n",
    "            # print(\"Using provided SQL statement.\") # Reduced verbosity\n",
    "            if mode is not None and (table_name is None or not table_name.strip()):\n",
    "                 print(\"Error: table_name is required when using sql_statement with a write mode ('overwrite', 'append', 'ignore').\")\n",
    "                 return 0\n",
    "\n",
    "        elif table_name is not None and table_name.strip():\n",
    "            sql_content = self._read_sql_file(table_name)\n",
    "            if sql_content is None:\n",
    "                 return 0\n",
    "            # print(f\"Using SQL from file for table: {table_name}\") # Reduced verbosity\n",
    "\n",
    "        else:\n",
    "            print(\"Error: Either 'table_name' (for file reading) or 'sql_statement' must be provided.\")\n",
    "            return 0\n",
    "\n",
    "        if not sql_content or not sql_content.strip():\n",
    "             print(\"Error: SQL content is empty after attempting to load.\")\n",
    "             return 0\n",
    "\n",
    "        self._create_onelake_secret() # Ensure secret is available\n",
    "\n",
    "        try:\n",
    "            if mode in ['overwrite', 'append', 'ignore']:\n",
    "                if table_name is None or not table_name.strip():\n",
    "                     print(f\"Error: table_name is required for Delta write mode '{mode}'.\")\n",
    "                     return 0\n",
    "\n",
    "                # print(f\"Running in mode: {mode} for table: {table_name}\") # Reduced verbosity\n",
    "                tbl_path = f\"{self.table_base_url}{self.schema}/{table_name}\"\n",
    "\n",
    "                if mode == 'overwrite':\n",
    "                    try:\n",
    "                        self.con.sql(f'drop VIEW if exists {table_name}')\n",
    "                        self._write_delta(sql_content, table_name, mode)\n",
    "                        self.con.sql(f\"\"\"\n",
    "                            CREATE OR REPLACE VIEW {table_name} AS\n",
    "                            SELECT * FROM delta_scan('{tbl_path}')\n",
    "                        \"\"\")\n",
    "                        successful_runs += 1\n",
    "                        dt = DeltaTable(tbl_path)\n",
    "                        dt.vacuum(retention_hours=0, dry_run=False, enforce_retention_duration=False)\n",
    "                        dt.cleanup_metadata()\n",
    "                        # print(\"Vacuum and metadata cleanup completed.\") # Reduced verbosity\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error updating data or creating view in overwrite mode for {table_name}: {e}\")\n",
    "\n",
    "                elif mode == 'append':\n",
    "                    try:\n",
    "                        self._write_delta(sql_content, table_name, mode)\n",
    "                        self.con.sql(f\"\"\"\n",
    "                            CREATE OR REPLACE VIEW {table_name} AS\n",
    "                            SELECT * FROM delta_scan('{tbl_path}')\n",
    "                        \"\"\")\n",
    "                        successful_runs += 1\n",
    "                        dt = DeltaTable(tbl_path)\n",
    "                        if len(dt.files()) > self.compaction_threshold:\n",
    "                            print(f\"Number of files ({len(dt.files())}) exceeds compaction threshold ({self.compaction_threshold}). Compacting...\")\n",
    "                            dt.optimize.compact()\n",
    "                            dt.vacuum(retention_hours=7, dry_run=False, enforce_retention_duration=False)\n",
    "                            dt.cleanup_metadata()\n",
    "                            print(\"Compaction, vacuum, and metadata cleanup completed.\")\n",
    "                        else:\n",
    "                             # print(f\"Number of files ({len(dt.files())}) is below compaction threshold ({self.compaction_threshold}). Skipping compaction.\") # Reduced verbosity\n",
    "                             pass\n",
    "\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error updating data or creating view in append mode for {table_name}: {e}\")\n",
    "\n",
    "                elif mode == 'ignore':\n",
    "                    try:\n",
    "                        dt = DeltaTable(tbl_path)\n",
    "                        # print(f\"{table_name} exists already. Ignoring write operation.\") # Reduced verbosity\n",
    "                        successful_runs += 1\n",
    "                    except Exception:\n",
    "                        print(f\"{table_name} does not exist. Writing in overwrite mode.\")\n",
    "                        try:\n",
    "                            self._write_delta(sql_content, table_name, 'overwrite')\n",
    "                            self.con.sql(f\"\"\"\n",
    "                                CREATE OR REPLACE VIEW {table_name} AS\n",
    "                                SELECT * FROM delta_scan('{tbl_path}')\n",
    "                            \"\"\")\n",
    "                            successful_runs += 1\n",
    "                            dt = DeltaTable(tbl_path)\n",
    "                            dt.vacuum(retention_hours=7, dry_run=False, enforce_retention_duration=False)\n",
    "                            dt.cleanup_metadata()\n",
    "                            # print(f\"{table_name} created and vacuumed.\") # Reduced verbosity\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error creating table in ignore mode (using overwrite) for {table_name}: {e}\")\n",
    "\n",
    "            elif mode is None:\n",
    "                # print(f\"Running direct SQL execution.\") # Reduced verbosity\n",
    "                try:\n",
    "                    self.con.sql(sql_content).show()\n",
    "                    # print(\"SQL executed successfully.\") # Reduced verbosity\n",
    "                    successful_runs += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error executing SQL: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during SQL execution: {e}\")\n",
    "\n",
    "        return successful_runs\n",
    "\n",
    "    def write(self, table_name: str, mode: str) -> int:\n",
    "        \"\"\"\n",
    "        Writes data to a Delta Lake table based on SQL read from a file.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): The name of the table. Used to construct the SQL file path\n",
    "                                and the target Delta table path.\n",
    "            mode (str): The write mode of operation for Delta Lake writes.\n",
    "                                 Allowed values are 'overwrite', 'append', 'ignore'.\n",
    "\n",
    "        Returns:\n",
    "            int: 1 if the write operation was successful, 0 otherwise.\n",
    "        \"\"\"\n",
    "        allowed_write_modes = ['overwrite', 'append', 'ignore']\n",
    "        if mode not in allowed_write_modes:\n",
    "            print(f\"Error: Invalid write mode '{mode}'. Allowed modes are: {', '.join(allowed_write_modes)}\")\n",
    "            return 0\n",
    "\n",
    "        if not isinstance(table_name, str) or not table_name.strip():\n",
    "             print(\"Error: 'table_name' must be a non-empty string for write operations.\")\n",
    "             return 0\n",
    "\n",
    "        # print(f\"Attempting to write to table: {table_name} in mode: {mode}\") # Reduced verbosity\n",
    "        # Call the internal helper method with mode and table_name\n",
    "        return self._run_sql_internal(table_name=table_name, mode=mode, sql_statement=None)\n",
    "\n",
    "    def sql(self, sql_statement: str | None = None, table_name: str | None = None) -> int:\n",
    "        \"\"\"\n",
    "        Executes a SQL query. Can accept a direct SQL statement or read from a file.\n",
    "        This method does NOT perform any Delta Lake write operations.\n",
    "\n",
    "        Args:\n",
    "            sql_statement (str, optional): A direct SQL statement string to execute.\n",
    "                                           If provided, the method will use this string.\n",
    "            table_name (str, optional): The name of the table. If sql_statement is None,\n",
    "                                        this is used to read SQL from a file.\n",
    "\n",
    "        Returns:\n",
    "            int: 1 if the SQL execution was successful, 0 otherwise.\n",
    "        \"\"\"\n",
    "        if sql_statement is None and (table_name is None or not isinstance(table_name, str) or not table_name.strip()):\n",
    "             print(\"Error: Either 'sql_statement' or 'table_name' must be provided for SQL execution.\")\n",
    "             return 0\n",
    "\n",
    "        # print(f\"Attempting to execute SQL (from statement: {sql_statement is not None}, from file: {table_name if table_name else 'None'})\") # Reduced verbosity\n",
    "        # Call the internal helper method with mode=None\n",
    "        return self._run_sql_internal(table_name=table_name, mode=None, sql_statement=sql_statement)\n",
    "\n",
    "\n",
    "    def run_sql_sequence(self, tasks_list: List[Union[Tuple[str, str], Tuple[str, Any, Any, Any]]]) -> bool:\n",
    "        \"\"\"\n",
    "        Runs a sequence of SQL tasks. Stops if any task fails.\n",
    "        Each task can be a tuple in one of two formats:\n",
    "          - (table_name, mode): For Delta Lake write operations (infers 'write' task type).\n",
    "                                table_name and mode ('overwrite', 'append', 'ignore') are required.\n",
    "          - (task_type, table_name, mode, sql_statement): Explicit task definition.\n",
    "                                task_type can be 'write' or 'sql'.\n",
    "                                Parameters depend on task_type as described in previous version.\n",
    "\n",
    "        Args:\n",
    "            tasks_list (List[Union[Tuple[str, str], Tuple[str, Any, Any, Any]]]): A list of task tuples.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if all tasks completed successfully, False otherwise.\n",
    "        \"\"\"\n",
    "        previous_task_successful = True\n",
    "\n",
    "        for i, task in enumerate(tasks_list):\n",
    "            if previous_task_successful:\n",
    "                result = 0\n",
    "                task_info = None # For logging\n",
    "\n",
    "                if isinstance(task, (list, tuple)):\n",
    "                    if len(task) == 2:\n",
    "                        # Assume it's a write task: (table_name, mode)\n",
    "                        table_name = task[0] if len(task) > 0 else None\n",
    "                        mode = task[1] if len(task) > 1 else None\n",
    "                        task_type = 'write' # Inferred\n",
    "                        task_info = f\"Write Task: Table: {table_name}, Mode: {mode}\"\n",
    "\n",
    "                        # Validate parameters for inferred 'write' task\n",
    "                        if not isinstance(table_name, str) or not table_name.strip() or mode not in ['overwrite', 'append', 'ignore']:\n",
    "                            print(f\"Error: Invalid parameters for inferred 'write' task at index {i}: {task}. Expected (table_name, mode) with valid table_name and mode. Skipping.\")\n",
    "                            previous_task_successful = False\n",
    "                            continue\n",
    "\n",
    "                        print(f\"\\n--- Running Task {i+1}: {task_info} ---\")\n",
    "                        result = self.write(table_name=table_name, mode=mode)\n",
    "\n",
    "                    elif len(task) == 4:\n",
    "                        # Assume it's an explicit task: (task_type, table_name, mode, sql_statement)\n",
    "                        task_type = task[0] if len(task) > 0 else None\n",
    "                        table_name = task[1] if len(task) > 1 else None\n",
    "                        mode = task[2] if len(task) > 2 else None\n",
    "                        sql_statement = task[3] if len(task) > 3 else None\n",
    "                        task_info = f\"Explicit Task: Type: {task_type}, Table: {table_name}, Mode: {mode}, Using Statement: {sql_statement is not None}\"\n",
    "\n",
    "\n",
    "                        if task_type not in ['write', 'sql']:\n",
    "                            print(f\"Error: Invalid task type at index {i}: {task_type}. Expected 'write' or 'sql'. Skipping this task.\")\n",
    "                            previous_task_successful = False\n",
    "                            continue\n",
    "\n",
    "                        print(f\"\\n--- Running Task {i+1}: {task_info} ---\")\n",
    "\n",
    "                        if task_type == 'write':\n",
    "                            # Validate parameters for explicit 'write' task\n",
    "                            if not isinstance(table_name, str) or not table_name.strip() or mode not in ['overwrite', 'append', 'ignore']:\n",
    "                                print(f\"Error: Invalid parameters for explicit 'write' task at index {i}: (table_name='{table_name}', mode='{mode}'). table_name must be a non-empty string and mode must be 'overwrite', 'append', or 'ignore'. Skipping.\")\n",
    "                                previous_task_successful = False\n",
    "                                continue\n",
    "                            result = self.write(table_name=table_name, mode=mode)\n",
    "                        elif task_type == 'sql':\n",
    "                            # Validate parameters for 'sql' task\n",
    "                            if (sql_statement is None or not isinstance(sql_statement, str) or not sql_statement.strip()) and \\\n",
    "                               (table_name is None or not isinstance(table_name, str) or not table_name.strip()):\n",
    "                                print(f\"Error: Invalid parameters for 'sql' task at index {i}: (sql_statement='{sql_statement}', table_name='{table_name}'). Either sql_statement or table_name must be provided. Skipping.\")\n",
    "                                previous_task_successful = False\n",
    "                                continue\n",
    "                            result = self.sql(sql_statement=sql_statement, table_name=table_name)\n",
    "                    else:\n",
    "                        print(f\"Error: Invalid task entry format at index {i}: {task}. Expected a tuple of length 2 (table_name, mode) or 4 (task_type, table_name, mode, sql_statement). Skipping this task.\")\n",
    "                        previous_task_successful = False\n",
    "                        continue\n",
    "                else:\n",
    "                    print(f\"Error: Invalid task entry type at index {i}: {task}. Expected a tuple or list. Skipping this task.\")\n",
    "                    previous_task_successful = False\n",
    "                    continue\n",
    "\n",
    "\n",
    "                if result == 1:\n",
    "                    print(f\"Task {i+1} successful.\")\n",
    "                    previous_task_successful = True\n",
    "                else:\n",
    "                    print(f\"Task {i+1} failed. Stopping sequence.\")\n",
    "                    previous_task_successful = False\n",
    "                    break\n",
    "            else:\n",
    "                # Construct skip info based on the task format encountered\n",
    "                skip_info = \"Unknown Task Format\"\n",
    "                if isinstance(task, (list, tuple)):\n",
    "                    if len(task) == 2:\n",
    "                        skip_info = f\"Write Task: Table: {task[0]}, Mode: {task[1]}\"\n",
    "                    elif len(task) == 4:\n",
    "                        skip_info = f\"Explicit Task: Type: {task[0]}, Table: {task[1]}, Statement: {task[3] is not None}\"\n",
    "\n",
    "                print(f\"Skipping Task {i+1} ({skip_info}) due to previous failure.\")\n",
    "\n",
    "\n",
    "        if previous_task_successful:\n",
    "            print(\"\\nAll specified SQL tasks completed successfully.\")\n",
    "        else:\n",
    "            print(\"\\nOne or more SQL tasks failed.\")\n",
    "\n",
    "        return previous_task_successful\n",
    "\n",
    "\n",
    "    def get_connection(self):\n",
    "        \"\"\"\n",
    "        Returns the active DuckDB connection object.\n",
    "        \"\"\"\n",
    "        return self.con\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Closes the DuckDB connection.\n",
    "        \"\"\"\n",
    "        if self.con:\n",
    "            self.con.close()\n",
    "            print(\"DuckDB connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02dab12-9c27-4be1-b4f4-4fca2d598dd0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "con = Tasksql.connect(\n",
    "     workspace='processing',\n",
    "     lakehouse_name='test',\n",
    "     schema='new',\n",
    "     sql_folder='https://github.com/djouallah/Fabric_Notebooks_Demo/raw/refs/heads/main/orchestration/sql/',\n",
    "     compaction_threshold=100 \n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f7e851-0aea-49e6-bffa-d120e236ce90",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sql_tasks_to_run_nightly = [\n",
    "    ('price', 'append'),\n",
    "    ('scada', 'append'),\n",
    "    ('duid', 'ignore'),\n",
    "    ('summary', 'overwrite'),\n",
    "    ('calendar', 'ignore'),\n",
    "    ('mstdatetime', 'ignore')\n",
    "    ]\n",
    "con.run_sql_sequence(sql_tasks_to_run_nightly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e18c5-1b23-49a1-b745-7d2664334042",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sql_tasks_to_intraday = [\n",
    "    ('price_today', 'append'),\n",
    "    ('scada_today', 'append'),\n",
    "    ('duid', 'ignore'),\n",
    "    ('summary', 'append')\n",
    "]\n",
    "con.run_sql_sequence(sql_tasks_to_intraday)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
