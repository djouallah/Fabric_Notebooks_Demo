{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Fabric specific, define nbr of cores, default 2, you can pick 4,8,16 ,32, 64\n",
        "#%%configure\n",
        "#{\"vCores\": 2}"
      ],
      "metadata": {
        "id": "ghHEhd2xlhK5"
      },
      "id": "ghHEhd2xlhK5",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "333a177b-f075-412e-8ca1-32d44f8c07eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "333a177b-f075-412e-8ca1-32d44f8c07eb",
        "outputId": "bcec8a93-dbfe-4f40-b665-e113c3f19127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-polars-cu12 25.6.0 requires polars<1.29,>=1.25, but you have polars 1.32.0b1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n",
            "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chdb\n",
            "  Downloading chdb-3.5.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pyarrow>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from chdb) (21.0.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from chdb) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->chdb) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->chdb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->chdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->chdb) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->chdb) (1.17.0)\n",
            "Downloading chdb-3.5.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.5/180.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: chdb\n",
            "Successfully installed chdb-3.5.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.6/28.6 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install    -q duckdb  --upgrade\n",
        "!pip install    -q polars  --pre --upgrade\n",
        "!pip install    -q pyarrow --upgrade\n",
        "!pip install    -q daft\n",
        "!pip install     chdb\n",
        "!pip install    -q datafusion\n",
        "!pip install    -q deltalake\n",
        "#use this in Fabric to restart the kernel\n",
        "#import sys\n",
        "#sys.exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Source             = \"/lakehouse/default/Files/0_Source/ARCHIVE/Daily_Reports/\"\n",
        "Destination        = \"/lakehouse/default/Files/1_Transform/0/ARCHIVE/Daily_Reports/\"\n",
        "# total files to process 60 * (nbr_copies +1)\n",
        "nbr_copies         = 0\n",
        "total_files        = 60 * (nbr_copies +1)\n",
        "# in onelake use abfss path, mounted storage does not works in deltalake > 1\n",
        "output_path        = f\"/lakehouse/default/Tables/T{total_files}/\"\n",
        "performance_result = \"/lakehouse/default/Tables/dbo/results\""
      ],
      "metadata": {
        "id": "7f1fkgfxfAiU"
      },
      "id": "7f1fkgfxfAiU",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "G3Hg6ij-5PUP",
      "metadata": {
        "id": "G3Hg6ij-5PUP"
      },
      "source": [
        "***Import***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "R-CybaLr5SBx",
      "metadata": {
        "id": "R-CybaLr5SBx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import shutil\n",
        "from   urllib.request import urlopen\n",
        "import os\n",
        "import requests\n",
        "import pyarrow.dataset as ds\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow         as pa\n",
        "from   pyarrow         import csv\n",
        "import pyarrow.compute as pc\n",
        "import multiprocessing\n",
        "from   shutil import unpack_archive\n",
        "import time\n",
        "from   datetime import datetime\n",
        "from   deltalake.writer import write_deltalake\n",
        "import glob\n",
        "import duckdb\n",
        "from   psutil import *\n",
        "import daft\n",
        "from   daft import DataType, col\n",
        "import polars as pl\n",
        "from   chdb import session as chs\n",
        "from   datafusion import SessionContext as session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "815f881e-4109-431e-90cd-dab1a806132f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "815f881e-4109-431e-90cd-dab1a806132f",
        "outputId": "73b80436-1633-45f9-d055-03c2bda46e50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 vCPU Memory:13.0\n"
          ]
        }
      ],
      "source": [
        "core = cpu_count()\n",
        "vCPU = str(core) + \" vCPU\"\n",
        "mem=round(virtual_memory().total/(1024 * 1024 * 1024),0)\n",
        "print(vCPU +' Memory:'+ str(mem))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee4d52d4-761a-4e7b-993b-0889fda74a94",
      "metadata": {
        "id": "ee4d52d4-761a-4e7b-993b-0889fda74a94"
      },
      "source": [
        "# Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "28416977-0f39-41ca-aa84-9a5d42a6e519",
      "metadata": {
        "id": "28416977-0f39-41ca-aa84-9a5d42a6e519"
      },
      "outputs": [],
      "source": [
        "def download(url,Path):\n",
        "    if not os.path.exists(Path):\n",
        "      os.makedirs(Path, exist_ok=True)\n",
        "    result = urlopen(url).read().decode('utf-8')\n",
        "    pattern = re.compile(r'[\\w.]*.zip')\n",
        "    filelist1 = pattern.findall(result)\n",
        "    filelist_unique = dict.fromkeys(filelist1)\n",
        "    filelist =sorted(filelist_unique, reverse=True)\n",
        "    current =  [os.path.basename(x) for x in glob.glob(Path+'*.zip')]\n",
        "    files_to_upload = list(set(filelist) - set(current))\n",
        "    files_to_upload = list(dict.fromkeys(files_to_upload))\n",
        "    print(str(len(files_to_upload)) + ' New File Loaded')\n",
        "    if len(files_to_upload) != 0 :\n",
        "      for x in files_to_upload:\n",
        "           with requests.get(url+x, stream=True) as resp:\n",
        "            if resp.ok:\n",
        "              with open(f\"{Path}{x}\", \"wb\") as f:\n",
        "               for chunk in resp.iter_content(chunk_size=4096):\n",
        "                f.write(chunk)\n",
        "    return \"done\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3b5f3047-8010-41ba-8e30-396e26de0e81",
      "metadata": {
        "id": "3b5f3047-8010-41ba-8e30-396e26de0e81"
      },
      "outputs": [],
      "source": [
        "def uncompress(x):\n",
        "        unpack_archive(str(Source+x), str(Destination), 'zip')\n",
        "def unzip(Source, Destination):\n",
        "    if not os.path.exists(Destination):\n",
        "      os.makedirs(Destination, exist_ok=True)\n",
        "    filelist=[os.path.basename(x) for x in glob.glob(Source+'*.zip')]\n",
        "    ### checl the unzipped files already\n",
        "    current = [os.path.basename(x) for x in glob.glob(Destination+'*.CSV')]\n",
        "    current = [w.replace('.CSV','.zip') for w in current]\n",
        "    #unzip only the delta\n",
        "    files_to_upload = list(set(filelist) - set(current))\n",
        "    files_to_upload = list(dict.fromkeys(files_to_upload))\n",
        "    print(str(len(files_to_upload)) + ' New File uncompressed')\n",
        "    if len(files_to_upload) != 0 :\n",
        "      with multiprocessing.Pool() as pool:\n",
        "       for _ in pool.imap_unordered(uncompress, files_to_upload, chunksize=1):\n",
        "         pass\n",
        "      return \"done\"\n",
        "    else:\n",
        "     return \"nothing to see here\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8LbA_eCFM-Zl",
      "metadata": {
        "id": "8LbA_eCFM-Zl"
      },
      "source": [
        "# DuckDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "21256ca2-634e-4d44-ae45-4e1850258ad1",
      "metadata": {
        "id": "21256ca2-634e-4d44-ae45-4e1850258ad1"
      },
      "outputs": [],
      "source": [
        "def duckdb_clean_csv(x):\n",
        "\tcon = duckdb.connect()\n",
        "\tcon.sql(\" SET preserve_insertion_order = false; \")\n",
        "\traw =con.sql(F\"\"\"\n",
        "\tfrom read_csv({x},\n",
        "\tSkip=1,header =0,all_varchar=1,\n",
        "\tcolumns={{\n",
        "\t'I': 'VARCHAR','UNIT': 'VARCHAR','XX': 'VARCHAR','VERSION': 'VARCHAR','SETTLEMENTDATE': 'VARCHAR','RUNNO': 'VARCHAR',\n",
        "\t'DUID': 'VARCHAR','INTERVENTION': 'VARCHAR','DISPATCHMODE': 'VARCHAR','AGCSTATUS': 'VARCHAR','INITIALMW': 'VARCHAR',\n",
        "\t'TOTALCLEARED': 'VARCHAR','RAMPDOWNRATE': 'VARCHAR','RAMPUPRATE': 'VARCHAR','LOWER5MIN': 'VARCHAR',\n",
        "\t'LOWER60SEC': 'VARCHAR','LOWER6SEC': 'VARCHAR','RAISE5MIN': 'VARCHAR','RAISE60SEC': 'VARCHAR',\n",
        "\t'RAISE6SEC': 'VARCHAR','MARGINAL5MINVALUE': 'VARCHAR','MARGINAL60SECVALUE': 'VARCHAR',\n",
        "\t'MARGINAL6SECVALUE': 'VARCHAR','MARGINALVALUE': 'VARCHAR','VIOLATION5MINDEGREE': 'VARCHAR',\n",
        "\t'VIOLATION60SECDEGREE': 'VARCHAR','VIOLATION6SECDEGREE': 'VARCHAR','VIOLATIONDEGREE': 'VARCHAR',\n",
        "\t'LOWERREG': 'VARCHAR','RAISEREG': 'VARCHAR','AVAILABILITY': 'VARCHAR','RAISE6SECFLAGS': 'VARCHAR',\n",
        "\t'RAISE60SECFLAGS': 'VARCHAR','RAISE5MINFLAGS': 'VARCHAR','RAISEREGFLAGS': 'VARCHAR',\n",
        "\t'LOWER6SECFLAGS': 'VARCHAR','LOWER60SECFLAGS': 'VARCHAR','LOWER5MINFLAGS': 'VARCHAR',\n",
        "\t'LOWERREGFLAGS': 'VARCHAR','RAISEREGAVAILABILITY': 'VARCHAR','RAISEREGENABLEMENTMAX': 'VARCHAR',\n",
        "\t'RAISEREGENABLEMENTMIN': 'VARCHAR','LOWERREGAVAILABILITY': 'VARCHAR','LOWERREGENABLEMENTMAX': 'VARCHAR',\n",
        "\t'LOWERREGENABLEMENTMIN': 'VARCHAR','RAISE6SECACTUALAVAILABILITY': 'VARCHAR',\n",
        "\t'RAISE60SECACTUALAVAILABILITY': 'VARCHAR','RAISE5MINACTUALAVAILABILITY': 'VARCHAR',\n",
        "\t'RAISEREGACTUALAVAILABILITY': 'VARCHAR','LOWER6SECACTUALAVAILABILITY': 'VARCHAR',\n",
        "\t'LOWER60SECACTUALAVAILABILITY': 'VARCHAR','LOWER5MINACTUALAVAILABILITY': 'VARCHAR','LOWERREGACTUALAVAILABILITY': 'VARCHAR'\n",
        "\t}},\n",
        "\tfilename =1,null_padding = true,ignore_errors=1,auto_detect=false)\n",
        "\twhere I='D' and UNIT ='DUNIT' AND VERSION = 3                  \"\"\")\n",
        "\n",
        "\tdf=con.sql(\"\"\"\n",
        "\tselect\n",
        "\tUNIT,\n",
        "\tDUID,\n",
        "\tfilename,\n",
        "\tcast(columns(*exclude(DUID,UNIT,SETTLEMENTDATE,I,XX,filename)) as double),\n",
        "\tcast (SETTLEMENTDATE as TIMESTAMPTZ) as SETTLEMENTDATE,\n",
        "\tyear (cast (SETTLEMENTDATE as timestamp)) as year  from raw\n",
        "\t\"\"\").record_batch()\n",
        "\twrite_deltalake(output_path + 'duckdb',df,mode=\"overwrite\",partition_by=['year'])\n",
        "\treturn \"done\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GHXhmrC28Zt_",
      "metadata": {
        "id": "GHXhmrC28Zt_"
      },
      "source": [
        "# Daft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "08b217c6-51a8-4e13-96cd-3f275754cad6",
      "metadata": {
        "id": "08b217c6-51a8-4e13-96cd-3f275754cad6"
      },
      "outputs": [],
      "source": [
        "def daft_clean_csv(files_to_upload_full_Path):\n",
        "\t#for onelake add this add this io_config=io_config\n",
        "\t#from   daft.io import IOConfig, AzureConfig\n",
        "\t#io_config = IOConfig(\tazure=AzureConfig(storage_account=\"onelake\",use_fabric_endpoint=True,bearer_token= notebookutils.credentials.getToken('storage')))\n",
        "\tschema={\n",
        "\t\t\t\t'I': DataType.string(),'UNIT': DataType.string(),'XX': DataType.string(),'VERSION': DataType.string(),'SETTLEMENTDATE': DataType.string(),'RUNNO': DataType.string(),\n",
        "\t\t\t\t'DUID': DataType.string(),'INTERVENTION': DataType.string(),'DISPATCHMODE': DataType.string(),'AGCSTATUS': DataType.string(),'INITIALMW': DataType.string(),\n",
        "\t\t\t\t'TOTALCLEARED': DataType.string(),'RAMPDOWNRATE': DataType.string(),'RAMPUPRATE': DataType.string(),'LOWER5MIN': DataType.string(),\n",
        "\t\t\t\t'LOWER60SEC': DataType.string(),'LOWER6SEC': DataType.string(),'RAISE5MIN': DataType.string(),'RAISE60SEC': DataType.string(),\n",
        "\t\t\t\t'RAISE6SEC': DataType.string(),'MARGINAL5MINVALUE': DataType.string(),'MARGINAL60SECVALUE': DataType.string(),\n",
        "\t\t\t\t'MARGINAL6SECVALUE': DataType.string(),'MARGINALVALUE': DataType.string(),'VIOLATION5MINDEGREE': DataType.string(),\n",
        "\t\t\t\t'VIOLATION60SECDEGREE': DataType.string(),'VIOLATION6SECDEGREE': DataType.string(),'VIOLATIONDEGREE': DataType.string(),\n",
        "\t\t\t\t'LOWERREG': DataType.string(),'RAISEREG': DataType.string(),'AVAILABILITY': DataType.string(),'RAISE6SECFLAGS': DataType.string(),\n",
        "\t\t\t\t'RAISE60SECFLAGS': DataType.string(),'RAISE5MINFLAGS': DataType.string(),'RAISEREGFLAGS': DataType.string(),\n",
        "\t\t\t\t'LOWER6SECFLAGS': DataType.string(),'LOWER60SECFLAGS': DataType.string(),'LOWER5MINFLAGS': DataType.string(),\n",
        "\t\t\t\t'LOWERREGFLAGS': DataType.string(),'RAISEREGAVAILABILITY': DataType.string(),'RAISEREGENABLEMENTMAX': DataType.string(),\n",
        "\t\t\t\t'RAISEREGENABLEMENTMIN': DataType.string(),'LOWERREGAVAILABILITY': DataType.string(),'LOWERREGENABLEMENTMAX': DataType.string(),\n",
        "\t\t\t\t'LOWERREGENABLEMENTMIN': DataType.string(),'RAISE6SECACTUALAVAILABILITY': DataType.string(),\n",
        "\t\t\t\t'RAISE60SECACTUALAVAILABILITY': DataType.string(),'RAISE5MINACTUALAVAILABILITY': DataType.string(),\n",
        "\t\t\t\t'RAISEREGACTUALAVAILABILITY': DataType.string(),'LOWER6SECACTUALAVAILABILITY': DataType.string(),\n",
        "\t\t\t\t'LOWER60SECACTUALAVAILABILITY': DataType.string(),'LOWER5MINACTUALAVAILABILITY': DataType.string(),'LOWERREGACTUALAVAILABILITY': DataType.string()}\n",
        "\tdf = daft.read_csv(files_to_upload_full_Path,schema=schema, infer_schema=False, has_headers=False, allow_variable_columns=True,file_path_column=\"filename\")\n",
        "\tdf = df.where((df[\"UNIT\"] == 'DUNIT' ) & (df[\"VERSION\"] == '3') & (df[\"I\"] == 'D'))\n",
        "\tdf = df.exclude('I','XX')\n",
        "\tdf_cols = list(set(df.column_names) - {'SETTLEMENTDATE','DUID','file','UNIT','transactionId','PRIORITY','file'})\n",
        "\tfor col_name in df_cols:\n",
        "\t\t\tdf = df.with_column(col_name, col(col_name).cast(DataType.float64()))\n",
        "\tdf = df.with_column(\"SETTLEMENTDATE\", df[\"SETTLEMENTDATE\"].str.to_datetime(\"%Y/%m/%d %H:%M:%S\"))\n",
        "\tdf = df.with_column('DATE', col('SETTLEMENTDATE').cast(DataType.date()))\n",
        "\tdf = df.with_column('year', col('SETTLEMENTDATE').dt.year())\n",
        "\tdf.write_deltalake(output_path + 'daft',mode=\"overwrite\",partition_cols=[\"year\"])\n",
        "\treturn \"done\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sBkDAT9PhxIx",
      "metadata": {
        "id": "sBkDAT9PhxIx"
      },
      "source": [
        "# Polars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ca89eb3a-a145-468f-8490-1a6ba970a25a",
      "metadata": {
        "id": "ca89eb3a-a145-468f-8490-1a6ba970a25a"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def polars_clean_csv(x):\n",
        "  schema ={'I': pl.String,'UNIT': pl.String,'XX': pl.String,'VERSION': pl.String,'SETTLEMENTDATE': pl.String,'RUNNO': pl.String,\n",
        "    'DUID': pl.String,'INTERVENTION': pl.String,'DISPATCHMODE': pl.String,'AGCSTATUS': pl.String,'INITIALMW': pl.String,\n",
        "    'TOTALCLEARED': pl.String,'RAMPDOWNRATE': pl.String,'RAMPUPRATE': pl.String,'LOWER5MIN': pl.String,\n",
        "    'LOWER60SEC': pl.String,'LOWER6SEC': pl.String,'RAISE5MIN': pl.String,'RAISE60SEC': pl.String,\n",
        "    'RAISE6SEC': pl.String,'MARGINAL5MINVALUE': pl.String,'MARGINAL60SECVALUE': pl.String,\n",
        "    'MARGINAL6SECVALUE': pl.String,'MARGINALVALUE': pl.String,'VIOLATION5MINDEGREE': pl.String,\n",
        "    'VIOLATION60SECDEGREE': pl.String,'VIOLATION6SECDEGREE': pl.String,'VIOLATIONDEGREE': pl.String,\n",
        "    'LOWERREG': pl.String,'RAISEREG': pl.String,'AVAILABILITY': pl.String,'RAISE6SECFLAGS': pl.String,\n",
        "    'RAISE60SECFLAGS': pl.String,'RAISE5MINFLAGS': pl.String,'RAISEREGFLAGS': pl.String,\n",
        "    'LOWER6SECFLAGS': pl.String,'LOWER60SECFLAGS': pl.String,'LOWER5MINFLAGS': pl.String,\n",
        "    'LOWERREGFLAGS': pl.String,'RAISEREGAVAILABILITY': pl.String,'RAISEREGENABLEMENTMAX': pl.String,\n",
        "    'RAISEREGENABLEMENTMIN': pl.String,'LOWERREGAVAILABILITY': pl.String,'LOWERREGENABLEMENTMAX': pl.String,\n",
        "    'LOWERREGENABLEMENTMIN': pl.String,'RAISE6SECACTUALAVAILABILITY': pl.String,\n",
        "    'RAISE60SECACTUALAVAILABILITY': pl.String,'RAISE5MINACTUALAVAILABILITY': pl.String,\n",
        "    'RAISEREGACTUALAVAILABILITY': pl.String,'LOWER6SECACTUALAVAILABILITY': pl.String,\n",
        "    'LOWER60SECACTUALAVAILABILITY': pl.String,'LOWER5MINACTUALAVAILABILITY': pl.String,'LOWERREGACTUALAVAILABILITY': pl.String}\n",
        "  raw = pl.scan_csv(x, skip_rows=1, schema=schema, has_header=False,truncate_ragged_lines=True,include_file_paths=\"filename\")\n",
        "  transform =(\n",
        "      raw\n",
        "      .filter( (pl.col(\"I\")=='D') &  (pl.col(\"UNIT\")=='DUNIT') & (pl.col(\"VERSION\")=='3') )\n",
        "      .drop(\"XX\",\"I\")\n",
        "  )\n",
        "\n",
        "  z = transform.with_columns(pl.col(\"SETTLEMENTDATE\").str.to_datetime())\n",
        "  columns = list(set(transform.collect_schema().names()) - {'SETTLEMENTDATE','DUID','UNIT','filename'})\n",
        "  final=z.with_columns(pl.col(columns).cast(pl.Float64),year=pl.col(\"SETTLEMENTDATE\").dt.iso_year()).collect(new_streaming=True)\n",
        "  # for onelake add this storage_options = {\"bearer_token\": notebookutils.credentials.getToken('storage'), \"use_fabric_endpoint\": \"true\"}\n",
        "  final.write_delta(output_path + 'polars',mode=\"overwrite\")\n",
        "  return \"done\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hEjpkeuATHiD",
      "metadata": {
        "id": "hEjpkeuATHiD"
      },
      "source": [
        "# Pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3epZDhdzTKv6",
      "metadata": {
        "id": "3epZDhdzTKv6"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def pyarrow_clean_csv(files_to_upload_full_Path):\n",
        "  new_schema = pa.schema([\n",
        "  ('I', pa.string()),  ('UNIT', pa.string()),  ('XX', pa.string()),  ('VERSION', pa.float64()),  ('SETTLEMENTDATE', pa.string()),  ('RUNNO', pa.float64()),  ('DUID', pa.string()),  ('INTERVENTION', pa.float64()),\n",
        "  ('DISPATCHMODE', pa.float64()),  ('AGCSTATUS', pa.float64()),  ('INITIALMW', pa.float64()),  ('TOTALCLEARED', pa.float64()),  ('RAMPDOWNRATE', pa.float64()),  ('RAMPUPRATE', pa.float64()),  ('LOWER5MIN', pa.float64()),\n",
        "  ('LOWER60SEC', pa.float64()),  ('LOWER6SEC', pa.float64()),  ('RAISE5MIN', pa.float64()),  ('RAISE60SEC', pa.float64()),  ('RAISE6SEC', pa.float64()),  ('MARGINAL5MINVALUE', pa.float64()),  ('MARGINAL60SECVALUE', pa.float64()),\n",
        "  ('MARGINAL6SECVALUE', pa.float64()),  ('MARGINALVALUE', pa.float64()),  ('VIOLATION5MINDEGREE', pa.float64()),  ('VIOLATION60SECDEGREE', pa.float64()),  ('VIOLATION6SECDEGREE', pa.float64()),  ('VIOLATIONDEGREE', pa.float64()),\n",
        "  ('LOWERREG', pa.float64()),  ('RAISEREG', pa.float64()),  ('AVAILABILITY', pa.float64()),  ('RAISE6SECFLAGS', pa.float64()),  ('RAISE60SECFLAGS', pa.float64()),  ('RAISE5MINFLAGS', pa.float64()),\n",
        "  ('RAISEREGFLAGS', pa.float64()),  ('LOWER6SECFLAGS', pa.float64()),  ('LOWER60SECFLAGS', pa.float64()),  ('LOWER5MINFLAGS', pa.float64()),  ('LOWERREGFLAGS', pa.float64()),  ('RAISEREGAVAILABILITY', pa.float64()),\n",
        "  ('RAISEREGENABLEMENTMAX', pa.float64()),  ('RAISEREGENABLEMENTMIN', pa.float64()),  ('LOWERREGAVAILABILITY', pa.float64()),  ('LOWERREGENABLEMENTMAX', pa.float64()),  ('LOWERREGENABLEMENTMIN', pa.float64()),\n",
        "  ('RAISE6SECACTUALAVAILABILITY', pa.float64()),  ('RAISE60SECACTUALAVAILABILITY', pa.float64()),  ('RAISE5MINACTUALAVAILABILITY', pa.float64()),  ('RAISEREGACTUALAVAILABILITY', pa.float64()),  ('LOWER6SECACTUALAVAILABILITY', pa.float64()),\n",
        "  ('LOWER60SECACTUALAVAILABILITY', pa.float64()),  ('LOWER5MINACTUALAVAILABILITY', pa.float64()),  ('LOWERREGACTUALAVAILABILITY', pa.float64()),\n",
        "    ])\n",
        "  ReadOptions = csv.ReadOptions(column_names=new_schema.names,skip_rows=1)\n",
        "  ParseOptions  = csv.ParseOptions(invalid_row_handler=lambda i: \"skip\")\n",
        "  ConvertOptions = csv.ConvertOptions(strings_can_be_null=True)\n",
        "  format = ds.CsvFileFormat(parse_options = ParseOptions ,convert_options = ConvertOptions ,read_options = ReadOptions  )\n",
        "  raw = ds.dataset(files_to_upload_full_Path, format =format).filter((pc.field('I') == 'D') & (pc.field('UNIT') == 'DUNIT') & (pc.field('VERSION') == 3)).to_table().cast(new_schema)\n",
        "  df= raw.drop_columns(['I','XX'])\n",
        "  df = df.set_column(df.schema.get_field_index(\"SETTLEMENTDATE\"), \"SETTLEMENTDATE\",pc.strptime(df['SETTLEMENTDATE'], format=\"%Y/%m/%d %H:%M:%S\", unit='s').cast(pa.timestamp(\"s\")))\n",
        "  df = df.add_column(0,\"year\", pc.year(df['SETTLEMENTDATE']))\n",
        "  write_deltalake(output_path + 'pyarrow',df, mode=\"overwrite\", partition_by=['year'])\n",
        "  return \"done\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72ae953a-3ab4-43e2-9a96-37cf9e6f71b8",
      "metadata": {
        "id": "72ae953a-3ab4-43e2-9a96-37cf9e6f71b8"
      },
      "source": [
        "# chDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "be7c80d4-41fb-4702-9ce4-bb9a072607ce",
      "metadata": {
        "id": "be7c80d4-41fb-4702-9ce4-bb9a072607ce"
      },
      "outputs": [],
      "source": [
        "def chdb_clean_csv(files_to_upload_full_Path):\n",
        "    L = tuple(files_to_upload_full_Path)\n",
        "    sql = f\"\"\"\n",
        "    WITH raw AS (\n",
        "        SELECT *, _file, _path\n",
        "        FROM file('{Destination}*.CSV','CSV')\n",
        "        WHERE c1 = 'D' AND c2 = 'DUNIT' AND c4 = '3'\n",
        "        AND _path IN {L}\n",
        "    )\n",
        "    SELECT\n",
        "        c2 as UNIT,\n",
        "        toFloat64OrNull(c4) AS VERSION,\n",
        "        parseDateTimeBestEffort(c5) AS SETTLEMENTDATE,\n",
        "        toFloat64OrNull(c6) AS RUNNO,\n",
        "        c7 AS DUID,\n",
        "        toFloat64OrNull(c8) AS INTERVENTION,\n",
        "        toFloat64OrNull(c9) AS DISPATCHMODE,\n",
        "        toFloat64OrNull(c10) AS AGCSTATUS,\n",
        "        toFloat64OrNull(c11) AS INITIALMW,\n",
        "        toFloat64OrNull(c12) AS TOTALCLEARED,\n",
        "        toFloat64OrNull(c13) AS RAMPDOWNRATE,\n",
        "        toFloat64OrNull(c14) AS RAMPUPRATE,\n",
        "        toFloat64OrNull(c15) AS LOWER5MIN,\n",
        "        toFloat64OrNull(c16) AS LOWER60SEC,\n",
        "        toFloat64OrNull(c17) AS LOWER6SEC,\n",
        "        toFloat64OrNull(c18) AS RAISE5MIN,\n",
        "        toFloat64OrNull(c19) AS RAISE60SEC,\n",
        "        toFloat64OrNull(c20) AS RAISE6SEC,\n",
        "        toFloat64OrNull(c21) AS MARGINAL5MINVALUE,\n",
        "        toFloat64OrNull(c22) AS MARGINAL60SECVALUE,\n",
        "        toFloat64OrNull(c23) AS MARGINAL6SECVALUE,\n",
        "        toFloat64OrNull(c24) AS MARGINALVALUE,\n",
        "        toFloat64OrNull(c25) AS VIOLATION5MINDEGREE,\n",
        "        toFloat64OrNull(c26) AS VIOLATION60SECDEGREE,\n",
        "        toFloat64OrNull(c27) AS VIOLATION6SECDEGREE,\n",
        "        toFloat64OrNull(c28) AS VIOLATIONDEGREE,\n",
        "        toFloat64OrNull(c29) AS LOWERREG,\n",
        "        toFloat64OrNull(c30) AS RAISEREG,\n",
        "        toFloat64OrNull(c31) AS AVAILABILITY,\n",
        "        toFloat64OrNull(c32) AS RAISE6SECFLAGS,\n",
        "        toFloat64OrNull(c33) AS RAISE60SECFLAGS,\n",
        "        toFloat64OrNull(c34) AS RAISE5MINFLAGS,\n",
        "        toFloat64OrNull(c35) AS RAISEREGFLAGS,\n",
        "        toFloat64OrNull(c36) AS LOWER6SECFLAGS,\n",
        "        toFloat64OrNull(c37) AS LOWER60SECFLAGS,\n",
        "        toFloat64OrNull(c38) AS LOWER5MINFLAGS,\n",
        "        toFloat64OrNull(c39) AS LOWERREGFLAGS,\n",
        "        toFloat64OrNull(c40) AS RAISEREGAVAILABILITY,\n",
        "        toFloat64OrNull(c41) AS RAISEREGENABLEMENTMAX,\n",
        "        toFloat64OrNull(c42) AS RAISEREGENABLEMENTMIN,\n",
        "        toFloat64OrNull(c43) AS LOWERREGAVAILABILITY,\n",
        "        toFloat64OrNull(c44) AS LOWERREGENABLEMENTMAX,\n",
        "        toFloat64OrNull(c45) AS LOWERREGENABLEMENTMIN,\n",
        "        toFloat64OrNull(c46) AS RAISE6SECACTUALAVAILABILITY,\n",
        "        toFloat64OrNull(c47) AS RAISE60SECACTUALAVAILABILITY,\n",
        "        toFloat64OrNull(c48) AS RAISE5MINACTUALAVAILABILITY,\n",
        "        toFloat64OrNull(c49) AS RAISEREGACTUALAVAILABILITY,\n",
        "        toFloat64OrNull(c50) AS LOWER6SECACTUALAVAILABILITY,\n",
        "        toFloat64OrNull(c51) AS LOWER60SECACTUALAVAILABILITY,\n",
        "        toFloat64OrNull(c52) AS LOWER5MINACTUALAVAILABILITY,\n",
        "        toFloat64OrNull(c53) AS LOWERREGACTUALAVAILABILITY,\n",
        "        _file as filename,\n",
        "        toYear(SETTLEMENTDATE) AS year\n",
        "    FROM raw\n",
        "    \"\"\"\n",
        "\n",
        "    sess = chs.Session()\n",
        "    sess.query('''\n",
        "    SET input_format_csv_use_best_effort_in_schema_inference = 0;\n",
        "    SET input_format_csv_skip_first_lines = 1;\n",
        "    SET input_format_csv_allow_variable_number_of_columns = 1;\n",
        "    ''')\n",
        "\n",
        "    streaming = sess.send_query(sql, \"Arrow\")\n",
        "    write_deltalake(output_path + 'chdb', streaming.record_batch(), mode=\"overwrite\", partition_by=['year'])\n",
        "    streaming.close()\n",
        "    sess.close()\n",
        "    return \"done\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jF8LwkZZJ1Pk",
      "metadata": {
        "id": "jF8LwkZZJ1Pk"
      },
      "source": [
        "# Datafusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9jiMWlrkaIAe",
      "metadata": {
        "id": "9jiMWlrkaIAe"
      },
      "outputs": [],
      "source": [
        "def datafusion_clean_csv(files_to_upload_full_Path):\n",
        "  ctx = session()\n",
        "  colum_names = [\n",
        "    'I', 'UNIT', 'XX', 'VERSION', 'SETTLEMENTDATE', 'RUNNO', 'DUID', 'INTERVENTION',\n",
        "    'DISPATCHMODE', 'AGCSTATUS', 'INITIALMW', 'TOTALCLEARED', 'RAMPDOWNRATE', 'RAMPUPRATE',\n",
        "    'LOWER5MIN', 'LOWER60SEC', 'LOWER6SEC', 'RAISE5MIN', 'RAISE60SEC', 'RAISE6SEC',\n",
        "    'MARGINAL5MINVALUE', 'MARGINAL60SECVALUE', 'MARGINAL6SECVALUE', 'MARGINALVALUE',\n",
        "    'VIOLATION5MINDEGREE', 'VIOLATION60SECDEGREE', 'VIOLATION6SECDEGREE', 'VIOLATIONDEGREE',\n",
        "    'LOWERREG', 'RAISEREG', 'AVAILABILITY', 'RAISE6SECFLAGS', 'RAISE60SECFLAGS',\n",
        "    'RAISE5MINFLAGS', 'RAISEREGFLAGS', 'LOWER6SECFLAGS', 'LOWER60SECFLAGS', 'LOWER5MINFLAGS',\n",
        "    'LOWERREGFLAGS', 'RAISEREGAVAILABILITY', 'RAISEREGENABLEMENTMAX', 'RAISEREGENABLEMENTMIN',\n",
        "    'LOWERREGAVAILABILITY', 'LOWERREGENABLEMENTMAX', 'LOWERREGENABLEMENTMIN',\n",
        "    'RAISE6SECACTUALAVAILABILITY', 'RAISE60SECACTUALAVAILABILITY', 'RAISE5MINACTUALAVAILABILITY',\n",
        "    'RAISEREGACTUALAVAILABILITY', 'LOWER6SECACTUALAVAILABILITY', 'LOWER60SECACTUALAVAILABILITY',\n",
        "    'LOWER5MINACTUALAVAILABILITY', 'LOWERREGACTUALAVAILABILITY'\n",
        "   ]\n",
        "\n",
        "  ReadOptions     = csv.ReadOptions(column_names=colum_names,skip_rows=1)\n",
        "  ParseOptions    = csv.ParseOptions(invalid_row_handler=lambda i: \"skip\")\n",
        "  ConvertOptions  = csv.ConvertOptions(strings_can_be_null=True)\n",
        "  format          = ds.CsvFileFormat(parse_options = ParseOptions ,convert_options = ConvertOptions ,read_options = ReadOptions  )\n",
        "  raw             = ds.dataset(files_to_upload_full_Path, format =format)\n",
        "  ctx.register_dataset(\"arrow_dataset\" , raw)\n",
        "  ######################################\n",
        "  df = ctx.sql(\"\"\"\n",
        "                     select * EXCLUDE(\"I\",\"XX\",\"SETTLEMENTDATE\"),\n",
        "                     to_timestamp_seconds (\"SETTLEMENTDATE\",'%Y/%m/%d %H:%M:%S') as SETTLEMENTDATE,\n",
        "                     cast(EXTRACT(YEAR FROM  to_timestamp_seconds (\"SETTLEMENTDATE\",'%Y/%m/%d %H:%M:%S')) as integer) as year\n",
        "                     from arrow_dataset where \"I\" ='D' and \"UNIT\"='DUNIT' and \"VERSION\" ='3'\n",
        "               \"\"\").collect()\n",
        "  write_deltalake(output_path + 'datafusion',df, mode=\"overwrite\", partition_by=['year'])\n",
        "  return \"done\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qpFll5xw3Mvq",
      "metadata": {
        "id": "qpFll5xw3Mvq"
      },
      "source": [
        "# Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7USg9dd-1ivc",
      "metadata": {
        "id": "7USg9dd-1ivc"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def pandas_clean_csv(chunk):\n",
        "    appended_data = []\n",
        "    for filename in chunk:\n",
        "        # Read only necessary columns\n",
        "        df = pd.read_csv(filename, skiprows=1,dtype=str, names=range(131), keep_default_na=False,  index_col=False)\n",
        "        df = df.iloc[:, :53]\n",
        "        df = df.rename(columns={1: 'DISPATCH', 3: 'VERSION'}).query('DISPATCH==\"DUNIT\" and VERSION==\"3\"')\n",
        "        df.columns = df.iloc[0]\n",
        "        df = df[1:]\n",
        "        df = df.rename(columns={'3': 'version', 'DUNIT': 'UNIT'})\n",
        "        df = df.drop(columns=['I'])\n",
        "        df.drop([\"\"], axis=1, inplace=True)\n",
        "        df['SETTLEMENTDATE'] = pd.to_datetime(df['SETTLEMENTDATE'])\n",
        "        columns = list(set(df.columns) - {'SETTLEMENTDATE', 'DUID', 'UNIT'})\n",
        "        df[columns] = df[columns].apply(pd.to_numeric, errors='coerce', dtype_backend='pyarrow')\n",
        "        df['year'] = df['SETTLEMENTDATE'].dt.year\n",
        "        appended_data.append(df)\n",
        "    appended_data = pd.concat(appended_data, ignore_index=True)\n",
        "    xx= pa.Table.from_pandas(appended_data,preserve_index=False)\n",
        "    write_deltalake(output_path + 'pandas',xx, mode=\"overwrite\", partition_by=['year'])\n",
        "    return \"done\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ccb8790-3d51-44a7-bb4d-4a79c50b8c1d",
      "metadata": {
        "id": "8ccb8790-3d51-44a7-bb4d-4a79c50b8c1d"
      },
      "source": [
        "#  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QjvU3_McjZaj",
      "metadata": {
        "id": "QjvU3_McjZaj"
      },
      "source": [
        "# Run Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "OaNbIPEK8y46",
      "metadata": {
        "id": "OaNbIPEK8y46"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def duplicate_data(Destination,total_files):\n",
        "  if not os.path.exists(Destination):\n",
        "      print(f\"Error: The directory '{Destination}' does not exist.\")\n",
        "  files = [f for f in os.listdir(Destination) if os.path.isfile(os.path.join(Destination, f))]\n",
        "  if len(files) >= total_files:\n",
        "    print(\"all good, data already genereated \")\n",
        "  else:\n",
        "      files = [f for f in os.listdir(Destination) if os.path.isfile(os.path.join(Destination, f))]\n",
        "      for file in files:\n",
        "          file_path = os.path.join(Destination, file)\n",
        "          name, ext = os.path.splitext(file)\n",
        "\n",
        "          for i in range(1, int(total_files/60) + 1):\n",
        "              new_file = os.path.join(Destination, f\"{name}_copy{i}{ext}\")\n",
        "              shutil.copy(file_path, new_file)\n",
        "\n",
        "      print(f\"Successfully duplicated files \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "PccFouvE6N9w",
      "metadata": {
        "id": "PccFouvE6N9w"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def run_test(engine,files_to_upload_full_Path,total_files):\n",
        "    results = pd.DataFrame(columns=['time','Engine','total_files','duration','core'])\n",
        "    start_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    start = time.time()\n",
        "    eval(f\"{engine}_clean_csv(files_to_upload_full_Path)\")\n",
        "    print(f'{engine} :' + str(time.time()-start))\n",
        "    arrow_table = pa.Table.from_pandas(pd.DataFrame([[start_time,engine,total_files,time.time()-start,core]], columns=results.columns))\n",
        "    write_deltalake(performance_result,arrow_table, mode=\"append\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U2rmLklGSQKe",
      "metadata": {
        "id": "U2rmLklGSQKe"
      },
      "outputs": [],
      "source": [
        "download(\"https://nemweb.com.au/Reports/Current/Daily_Reports/\",Source)\n",
        "unzip(Source,Destination)\n",
        "duplicate_data(Destination,total_files)\n",
        "list_files=[os.path.basename(x) for x in glob.glob(Destination+'*.CSV')]\n",
        "files_to_upload_full_Path = [Destination + i for i in list_files][:total_files]\n",
        "print(len(files_to_upload_full_Path))\n",
        "for engine in ['daft','duckdb',\"chdb\",'polars','datafusion','pyarrow','pandas']:\n",
        " try:\n",
        "  run_test(engine,files_to_upload_full_Path,total_files)\n",
        " except Exception as e:\n",
        "  print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0289b68-517a-4eb7-977c-298eea26d6de",
      "metadata": {
        "id": "f0289b68-517a-4eb7-977c-298eea26d6de"
      },
      "outputs": [],
      "source": [
        "vCPU = str(cpu_count()) + \" vCPU\"\n",
        "mem=round(virtual_memory().total/(1024 * 1024 * 1024),0)\n",
        "runtime = vCPU+' '+str(mem)+'GB'+ f', Transform {total_files} CSV files and save as Delta Table,Run date: '+ str(time.strftime(\"%Y-%m-%d\"))\n",
        "result = duckdb.sql(f\"\"\"\n",
        "            with raw as (select Engine,core, avg(duration) as duration from delta_scan('{performance_result}') where total_files = {total_files} group by all),\n",
        "            result as (select Engine,core, round(sum(duration)/60,2) as duration from raw group by all order by duration)\n",
        "            pivot result on Engine using avg(duration) order by core\n",
        "                    \"\"\").df()\n",
        "ax = result.plot.bar(rot=0,x='core',title=runtime,ylabel='Duration in Minutes, Lower is Better',figsize=(18,8))\n",
        "for c in ax.containers:\n",
        "    ax.bar_label(c, label_type='edge')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ee4d52d4-761a-4e7b-993b-0889fda74a94"
      ],
      "provenance": []
    },
    "dependencies": {
      "environment": {},
      "lakehouse": {
        "default_lakehouse": "b5a8c0d6-c86f-4230-939f-5437f7ed1d87",
        "default_lakehouse_name": "ETL",
        "default_lakehouse_workspace_id": "450bf196-431f-463f-9316-2d1ce1da98db"
      }
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "notebook_environment": {},
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "save_output": true,
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {},
        "enableDebugMode": false
      }
    },
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}