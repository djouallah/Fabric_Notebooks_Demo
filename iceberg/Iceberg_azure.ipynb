{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a059bb73",
      "metadata": {},
      "outputs": [],
      "source": [
        "import configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read(\"C:/KV/variable.ini\")\n",
        "postgresql_db               = config.get(\"myvars\", \"postgresql_db\")\n",
        "AZURE_STORAGE_ACCOUNT_KEY   = config.get(\"myvars\", \"AccountKey\")\n",
        "azure_storage_tenant_id     = config.get(\"myvars\", \"tenantId\")\n",
        "CONNECTION_STRING           = config.get(\"myvars\", \"CONNECTION_STRING\")\n",
        "account_name                = config.get(\"myvars\", \"account_name\")\n",
        "table_location              = config.get(\"myvars\", \"table_location_azure\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z9VP1eIxjnWf",
      "metadata": {
        "id": "Z9VP1eIxjnWf"
      },
      "source": [
        "**<mark>Install Package</mark>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Hz6YXXBgv88d",
      "metadata": {
        "collapsed": true,
        "id": "Hz6YXXBgv88d"
      },
      "outputs": [],
      "source": [
        "#!pip install -q pyiceberg[adlfs]\n",
        "#!pip install -q boto3\n",
        "#!pip install -q getdaft\n",
        "#!pip install -q sqlalchemy\n",
        "#!pip install -q psycopg2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a98d5916",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyiceberg.catalog import load_catalog\n",
        "from pyiceberg.catalog.sql import SqlCatalog\n",
        "from pyiceberg.io.fsspec import FsspecFileIO\n",
        "import duckdb\n",
        "import os\n",
        "from   datetime import datetime\n",
        "import glob\n",
        "import os\n",
        "from   psutil import *\n",
        "import re \n",
        "import requests\n",
        "from   shutil import unpack_archive\n",
        "from   urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6QGCr5FwpzSN",
      "metadata": {
        "id": "6QGCr5FwpzSN"
      },
      "source": [
        "**<mark>Connect to the Catalog</mark>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "nATAZIr6eAHA",
      "metadata": {
        "id": "nATAZIr6eAHA"
      },
      "outputs": [],
      "source": [
        "def connect_catalog():\n",
        "      catalog = SqlCatalog(\n",
        "      \"default\",\n",
        "      **{\n",
        "          \"uri\"                : postgresql_db,\n",
        "          \"adlfs.account-name\" : account_name ,\n",
        "          \"adlfs.account-key\"  : AZURE_STORAGE_ACCOUNT_KEY,\n",
        "          \"adlfs.tenant-id\"    : azure_storage_tenant_id,\n",
        "          \"py-io-impl\"         : \"pyiceberg.io.fsspec.FsspecFileIO\",\n",
        "          \"legacy-current-snapshot-id\": True\n",
        "      },\n",
        "                        )\n",
        "      return catalog "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "GyYCFVwSxH7O",
      "metadata": {
        "id": "GyYCFVwSxH7O"
      },
      "outputs": [],
      "source": [
        "catalog  = connect_catalog()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1xGmG4tLxalR",
      "metadata": {
        "id": "1xGmG4tLxalR"
      },
      "outputs": [],
      "source": [
        "db = \"azure\"\n",
        "catalog.create_namespace_if_not_exists(db)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "lreeBWtGaOI9",
      "metadata": {
        "cellView": "form",
        "id": "lreeBWtGaOI9"
      },
      "outputs": [],
      "source": [
        "def get_table_files(db,table):\n",
        "    table = catalog.load_table(db+'.'+table)\n",
        "    zzz = table.scan(selected_fields=(\"file\", )).to_arrow_batch_reader()\n",
        "    table_files = duckdb.sql(f\"select distinct file  from  zzz \").df()['file'].tolist()\n",
        "    return table_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "28416977-0f39-41ca-aa84-9a5d42a6e519",
      "metadata": {
        "cellView": "form",
        "id": "28416977-0f39-41ca-aa84-9a5d42a6e519"
      },
      "outputs": [],
      "source": [
        "def download(url,Path,total_files):\n",
        "    if not os.path.exists(Path):\n",
        "      os.makedirs(Path, exist_ok=True)\n",
        "    result = urlopen(url).read().decode('utf-8')\n",
        "    pattern = re.compile(r'[\\w.]*.zip')\n",
        "    filelist1 = pattern.findall(result)\n",
        "    filelist_unique = dict.fromkeys(filelist1)\n",
        "    filelist =sorted(filelist_unique, reverse=True)\n",
        "    current =  [os.path.basename(x) for x in glob.glob(Path+'*.zip')]\n",
        "    files_to_upload = list(set(filelist) - set(current))\n",
        "    files_to_upload = list(dict.fromkeys(files_to_upload))[:total_files] \n",
        "    print(str(len(files_to_upload)) + ' New File Loaded')\n",
        "    if len(files_to_upload) != 0 :\n",
        "      for x in files_to_upload:\n",
        "           with requests.get(url+x, stream=True) as resp:\n",
        "            if resp.ok:\n",
        "              with open(f\"{Path}{x}\", \"wb\") as f:\n",
        "               for chunk in resp.iter_content(chunk_size=4096):\n",
        "                f.write(chunk)\n",
        "    return \"done\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9zzwvL21EJyq",
      "metadata": {
        "cellView": "form",
        "id": "9zzwvL21EJyq"
      },
      "outputs": [],
      "source": [
        "def unzip(Source, Destination):\n",
        "    if not os.path.exists(Destination):\n",
        "      os.makedirs(Destination, exist_ok=True)\n",
        "    filelist=[os.path.basename(x) for x in glob.glob(Source+'*.zip')]\n",
        "    ### checl the unzipped files already\n",
        "    current = [os.path.basename(x) for x in glob.glob(Destination+'*.CSV')]\n",
        "    current = [w.replace('.CSV','.zip') for w in current]\n",
        "    #unzip only the delta\n",
        "    files_to_upload = list(set(filelist) - set(current))\n",
        "    files_to_upload = list(dict.fromkeys(files_to_upload))\n",
        "    print(str(len(files_to_upload)) + ' New File uncompressed')\n",
        "    if len(files_to_upload) != 0 :\n",
        "      for x in files_to_upload:\n",
        "        unpack_archive(str(Source+x), str(Destination), 'zip')\n",
        "      return \"done\"\n",
        "    else:\n",
        "     return \"nothing to see here\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scnMCRIq0j6F",
      "metadata": {
        "id": "scnMCRIq0j6F"
      },
      "source": [
        "**<mark>Check files already Ingested</mark>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "TZ29HGpXx6BN",
      "metadata": {
        "cellView": "form",
        "id": "TZ29HGpXx6BN"
      },
      "outputs": [],
      "source": [
        "def get_Path(Source,Destination):\n",
        " if catalog.table_exists(db+\".\"+Destination):\n",
        "  existing_files = get_table_files(db,Destination)\n",
        " else:\n",
        "  existing_files = []\n",
        " print(len(existing_files))\n",
        " filelist_csv = [os.path.basename(x) for x in glob.glob(Source+'*.CSV')]\n",
        " files_to_upload = list(set(filelist_csv) - set(existing_files))\n",
        " files_to_upload = list(dict.fromkeys(files_to_upload))\n",
        " files_to_upload_full_Path = [Source + i for i in files_to_upload]\n",
        " return files_to_upload_full_Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "OjmPm7hXuAZh",
      "metadata": {
        "cellView": "form",
        "id": "OjmPm7hXuAZh"
      },
      "outputs": [],
      "source": [
        "def clean_scada(files_to_upload_full_Path):\n",
        "    raw =duckdb.sql(F\"\"\"from read_csv({files_to_upload_full_Path},\n",
        "    Skip=1,header =0,all_varchar=1,\n",
        "    columns={{\n",
        "    'I': 'VARCHAR','UNIT': 'VARCHAR','XX': 'VARCHAR','VERSION': 'VARCHAR','SETTLEMENTDATE': 'VARCHAR','RUNNO': 'VARCHAR',\n",
        "    'DUID': 'VARCHAR','INTERVENTION': 'VARCHAR','DISPATCHMODE': 'VARCHAR','AGCSTATUS': 'VARCHAR','INITIALMW': 'VARCHAR',\n",
        "    'TOTALCLEARED': 'VARCHAR','RAMPDOWNRATE': 'VARCHAR','RAMPUPRATE': 'VARCHAR','LOWER5MIN': 'VARCHAR',\n",
        "    'LOWER60SEC': 'VARCHAR','LOWER6SEC': 'VARCHAR','RAISE5MIN': 'VARCHAR','RAISE60SEC': 'VARCHAR',\n",
        "    'RAISE6SEC': 'VARCHAR','MARGINAL5MINVALUE': 'VARCHAR','MARGINAL60SECVALUE': 'VARCHAR',\n",
        "    'MARGINAL6SECVALUE': 'VARCHAR','MARGINALVALUE': 'VARCHAR','VIOLATION5MINDEGREE': 'VARCHAR',\n",
        "    'VIOLATION60SECDEGREE': 'VARCHAR','VIOLATION6SECDEGREE': 'VARCHAR','VIOLATIONDEGREE': 'VARCHAR',\n",
        "    'LOWERREG': 'VARCHAR','RAISEREG': 'VARCHAR','AVAILABILITY': 'VARCHAR','RAISE6SECFLAGS': 'VARCHAR',\n",
        "    'RAISE60SECFLAGS': 'VARCHAR','RAISE5MINFLAGS': 'VARCHAR','RAISEREGFLAGS': 'VARCHAR',\n",
        "    'LOWER6SECFLAGS': 'VARCHAR','LOWER60SECFLAGS': 'VARCHAR','LOWER5MINFLAGS': 'VARCHAR',\n",
        "    'LOWERREGFLAGS': 'VARCHAR','RAISEREGAVAILABILITY': 'VARCHAR','RAISEREGENABLEMENTMAX': 'VARCHAR',\n",
        "    'RAISEREGENABLEMENTMIN': 'VARCHAR','LOWERREGAVAILABILITY': 'VARCHAR','LOWERREGENABLEMENTMAX': 'VARCHAR',\n",
        "    'LOWERREGENABLEMENTMIN': 'VARCHAR','RAISE6SECACTUALAVAILABILITY': 'VARCHAR',\n",
        "    'RAISE60SECACTUALAVAILABILITY': 'VARCHAR','RAISE5MINACTUALAVAILABILITY': 'VARCHAR',\n",
        "    'RAISEREGACTUALAVAILABILITY': 'VARCHAR','LOWER6SECACTUALAVAILABILITY': 'VARCHAR',\n",
        "    'LOWER60SECACTUALAVAILABILITY': 'VARCHAR','LOWER5MINACTUALAVAILABILITY': 'VARCHAR','LOWERREGACTUALAVAILABILITY': 'VARCHAR'\n",
        "    }},\n",
        "    filename =1,null_padding = true,ignore_errors=1,auto_detect=false)\n",
        "    where I='D' and UNIT ='DUNIT' AND VERSION = '3'                  \"\"\")\n",
        "    columns = list(set(raw.columns) - {'SETTLEMENTDATE','DUID','I','filename','UNIT'})\n",
        "    exprs = [\n",
        "      duckdb.ColumnExpression(x).cast(duckdb.typing.DOUBLE).alias(x)\n",
        "      for x in columns\n",
        "    ]\n",
        "    rel2 = raw.select('SETTLEMENTDATE','DUID','I','filename','UNIT',*exprs)\n",
        "    final=duckdb.sql(\"\"\" select *exclude(SETTLEMENTDATE,I,XX,filename),cast (SETTLEMENTDATE as timestamp) as SETTLEMENTDATE,\n",
        "    parse_filename(filename) as file,isoyear (cast (SETTLEMENTDATE as timestamp)) as YEAR  from rel2  \"\"\")\n",
        "    ####################\n",
        "    return final.arrow()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3GAhzSxKtgfD",
      "metadata": {
        "cellView": "form",
        "id": "3GAhzSxKtgfD"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def clean_price(files_to_upload_full_Path):\n",
        "  raw =duckdb.sql(F\"\"\"from read_csv({files_to_upload_full_Path},\n",
        "  Skip=1,header =0,all_varchar=1,\n",
        "  columns={{\n",
        "  'I': 'VARCHAR','UNIT': 'VARCHAR','XX': 'VARCHAR','VERSION': 'VARCHAR','SETTLEMENTDATE': 'VARCHAR','RUNNO': 'VARCHAR',\n",
        "  'REGIONID': 'VARCHAR','INTERVENTION': 'VARCHAR','RRP': 'VARCHAR','EEP': 'VARCHAR','ROP': 'VARCHAR','APCFLAG': 'VARCHAR',\n",
        "  'MARKETSUSPENDEDFLAG': 'VARCHAR','TOTALDEMAND': 'VARCHAR','DEMANDFORECAST': 'VARCHAR','DISPATCHABLEGENERATION': 'VARCHAR',\n",
        "  'DISPATCHABLELOAD': 'VARCHAR','NETINTERCHANGE': 'VARCHAR','EXCESSGENERATION': 'VARCHAR','LOWER5MINDISPATCH': 'VARCHAR',\n",
        "  'LOWER5MINIMPORT': 'VARCHAR','LOWER5MINLOCALDISPATCH': 'VARCHAR','LOWER5MINLOCALPRICE': 'VARCHAR','LOWER5MINLOCALREQ': 'VARCHAR',\n",
        "  'LOWER5MINPRICE': 'VARCHAR','LOWER5MINREQ': 'VARCHAR','LOWER5MINSUPPLYPRICE': 'VARCHAR','LOWER60SECDISPATCH': 'VARCHAR','LOWER60SECIMPORT': 'VARCHAR',\n",
        "  'LOWER60SECLOCALDISPATCH': 'VARCHAR','LOWER60SECLOCALPRICE': 'VARCHAR','LOWER60SECLOCALREQ': 'VARCHAR','LOWER60SECPRICE': 'VARCHAR',\n",
        "  'LOWER60SECREQ': 'VARCHAR','LOWER60SECSUPPLYPRICE': 'VARCHAR','LOWER6SECDISPATCH': 'VARCHAR','LOWER6SECIMPORT': 'VARCHAR',\n",
        "  'LOWER6SECLOCALDISPATCH': 'VARCHAR','LOWER6SECLOCALPRICE': 'VARCHAR','LOWER6SECLOCALREQ': 'VARCHAR','LOWER6SECPRICE': 'VARCHAR',\n",
        "  'LOWER6SECREQ': 'VARCHAR','LOWER6SECSUPPLYPRICE': 'VARCHAR','RAISE5MINDISPATCH': 'VARCHAR','RAISE5MINIMPORT': 'VARCHAR',\n",
        "  'RAISE5MINLOCALDISPATCH': 'VARCHAR','RAISE5MINLOCALPRICE': 'VARCHAR','RAISE5MINLOCALREQ': 'VARCHAR','RAISE5MINPRICE': 'VARCHAR',\n",
        "  'RAISE5MINREQ': 'VARCHAR','RAISE5MINSUPPLYPRICE': 'VARCHAR','RAISE60SECDISPATCH': 'VARCHAR','RAISE60SECIMPORT': 'VARCHAR',\n",
        "  'RAISE60SECLOCALDISPATCH': 'VARCHAR','RAISE60SECLOCALPRICE': 'VARCHAR','RAISE60SECLOCALREQ': 'VARCHAR','RAISE60SECPRICE': 'VARCHAR',\n",
        "  'RAISE60SECREQ': 'VARCHAR','RAISE60SECSUPPLYPRICE': 'VARCHAR','RAISE6SECDISPATCH': 'VARCHAR','RAISE6SECIMPORT': 'VARCHAR',\n",
        "  'RAISE6SECLOCALDISPATCH': 'VARCHAR','RAISE6SECLOCALPRICE': 'VARCHAR','RAISE6SECLOCALREQ': 'VARCHAR','RAISE6SECPRICE': 'VARCHAR',\n",
        "  'RAISE6SECREQ': 'VARCHAR','RAISE6SECSUPPLYPRICE': 'VARCHAR','AGGREGATEDISPATCHERROR': 'VARCHAR','AVAILABLEGENERATION': 'VARCHAR',\n",
        "  'AVAILABLELOAD': 'VARCHAR','INITIALSUPPLY': 'VARCHAR','CLEAREDSUPPLY': 'VARCHAR','LOWERREGIMPORT': 'VARCHAR','LOWERREGLOCALDISPATCH': 'VARCHAR',\n",
        "  'LOWERREGLOCALREQ': 'VARCHAR','LOWERREGREQ': 'VARCHAR','RAISEREGIMPORT': 'VARCHAR','RAISEREGLOCALDISPATCH': 'VARCHAR','RAISEREGLOCALREQ': 'VARCHAR',\n",
        "  'RAISEREGREQ': 'VARCHAR','RAISE5MINLOCALVIOLATION': 'VARCHAR','RAISEREGLOCALVIOLATION': 'VARCHAR','RAISE60SECLOCALVIOLATION': 'VARCHAR',\n",
        "  'RAISE6SECLOCALVIOLATION': 'VARCHAR','LOWER5MINLOCALVIOLATION': 'VARCHAR','LOWERREGLOCALVIOLATION': 'VARCHAR','LOWER60SECLOCALVIOLATION': 'VARCHAR',\n",
        "  'LOWER6SECLOCALVIOLATION': 'VARCHAR','RAISE5MINVIOLATION': 'VARCHAR','RAISEREGVIOLATION': 'VARCHAR','RAISE60SECVIOLATION': 'VARCHAR',\n",
        "  'RAISE6SECVIOLATION': 'VARCHAR','LOWER5MINVIOLATION': 'VARCHAR','LOWERREGVIOLATION': 'VARCHAR','LOWER60SECVIOLATION': 'VARCHAR',\n",
        "  'LOWER6SECVIOLATION': 'VARCHAR','RAISE6SECRRP': 'VARCHAR','RAISE6SECROP': 'VARCHAR','RAISE6SECAPCFLAG': 'VARCHAR','RAISE60SECRRP': 'VARCHAR',\n",
        "  'RAISE60SECROP': 'VARCHAR','RAISE60SECAPCFLAG': 'VARCHAR','RAISE5MINRRP': 'VARCHAR','RAISE5MINROP': 'VARCHAR','RAISE5MINAPCFLAG': 'VARCHAR',\n",
        "  'RAISEREGRRP': 'VARCHAR','RAISEREGROP': 'VARCHAR','RAISEREGAPCFLAG': 'VARCHAR','LOWER6SECRRP': 'VARCHAR','LOWER6SECROP': 'VARCHAR',\n",
        "  'LOWER6SECAPCFLAG': 'VARCHAR','LOWER60SECRRP': 'VARCHAR','LOWER60SECROP': 'VARCHAR','LOWER60SECAPCFLAG': 'VARCHAR','LOWER5MINRRP': 'VARCHAR',\n",
        "  'LOWER5MINROP': 'VARCHAR','LOWER5MINAPCFLAG': 'VARCHAR','LOWERREGRRP': 'VARCHAR','LOWERREGROP': 'VARCHAR','LOWERREGAPCFLAG': 'VARCHAR',\n",
        "  'RAISE6SECACTUALAVAILABILITY': 'VARCHAR','RAISE60SECACTUALAVAILABILITY': 'VARCHAR','RAISE5MINACTUALAVAILABILITY': 'VARCHAR',\n",
        "  'RAISEREGACTUALAVAILABILITY': 'VARCHAR','LOWER6SECACTUALAVAILABILITY': 'VARCHAR','LOWER60SECACTUALAVAILABILITY': 'VARCHAR',\n",
        "  'LOWER5MINACTUALAVAILABILITY': 'VARCHAR','LOWERREGACTUALAVAILABILITY': 'VARCHAR','LORSURPLUS': 'VARCHAR','LRCSURPLUS': 'VARCHAR',\n",
        "  }},\n",
        "  filename =1,null_padding = true,ignore_errors=1,auto_detect=false)\n",
        "  where I='D' and UNIT ='DREGION' AND VERSION = 3\n",
        "                  \"\"\")\n",
        "  columns = list(set(raw.columns) - {'SETTLEMENTDATE','REGIONID','I','filename','UNIT'})\n",
        "  exprs = [\n",
        "    duckdb.ColumnExpression(x).cast(duckdb.typing.DOUBLE).alias(x)\n",
        "    for x in columns\n",
        "  ]\n",
        "  rel2 = raw.select('SETTLEMENTDATE','REGIONID','I','filename','UNIT',*exprs)\n",
        "  final=duckdb.sql(\"\"\" select *exclude(SETTLEMENTDATE,I,XX,'filename'),cast (SETTLEMENTDATE as timestamp) as SETTLEMENTDATE,\n",
        "   cast(SETTLEMENTDATE as date) as date,\n",
        "   parse_filename(filename) as file,\n",
        "   0 as PRIORITY ,\n",
        "   isoyear (cast (SETTLEMENTDATE as timestamp)) as YEAR  from rel2  \"\"\")\n",
        "  return final.arrow()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VHevvlcgAyXT",
      "metadata": {
        "id": "VHevvlcgAyXT"
      },
      "source": [
        "**<mark>Fact Tables</mark>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "gr5m9tMWEmfc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646,
          "referenced_widgets": [
            "dcd0e62915164ecfbb9d4be35dcd13ee",
            "1fca409cd25f41bfa979f06be7888324",
            "a516c4306c6b4fa0a88e9abcaae8ddf5"
          ]
        },
        "collapsed": true,
        "id": "gr5m9tMWEmfc",
        "outputId": "7ce58361-6a39-4105-e4ca-7d728b8f160e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 New File Loaded\n",
            "0 New File uncompressed\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d1e2cb095164390ba6c7c7746f34f7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "94\n",
            "scada loaded already\n",
            "94\n",
            "price loaded already\n"
          ]
        }
      ],
      "source": [
        "Nbr_Files_to_Download = 60\n",
        "Source                = \"/lakehouse/default/Files/0_Source/ARCHIVE/Daily_Reports/\"\n",
        "Destination           = \"/lakehouse/default/Files/1_Transform/CSV/Daily_Reports/\"\n",
        "while True:\n",
        "    download(\"https://nemweb.com.au/Reports/Current/Daily_Reports/\", Source, Nbr_Files_to_Download)\n",
        "    unzip(Source, Destination)\n",
        "    processed = False\n",
        "    for tbl in ['scada', 'price']:\n",
        "        catalog = connect_catalog()\n",
        "        files_to_upload_full_Path = get_Path(Destination, tbl)\n",
        "        if len(files_to_upload_full_Path) > 0:\n",
        "            df = eval(f\"clean_{tbl}(files_to_upload_full_Path)\")\n",
        "            catalog.create_table_if_not_exists(f'{db}.{tbl}', schema=df.schema, location=table_location + f'/{db}/{tbl}')\n",
        "            catalog.load_table(f'{db}.{tbl}').append(df)\n",
        "            print(f'{tbl} updated')\n",
        "            processed = True\n",
        "        else:\n",
        "            print(f'{tbl} loaded already')\n",
        "    if not processed:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JZuOf87fMyX2",
      "metadata": {
        "id": "JZuOf87fMyX2"
      },
      "source": [
        "**<mark>CALENDAR</mark>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c0Zo4V_1MeYa",
      "metadata": {
        "id": "c0Zo4V_1MeYa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\mdjouallah\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pyiceberg\\table\\__init__.py:651: UserWarning: Delete operation did not match any records\n",
            "  warnings.warn(\"Delete operation did not match any records\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "calendar created\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "tbl = db+\".\"+\"calendar\"\n",
        "if not catalog.table_exists(tbl):\n",
        "  df=duckdb.sql(\"\"\" SELECT cast(unnest(generate_series(cast ('2018-04-01' as date), cast('2024-12-31' as date), interval 1 day)) as date) as date,\n",
        "            EXTRACT(year from date) as year,\n",
        "            EXTRACT(month from date) as month\n",
        "            \"\"\").arrow()\n",
        "  catalog.create_table(tbl,schema=df.schema,location= table_location+f'/{db}/calendar')\n",
        "  catalog.load_table(tbl).overwrite(df)\n",
        "  print('calendar created')\n",
        "else:\n",
        "    print(\"table exist already\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_ai5wr-WWCwU",
      "metadata": {
        "id": "_ai5wr-WWCwU"
      },
      "source": [
        "**<mark>DUID</mark>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "RetY7JRvRXF1",
      "metadata": {
        "id": "RetY7JRvRXF1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d266c488fe2f4850995d3e9e716950bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "duid updated\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "import requests\n",
        "import duckdb\n",
        "DUID_Path = \"/lakehouse/default/Files/0_Source/Dimensions/DUID/\"\n",
        "import pathlib\n",
        "pathlib.Path(DUID_Path).mkdir(parents=True, exist_ok=True)\n",
        "url = \"https://www.aemo.com.au/-/media/Files/Electricity/NEM/Participant_Information/NEM-Registration-and-Exemption-List.xls\"\n",
        "s = requests.Session()\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36'}\n",
        "r = s.get(url,headers=headers)\n",
        "r.content\n",
        "output = open(DUID_Path+\"NEM-Registration-and-Exemption-List.xls\", 'wb')\n",
        "output.write(r.content)\n",
        "output.close()\n",
        "duckdb.sql(f\"\"\"\n",
        "INSTALL spatial;\n",
        "LOAD spatial;\n",
        "create or replace table DUID_raw as SELECT Region,DUID,first(\"Fuel Source - Descriptor\") as FuelSourceDescriptor,first(Participant) as Participant\n",
        "FROM st_read('{DUID_Path}NEM-Registration-and-Exemption-List.xls', layer = 'PU and Scheduled Loads',open_options = ['HEADERS=FORCE'])\n",
        "group by all\n",
        "\"\"\")\n",
        "\n",
        "import requests\n",
        "dls = \"https://data.wa.aemo.com.au/datafiles/post-facilities/facilities.csv\"\n",
        "resp = requests.get(dls)\n",
        "output = open(DUID_Path+\"facilities_WA.csv\", 'wb')\n",
        "output.write(resp.content)\n",
        "output.close()\n",
        "\n",
        "duckdb.sql(f\"\"\" create or replace view x as select 'WA1' as Region  , \"Facility Code\" as DUID ,\"Participant Name\" as Participant from read_csv_auto('{DUID_Path}facilities_WA.csv')\"\"\")\n",
        "\n",
        "duckdb.sql(\"\"\"select x.Region,x.DUID, TECHNOLOGY as FuelSourceDescriptor,Participant from x\n",
        "  left join (select * FROM read_csv_auto('https://github.com/djouallah/aemo_fabric/raw/main/WA_ENERGY.csv',header=1)) as z\n",
        "  on x.duid=z.duid \"\"\").to_view('DUID_WA')\n",
        "\n",
        "df=duckdb.sql(f\"\"\" with xx as (select * from DUID_raw union BY NAME select * from DUID_WA)\n",
        "                select trim(DUID) as DUID,min(Region) as Region, min(FuelSourceDescriptor) as FuelSourceDescriptor,min(Participant) as Participant from xx group by all\n",
        "                \"\"\").arrow()\n",
        "catalog.create_table_if_not_exists(db+\".duid\",schema=df.schema,location= table_location+f'/{db}/duid')\n",
        "catalog.load_table(db+\".duid\").overwrite(df)\n",
        "print('duid updated')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rjvDnVFuk7TE",
      "metadata": {
        "id": "rjvDnVFuk7TE"
      },
      "source": [
        "# Read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "JcJaH3sE7Al7",
      "metadata": {
        "id": "JcJaH3sE7Al7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['scada', 'price', 'calendar', 'duid']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tables_list = [t[1] for t in catalog.list_tables(db)]\n",
        "tables_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "KPNLh9HFlGho",
      "metadata": {
        "id": "KPNLh9HFlGho"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 1.19 s\n",
            "Wall time: 3.31 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "duckdb.sql(f\"\"\"\n",
        "SET azure_transport_option_type = 'curl';\n",
        "CREATE or replace SECRET secret1 ( TYPE AZURE,account_name {account_name}, CONNECTION_STRING '{CONNECTION_STRING}');\n",
        "INSTALL iceberg;LOAD iceberg;\n",
        "\"\"\")\n",
        "for tbl in tables_list:\n",
        " duckdb.sql(f\"\"\"\n",
        " SET  VARIABLE last_snapshot_{tbl} = (select max(file) from glob ('{table_location}/{db}/{tbl}/metadata/*.metadata.json')) ;\n",
        " create or replace view {tbl} as select * from  iceberg_scan(getvariable('last_snapshot_{tbl}'))\n",
        " \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "BiQELamtpdws",
      "metadata": {
        "id": "BiQELamtpdws"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "┌──────────────┐\n",
              "│ count_star() │\n",
              "│    int64     │\n",
              "├──────────────┤\n",
              "│     13551947 │\n",
              "└──────────────┘"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duckdb.sql(\"\"\" select count(*) from scada \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "YfQXe_2lrz9N",
      "metadata": {
        "id": "YfQXe_2lrz9N"
      },
      "outputs": [],
      "source": [
        "import daft\n",
        "catalog  = connect_catalog()\n",
        "df = daft.read_iceberg(catalog.load_table(\"azure.scada\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "KevG9TkZrKSu",
      "metadata": {
        "id": "KevG9TkZrKSu"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "908378779b0b41be9508cc728181af15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ScanWithTask-Aggregate [Stage:2]:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad686234141e4d85a06e08798974854f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ReduceMerge-Aggregate-Project-Project [Stage:1]:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "13551947"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.count_rows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "language": "python",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "notebook_environment": {},
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "save_output": true,
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "conf": {},
        "enableDebugMode": false
      }
    },
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    },
    "trident": {
      "environment": {},
      "lakehouse": {
        "default_lakehouse": "b3878d44-fec5-4682-899b-e338043109d0",
        "default_lakehouse_name": "aemo",
        "default_lakehouse_workspace_id": "21564e48-ed2c-42e8-9ea4-64d83285ba45",
        "known_lakehouses": [
          {
            "id": "b3878d44-fec5-4682-899b-e338043109d0"
          }
        ]
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1fca409cd25f41bfa979f06be7888324": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "auto"
          }
        },
        "a516c4306c6b4fa0a88e9abcaae8ddf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": "black",
            "description_width": ""
          }
        },
        "dcd0e62915164ecfbb9d4be35dcd13ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fca409cd25f41bfa979f06be7888324",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a516c4306c6b4fa0a88e9abcaae8ddf5",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
