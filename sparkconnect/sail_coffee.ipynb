{"cells":[{"cell_type":"code","source":["!pip install    -q pyspark-client\n","!pip install    -q pysail\n","import sys\n","sys.exit(0)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"47cbe091-8273-43f8-b91a-cc0ab399fcf6","normalized_state":"finished","queued_time":"2025-08-08T15:24:33.2435266Z","session_start_time":"2025-08-08T15:24:33.2443447Z","execution_start_time":"2025-08-08T15:24:37.9697618Z","execution_finish_time":"2025-08-08T15:24:53.0642207Z","parent_msg_id":"8d70771d-eda5-43de-b13f-4bda64ec24d2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["sys.exit called with value 0. The interpreter will be restarted.\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"jupyter":{"source_hidden":false}},"id":"8695b0c0-65b4-4734-bc51-2534be2e09a2"},{"cell_type":"code","source":["from   pysail.spark import SparkConnectServer\n","from   pyspark.sql import SparkSession , functions as F\n","server = SparkConnectServer()\n","server.start()\n","_, port = server.listening_address\n","spark = SparkSession.builder.remote(f\"sc://localhost:{port}\").getOrCreate()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"47cbe091-8273-43f8-b91a-cc0ab399fcf6","normalized_state":"finished","queued_time":"2025-08-08T15:24:33.2458377Z","session_start_time":null,"execution_start_time":"2025-08-08T15:24:53.0654765Z","execution_finish_time":"2025-08-08T15:24:56.0394406Z","parent_msg_id":"1792ff21-3f9c-4341-a791-cec3abc82b9b"}},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"96ab8bbd-b50b-4c87-a16f-eb6de6434d2d"},{"cell_type":"code","source":["###  original script https://github.com/JosueBogran/coffeeshopdatagenerator\n","#Note that number of rows below is really number of orders. Every order can have 1 - 5 lines. So generating as-is will have some variability on the actualy row_count\n","NUM_ROWS = 1_000_000\n","NUM_PARTITIONS = 10_000\n","# Pre-build arrays of Product_IDs as Spark Column objects:\n","product_list_summer = [F.lit(x) for x in [5, 6]]\n","product_list_non_summer = [F.lit(x) for x in [1,2,3,4,7,8,9,10]]\n","product_list_others = [F.lit(x) for x in [11,12,13]]\n","\n","arr_summer = F.array(*product_list_summer)\n","arr_non_summer = F.array(*product_list_non_summer)\n","arr_others = F.array(*product_list_others)\n","\n","# ------------------------------------------------------------------------------\n","# 1) Generate a base DataFrame with one row per order:\n","#    - We pick Location_ID once per order so all lines share the same location.\n","base_df = (\n","    spark.range(start=0, end=NUM_ROWS, numPartitions=NUM_PARTITIONS)\n","    # Generate a unique order ID from id + random\n","    .withColumn(\"Order_ID\", F.hex(F.md5(F.concat_ws(\"_\", F.col(\"id\").cast(\"string\"), F.rand()))))\n","    \n","    # Uniform random for date in [2023-01-01 .. 2024-12-31]\n","    .withColumn(\"rand_date\", F.rand())\n","    .withColumn(\"Order_Date\", F.expr(\"date_add(to_date('2023-01-01'), cast(rand_date * 730 as int))\"))\n","    \n","    # Derive month & season\n","    .withColumn(\"Month\", F.month(\"Order_Date\"))\n","    .withColumn(\n","        \"Season\",\n","        F.when(F.col(\"Month\").isin([12,1,2]), \"winter\")\n","         .when(F.col(\"Month\").isin([3,4,5]), \"spring\")\n","         .when(F.col(\"Month\").isin([6,7,8]), \"summer\")\n","         .otherwise(\"fall\")\n","    )\n","    \n","    # Number of lines distribution (60%=1, 30%=2, 5%=3, 1%=4, 4%=5)\n","    .withColumn(\"rand_lines\", F.rand())\n","    .withColumn(\n","        \"Num_Lines\",\n","        F.when(F.col(\"rand_lines\") < 0.60, 1)\n","         .when(F.col(\"rand_lines\") < 0.90, 2)\n","         .when(F.col(\"rand_lines\") < 0.95, 3)\n","         .when(F.col(\"rand_lines\") < 0.96, 4)\n","         .otherwise(5)\n","    )\n","\n","    # Time-of-Day distribution (Morning=50%, Afternoon=30%, Night=20%)\n","    .withColumn(\"rand_tod\", F.rand())\n","    .withColumn(\n","        \"Time_Of_Day\",\n","        F.when(F.col(\"rand_tod\") < 0.50, \"Morning\")\n","         .when(F.col(\"rand_tod\") < 0.80, \"Afternoon\")\n","         .otherwise(\"Night\")\n","    )\n","\n","    # Location_ID with variability (assigned once per order)\n","    .withColumn(\"rand_loc\", F.rand())\n","    .withColumn(\n","        \"Location_ID\",\n","        F.when(F.col(\"rand_loc\") < 0.30, F.floor(F.rand() * 50) + 1)       # 1..50   (30%)\n","         .when(F.col(\"rand_loc\") < 0.80, F.floor(F.rand() * 150) + 51)     # 51..200 (50%)\n","         .when(F.col(\"rand_loc\") < 0.95, F.floor(F.rand() * 300) + 201)    # 201..500 (15%)\n","         .otherwise(F.floor(F.rand() * 500) + 501)                         # 501..1000 (5%)\n","    )\n","\n","    # Drop intermediate columns\n","    .drop(\"rand_date\", \"rand_lines\", \"rand_tod\", \"rand_loc\")\n",")\n","\n","# ------------------------------------------------------------------------------\n","# 2) Explode out by Num_Lines (one row per line_item), carrying forward the base fields.\n","exploded_df = (\n","    base_df\n","    .withColumn(\"line_array\", F.expr(\"sequence(1, Num_Lines)\"))\n","    .select(\"*\", F.posexplode(\"line_array\").alias(\"Line_Pos\", \"Line_Val\"))\n","    .drop(\"line_array\", \"Num_Lines\")\n","    .withColumn(\"Order_Line_ID\", F.concat(F.col(\"Order_ID\"), F.lit(\"_\"), F.col(\"Line_Val\").cast(\"string\")))\n",")\n","\n","# ------------------------------------------------------------------------------\n","# 3) Add line-level random columns:\n","#    - We do new random draws at the line level for Quantity, Discount, and Product.\n","line_df = (\n","    exploded_df\n","    # Quantity (1–5, skewed)\n","    .withColumn(\"rand_qty\", F.rand())\n","    .withColumn(\n","        \"Quantity\",\n","        F.when(F.col(\"rand_qty\") < 0.40, 1)\n","         .when(F.col(\"rand_qty\") < 0.70, 2)\n","         .when(F.col(\"rand_qty\") < 0.85, 3)\n","         .when(F.col(\"rand_qty\") < 0.95, 4)\n","         .otherwise(5)\n","    )\n","    # Discount (80% = 0, else 1–15)\n","    .withColumn(\"rand_disc\", F.rand())\n","    .withColumn(\n","        \"Discount_Rate\",\n","        F.when(F.col(\"rand_disc\") < 0.80, 0)\n","         .otherwise(F.floor(F.rand() * 15 + 1))\n","    )\n","    # Product distribution\n","    .withColumn(\"rand_prod\", F.rand())\n","    .withColumn(\n","        \"Product_ID\",\n","        F.when(\n","            F.col(\"Season\") == \"summer\",\n","            # summer => (5,6)=40%, (1..4,7..10)=50%, (11..13)=10%\n","            F.when(F.col(\"rand_prod\") < 0.40,\n","                   F.element_at(arr_summer, 1 + F.floor(F.rand() * 2).cast(\"int\")))\n","             .when(F.col(\"rand_prod\") < 0.90,\n","                   F.element_at(arr_non_summer, 1 + F.floor(F.rand() * 8).cast(\"int\")))\n","             .otherwise(F.element_at(arr_others, 1 + F.floor(F.rand() * 3).cast(\"int\")))\n","        ).otherwise(\n","            # non-summer => (1..4,7..10)=70%, (5,6)=10%, (11..13)=20%\n","            F.when(F.col(\"rand_prod\") < 0.70,\n","                   F.element_at(arr_non_summer, 1 + F.floor(F.rand() * 8).cast(\"int\")))\n","             .when(F.col(\"rand_prod\") < 0.80,\n","                   F.element_at(arr_summer, 1 + F.floor(F.rand() * 2).cast(\"int\")))\n","             .otherwise(F.element_at(arr_others, 1 + F.floor(F.rand() * 3).cast(\"int\")))\n","        )\n","    )\n","    .drop(\"rand_qty\", \"rand_disc\", \"rand_prod\")\n",")\n","\n","# ------------------------------------------------------------------------------\n","# 4) Final select & write the dataset\n","final_df = line_df.select(\n","    \"Order_ID\",\n","    \"Order_Line_ID\",\n","    \"Order_Date\",\n","    \"Time_Of_Day\",\n","    \"Season\",\n","    \"Month\",\n","    \"Location_ID\",   # same for all lines of the same order\n","    \"Product_ID\",\n","    \"Quantity\",\n","    \"Discount_Rate\"\n",")\n","delta_path = \"abfss://sparkconnect@onelake.dfs.fabric.microsoft.com/coffee.Lakehouse/Tables/coffee/baseee\"\n","final_df.write.format('delta')\\\n","        .mode(\"overwrite\")\\\n","        .save(delta_path)\n","\n","df = spark.read.format(\"delta\").load(delta_path)\n","df.createOrReplaceTempView(\"base\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"47cbe091-8273-43f8-b91a-cc0ab399fcf6","normalized_state":"finished","queued_time":"2025-08-08T15:24:33.2480958Z","session_start_time":null,"execution_start_time":"2025-08-08T15:24:56.0407013Z","execution_finish_time":"2025-08-08T15:25:13.0802457Z","parent_msg_id":"36de23e9-ef0a-4a20-9622-c6e448dc6b97"}},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"40f2c3c0-3a77-4cf0-a00e-f20d7cf8b47d"},{"cell_type":"code","source":["spark.sql(\" select count(*) from base \").show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"47cbe091-8273-43f8-b91a-cc0ab399fcf6","normalized_state":"finished","queued_time":"2025-08-08T15:24:33.2504502Z","session_start_time":null,"execution_start_time":"2025-08-08T15:25:13.0814279Z","execution_finish_time":"2025-08-08T15:25:14.4336569Z","parent_msg_id":"ee561777-9322-4c4e-9ce6-9685571112ca"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+--------+\n|count(1)|\n+--------+\n| 1650000|\n+--------+\n\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"46e1eb48-24e9-4521-a44e-87e9614aebef"}],"metadata":{"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"name":"jupyter","display_name":"Jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"26aa279d-a95e-40e1-b165-226080afb2a3"}],"default_lakehouse":"26aa279d-a95e-40e1-b165-226080afb2a3","default_lakehouse_name":"coffee","default_lakehouse_workspace_id":"b302bc31-ec70-48d0-9dea-16df4ca3f1d6"}}},"nbformat":4,"nbformat_minor":5}