{"cells":[{"cell_type":"code","source":["import os\n","import json\n","import uuid\n","import time\n","import duckdb\n","import pandas as pd\n","import pyarrow as pa\n","import pyarrow.parquet as pq\n","\n","def map_type_ducklake_to_spark(t):\n","    \"\"\"Maps DuckDB data types to their Spark SQL equivalents for the Delta schema.\"\"\"\n","    t = t.lower()\n","    if 'int' in t:\n","        return 'long' if '64' in t else 'integer'\n","    elif 'float' in t:\n","        return 'double' # Changed from 'float' to 'double'\n","    elif 'double' in t:\n","        return 'double'\n","    elif 'decimal' in t:\n","        return 'decimal(10,0)'\n","    elif 'bool' in t:\n","        return 'boolean'\n","    elif 'timestamp' in t:\n","        return 'timestamp'\n","    elif 'date' in t:\n","        return 'date'\n","    return 'string'\n","\n","def create_spark_schema_string(fields):\n","    \"\"\"Creates a JSON string for the Spark schema from a list of fields.\"\"\"\n","    return json.dumps({\"type\": \"struct\", \"fields\": fields})\n","\n","def get_spark_checkpoint_schema():\n","    \"\"\"Returns the PyArrow schema for a Delta Lake checkpoint file.\"\"\"\n","    return pa.schema([\n","        pa.field(\"protocol\", pa.struct([\n","            pa.field(\"minReaderVersion\", pa.int32(), False),\n","            pa.field(\"minWriterVersion\", pa.int32(), False)\n","        ]), nullable=True),\n","        pa.field(\"metaData\", pa.struct([\n","            pa.field(\"id\", pa.string()),\n","            pa.field(\"name\", pa.string()),\n","            pa.field(\"description\", pa.string()),\n","            pa.field(\"format\", pa.struct([\n","                pa.field(\"provider\", pa.string()),\n","                pa.field(\"options\", pa.map_(pa.string(), pa.string()))\n","            ])),\n","            pa.field(\"schemaString\", pa.string()),\n","            pa.field(\"partitionColumns\", pa.list_(pa.string())),\n","            pa.field(\"createdTime\", pa.int64()), # This is generally a long integer representing ms\n","            pa.field(\"configuration\", pa.map_(pa.string(), pa.string()))\n","        ]), nullable=True),\n","        pa.field(\"add\", pa.struct([\n","            pa.field(\"path\", pa.string()),\n","            pa.field(\"partitionValues\", pa.map_(pa.string(), pa.string())),\n","            pa.field(\"size\", pa.int64()),\n","            pa.field(\"modificationTime\", pa.timestamp('ms')), # Changed to timestamp('ms')\n","            pa.field(\"dataChange\", pa.bool_()),\n","            pa.field(\"stats\", pa.string(), nullable=True),\n","            pa.field(\"tags\", pa.map_(pa.string(), pa.string()), nullable=True)\n","        ]), nullable=True),\n","        pa.field(\"remove\", pa.struct([\n","            pa.field(\"path\", pa.string()),\n","            pa.field(\"deletionTimestamp\", pa.timestamp('ms')), # Changed to timestamp('ms')\n","            pa.field(\"dataChange\", pa.bool_())\n","        ]), nullable=True),\n","        pa.field(\"commitInfo\", pa.struct([\n","            pa.field(\"timestamp\", pa.timestamp('ms')), # Changed to timestamp('ms')\n","            pa.field(\"operation\", pa.string()),\n","            pa.field(\"operationParameters\", pa.map_(pa.string(), pa.string())),\n","            pa.field(\"isBlindAppend\", pa.bool_(), nullable=True),\n","            pa.field(\"engineInfo\", pa.string(), nullable=True)\n","        ]), nullable=True)\n","    ])\n","\n","def get_latest_delta_version_info(delta_log_path, con, table_id):\n","    \"\"\"\n","    Determines the latest Delta version exported and reconstructs the set of files\n","    that were part of that Delta version, based on the embedded DuckLake snapshot ID.\n","    Also retrieves the consistent metaData.id if available from version 0.\n","\n","    Returns (latest_delta_version, set_of_files_in_that_version, latest_ducklake_snapshot_id_in_delta, meta_id_from_delta_log).\n","    \"\"\"\n","    last_delta_version_idx = -1\n","    last_exported_ducklake_snapshot_id = None\n","    files_in_last_delta_version = set()\n","    meta_id_from_delta_log = None # This should be consistent for the table\n","\n","    # Collect all files ending with .json\n","    log_files = [f for f in os.listdir(delta_log_path) if f.endswith('.json')]\n","    \n","    if not log_files:\n","        return last_delta_version_idx, files_in_last_delta_version, last_exported_ducklake_snapshot_id, meta_id_from_delta_log\n","\n","    try:\n","        # Collect valid version numbers from file names\n","        found_versions = []\n","        for f_name in log_files:\n","            base_name = f_name.split('.')[0]\n","            # Check if filename starts with '0000' and consists entirely of digits\n","            if base_name.startswith('0000') and base_name.isdigit():\n","                found_versions.append(int(base_name))\n","\n","        if not found_versions:\n","            # No valid versioned log files found with the '0000' prefix\n","            return last_delta_version_idx, files_in_last_delta_version, last_exported_ducklake_snapshot_id, meta_id_from_delta_log\n","\n","        # Get the highest version index\n","        last_delta_version_idx = max(found_versions)\n","        last_log_file = os.path.join(delta_log_path, f\"{last_delta_version_idx:020d}.json\")\n","        \n","        # Attempt to read the last log file for commitInfo and metaData (if present)\n","        with open(last_log_file, 'r') as f:\n","            for line in f:\n","                try:\n","                    action = json.loads(line)\n","                    if 'commitInfo' in action:\n","                        commit_info = action['commitInfo']\n","                        if 'operationParameters' in commit_info and 'duckLakeSnapshotId' in commit_info['operationParameters']:\n","                            last_exported_ducklake_snapshot_id = int(commit_info['operationParameters']['duckLakeSnapshotId'])\n","                    if 'metaData' in action:\n","                        meta_id_from_delta_log = action['metaData'].get('id')\n","                except json.JSONDecodeError as e:\n","                    print(f\"ERROR: Failed to parse JSON line in {last_log_file}: {line.strip()}. Error: {e}\")\n","                except Exception as e:\n","                    print(f\"ERROR: Unexpected error processing line in {last_log_file}: {e}\")\n","        \n","        # If metaData.id was not found in the latest log file, try to get it from version 0\n","        if meta_id_from_delta_log is None:\n","            v0_log_file = os.path.join(delta_log_path, \"00000000000000000000.json\")\n","            if os.path.exists(v0_log_file):\n","                with open(v0_log_file, 'r') as v0f:\n","                    for v0_line in v0f:\n","                        try:\n","                            v0_action = json.loads(v0_line)\n","                            if 'metaData' in v0_action:\n","                                meta_id_from_delta_log = v0_action['metaData'].get('id')\n","                                break\n","                        except json.JSONDecodeError:\n","                            pass # Ignore parsing errors for v0 metadata, just try next line\n","\n","        # If a valid last_exported_ducklake_snapshot_id was found, reconstruct the files\n","        if last_exported_ducklake_snapshot_id is not None:\n","            file_rows = con.execute(f\"\"\"\n","                SELECT path FROM ducklake_data_file\n","                WHERE table_id = {table_id}\n","                  AND begin_snapshot <= {last_exported_ducklake_snapshot_id} AND (end_snapshot IS NULL OR end_snapshot > {last_exported_ducklake_snapshot_id})\n","            \"\"\").fetchall()\n","            files_in_last_delta_version = {path.lstrip('/') for path, in file_rows}\n","        else:\n","            print(f\"WARNING: 'duckLakeSnapshotId' not found or parsed from latest log ({last_log_file}). Cannot reconstruct previous Delta table state accurately for diffing.\")\n","\n","    except Exception as e:\n","        print(f\"ERROR: Unhandled exception in get_latest_delta_version_info for {delta_log_path}. Resetting state. Error: {e}\")\n","        last_delta_version_idx = -1 # Reset to -1 if there's an issue parsing or finding files\n","\n","    return last_delta_version_idx, files_in_last_delta_version, last_exported_ducklake_snapshot_id, meta_id_from_delta_log\n","\n","\n","def generate_latest_delta_log(db_path: str, checkpoint_interval: int = 3): # Changed default to 4\n","    \"\"\"\n","    Generates a Delta Lake transaction log for the LATEST state of each table in a DuckLake database.\n","    This creates incremental updates to Delta, not a full history.\n","    \n","    Args:\n","        db_path (str): The path to the DuckLake database file.\n","        checkpoint_interval (int): The interval at which to create checkpoint files.\n","    \"\"\"\n","    con = duckdb.connect(db_path, read_only=True)\n","    try:\n","        data_root = con.sql(\"SELECT value FROM ducklake_metadata WHERE key = 'data_path'\").fetchone()[0]\n","    except Exception as e:\n","        print(f\"‚ùå Could not determine data_path from ducklake_metadata. Error: {e}\")\n","        con.close()\n","        return\n","\n","    tables = con.sql(\"\"\"\n","        SELECT \n","            t.table_id, \n","            t.table_name, \n","            s.schema_name,\n","            t.path as table_path, \n","            s.path as schema_path\n","        FROM ducklake_table t\n","        JOIN ducklake_schema s USING(schema_id)\n","        WHERE t.end_snapshot IS NULL\n","    \"\"\").df()\n","\n","    for row in tables.itertuples():\n","        table_key = f\"{row.schema_name}.{row.table_name}\"\n","        table_root = os.path.join(data_root, row.schema_path, row.table_path)\n","        delta_log_path = os.path.join(table_root, \"_delta_log\")\n","        os.makedirs(delta_log_path, exist_ok=True)\n","        \n","        # 1. Get the LATEST DuckLake snapshot for this table\n","        latest_ducklake_snapshot_raw = con.execute(f\"\"\"\n","            SELECT MAX(begin_snapshot) FROM ducklake_data_file\n","            WHERE table_id = {row.table_id}\n","        \"\"\").fetchone()\n","        \n","        if not latest_ducklake_snapshot_raw or latest_ducklake_snapshot_raw[0] is None:\n","            print(f\"‚ö†Ô∏è {table_key}: No data files found in DuckLake, skipping Delta log generation.\")\n","            continue\n","        \n","        latest_ducklake_snapshot_id = latest_ducklake_snapshot_raw[0]\n","\n","        # 2. Determine the current state of the Delta table and next Delta version\n","        last_delta_version_idx, previously_exported_files, last_exported_ducklake_snapshot_id, existing_meta_id = \\\n","            get_latest_delta_version_info(delta_log_path, con, row.table_id)\n","        \n","        next_delta_version = last_delta_version_idx + 1\n","\n","        # Check if the Delta table is already up-to-date with the latest DuckLake snapshot\n","        if last_exported_ducklake_snapshot_id == latest_ducklake_snapshot_id:\n","            print(f\"‚úÖ {table_key}: Delta table already at latest DuckLake snapshot {latest_ducklake_snapshot_id} (Delta version {last_delta_version_idx}), skipping export.\")\n","            continue # Nothing to do, skip to next table\n","\n","        try:\n","            now = int(time.time() * 1000)\n","            log_file = os.path.join(delta_log_path, f\"{next_delta_version:020d}.json\")\n","            checkpoint_file = os.path.join(delta_log_path, f\"{next_delta_version:020d}.checkpoint.parquet\")\n","\n","            # Fetch all current files associated with the LATEST DuckLake snapshot\n","            file_rows_for_current_version = con.execute(f\"\"\"\n","                SELECT path, file_size_bytes FROM ducklake_data_file\n","                WHERE table_id = {row.table_id}\n","                  AND begin_snapshot <= {latest_ducklake_snapshot_id} AND (end_snapshot IS NULL OR end_snapshot > {latest_ducklake_snapshot_id})\n","            \"\"\").fetchall()\n","\n","            current_files_map = {}\n","            for path, size in file_rows_for_current_version:\n","                rel_path = path.lstrip('/')\n","                full_path = os.path.join(table_root, rel_path)\n","                mod_time = int(os.path.getmtime(full_path) * 1000) if os.path.exists(full_path) else now\n","                current_files_map[rel_path] = {\n","                    \"path\": rel_path, \"size\": size, \"modification_time\": mod_time,\n","                    \"stats\": json.dumps({\"numRecords\": None}) # Stats would require reading files\n","                }\n","            current_file_paths = set(current_files_map.keys())\n","\n","            added_files_data = []\n","            removed_files_paths = []\n","\n","            # Calculate the diff between the previous Delta state and the current latest DuckLake snapshot\n","            added_file_paths = current_file_paths - previously_exported_files\n","            removed_file_paths_set = previously_exported_files - current_file_paths\n","            \n","            added_files_data = [current_files_map[p] for p in added_file_paths]\n","            # removed_files_paths only need the path, not full dict\n","            removed_files_paths = list(removed_file_paths_set)\n","\n","            # If no changes and not the initial version 0, skip writing a log file\n","            # Version 0 should always be written if it's the first export, even if empty (e.g., empty table)\n","            if not added_files_data and not removed_files_paths and next_delta_version > 0:\n","                print(f\"ü§î {table_key}: No *detectable* changes between previous Delta state and latest DuckLake snapshot {latest_ducklake_snapshot_id}. Skipping new Delta log for version {next_delta_version}.\")\n","                continue # Skip to the next table\n","\n","            # Get schema for metadata (always from the latest DuckLake snapshot)\n","            columns = con.execute(f\"\"\"\n","                SELECT column_name, column_type FROM ducklake_column\n","                WHERE table_id = {row.table_id}\n","                  AND begin_snapshot <= {latest_ducklake_snapshot_id} AND (end_snapshot IS NULL OR end_snapshot > {latest_ducklake_snapshot_id})\n","                ORDER BY column_order\n","            \"\"\").fetchall()\n","\n","            with open(log_file, 'w') as f:\n","                # Write CommitInfo first, as it's always present\n","                commit_info = {\n","                    \"commitInfo\": {\n","                        \"timestamp\": now, \"operation\": \"WRITE\",\n","                        \"operationParameters\": {\n","                            \"mode\": \"Overwrite\", # This operation type means we are overwriting the table state\n","                            \"partitionBy\": \"[]\",\n","                            \"duckLakeSnapshotId\": str(latest_ducklake_snapshot_id) # Special field for DuckLake snapshot ID\n","                        },\n","                        \"isBlindAppend\": not removed_files_paths, # True if only adds, False if adds and removes\n","                        \"engineInfo\": \"DuckLake-Delta-Export-Latest\"\n","                    }\n","                }\n","\n","                # For Delta version 0, always add protocol and metadata\n","                if next_delta_version == 0:\n","                    schema_fields = [{\"name\": name, \"type\": map_type_ducklake_to_spark(typ), \"nullable\": True, \"metadata\": {}} for name, typ in columns]\n","                    # Use existing_meta_id if found, otherwise generate a new one\n","                    table_meta_id = existing_meta_id if existing_meta_id else str(uuid.uuid4())\n","\n","                    f.write(json.dumps({\"protocol\": {\"minReaderVersion\": 1, \"minWriterVersion\": 2}}) + \"\\n\")\n","                    f.write(json.dumps({\n","                        \"metaData\": {\n","                            \"id\": table_meta_id, \"name\": row.table_name, \n","                            \"description\": f\"DuckLake table {table_key}\",\n","                            \"format\": {\"provider\": \"parquet\", \"options\": {}},\n","                            \"schemaString\": create_spark_schema_string(schema_fields),\n","                            \"partitionColumns\": [], \"createdTime\": now, \n","                            \"configuration\": {\"delta.logRetentionDuration\": \"interval 0 hour\"} # Added logRetentionDuration\n","                        }\n","                    }) + \"\\n\")\n","\n","                # Write remove actions\n","                for path in removed_files_paths:\n","                    f.write(json.dumps({\"remove\": {\"path\": path, \"deletionTimestamp\": now, \"dataChange\": True}}) + \"\\n\")\n","                \n","                # Write add actions\n","                for af in added_files_data:\n","                    f.write(json.dumps({\n","                        \"add\": {\n","                            \"path\": af[\"path\"], \"partitionValues\": {}, \"size\": af[\"size\"],\n","                            \"modificationTime\": af[\"modification_time\"], \"dataChange\": True, \"stats\": af[\"stats\"]\n","                        }\n","                    }) + \"\\n\")\n","                \n","                f.write(json.dumps(commit_info) + \"\\n\")\n","\n","            print(f\"‚úÖ {table_key}: Delta log written v{next_delta_version} (DuckLake snapshot: {latest_ducklake_snapshot_id})\")\n","            \n","            # --- CHECKPOINT LOGIC ---\n","            # Create checkpoint if it's a checkpoint version and doesn't already exist\n","            if next_delta_version > 0 and next_delta_version % checkpoint_interval == 0 and not os.path.exists(checkpoint_file):\n","                checkpoint_records = []\n","                checkpoint_records.append({\"protocol\": {\"minReaderVersion\": 1, \"minWriterVersion\": 2}, \"metaData\": None, \"add\": None, \"remove\": None, \"commitInfo\": None})\n","                \n","                schema_fields = [{\"name\": name, \"type\": map_type_ducklake_to_spark(typ), \"nullable\": True, \"metadata\": {}} for name, typ in columns]\n","                \n","                # Use the same metaData.id for the checkpoint as the table\n","                checkpoint_meta_id = existing_meta_id if existing_meta_id else str(uuid.uuid4())\n","\n","                checkpoint_records.append({\n","                    \"protocol\": None, \"commitInfo\": None, \"remove\": None, \"add\": None,\n","                    \"metaData\": {\n","                        \"id\": checkpoint_meta_id, \"name\": row.table_name, \"description\": f\"DuckLake table {table_key}\",\n","                        \"format\": {\"provider\": \"parquet\", \"options\": {}},\n","                        \"schemaString\": create_spark_schema_string(schema_fields),\n","                        \"partitionColumns\": [], \"createdTime\": now, \n","                        \"configuration\": {\"delta.logRetentionDuration\": \"interval 0 hour\"} # Added logRetentionDuration\n","                    },\n","                })\n","\n","                # Add all current files from the latest DuckLake snapshot to the checkpoint\n","                for af_path in current_file_paths:\n","                    af = current_files_map[af_path]\n","                    checkpoint_records.append({\n","                        \"protocol\": None, \"metaData\": None, \"remove\": None, \"commitInfo\": None,\n","                        \"add\": {\n","                            \"path\": af[\"path\"], \"partitionValues\": {}, \"size\": af[\"size\"],\n","                            \"modificationTime\": af[\"modification_time\"], \"dataChange\": True,\n","                            \"stats\": af[\"stats\"], \"tags\": None\n","                        },\n","                    })\n","                \n","                df = pd.DataFrame(checkpoint_records)\n","                table = pa.Table.from_pandas(df, schema=get_spark_checkpoint_schema())\n","                pq.write_table(table, checkpoint_file, compression='snappy')\n","                with open(os.path.join(delta_log_path, \"_last_checkpoint\"), 'w') as f:\n","                    json.dump({\"version\": next_delta_version, \"size\": len(checkpoint_records)}, f)\n","                \n","                print(f\"üì∏ {table_key}: Checkpoint created at Delta version {next_delta_version} (DuckLake snapshot: {latest_ducklake_snapshot_id})\")\n","\n","                # --- NEW LOGIC: Delete JSON log files AND Checkpoint files older than this checkpoint ---\n","                print(f\"üßπ {table_key}: Cleaning up old log and checkpoint files before Delta version {next_delta_version}...\")\n","                for f_name in os.listdir(delta_log_path):\n","                    base_name = f_name.split('.')[0]\n","                    # Check for versioned JSON log files\n","                    if f_name.endswith('.json') and base_name.startswith('0000') and base_name.isdigit():\n","                        log_version = int(base_name)\n","                        if log_version < next_delta_version:\n","                            file_to_delete = os.path.join(delta_log_path, f_name)\n","                            try:\n","                                os.remove(file_to_delete)\n","                                print(f\"   Deleted JSON log: {f_name}\")\n","                            except OSError as e:\n","                                print(f\"   Error deleting JSON log {f_name}: {e}\")\n","                    # Check for versioned Parquet checkpoint files\n","                    elif f_name.endswith('.checkpoint.parquet'):\n","                        # Check that the part before .checkpoint.parquet is a 20-digit number\n","                        checkpoint_base_name = f_name.split('.checkpoint.parquet')[0]\n","                        if checkpoint_base_name.startswith('0000') and checkpoint_base_name.isdigit():\n","                            checkpoint_version = int(checkpoint_base_name)\n","                            if checkpoint_version < next_delta_version:\n","                                file_to_delete = os.path.join(delta_log_path, f_name)\n","                                try:\n","                                    os.remove(file_to_delete)\n","                                    print(f\"   Deleted checkpoint: {f_name}\")\n","                                except OSError as e:\n","                                    print(f\"   Error deleting checkpoint {f_name}: {e}\")\n","                print(f\"üßπ {table_key}: Cleanup complete.\")\n","\n","            elif next_delta_version > 0 and next_delta_version % checkpoint_interval == 0 and os.path.exists(checkpoint_file):\n","                print(f\"‚è© {table_key}: Checkpoint for Delta version {next_delta_version} (DuckLake snapshot: {latest_ducklake_snapshot_id}) already exists, skipping generation.\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Failed processing {table_key} for Delta version {next_delta_version} (DuckLake snapshot: {latest_ducklake_snapshot_id}): {e}\")\n","            # This should ideally rollback the written log file if it partially succeeded,\n","            # but for this script, we just log and continue to next table.\n","\n","    con.close()\n","    print(\"\\nüéâ Delta export finished.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"c270a302-ece7-4b10-93c3-763257196ce3","normalized_state":"finished","queued_time":"2025-06-21T13:45:48.6899187Z","session_start_time":"2025-06-21T13:45:48.6909507Z","execution_start_time":"2025-06-21T13:45:52.8978182Z","execution_finish_time":"2025-06-21T13:45:56.0483104Z","parent_msg_id":"86bc31fe-f5ad-497b-8c64-1d1950813622"}},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"jupyter":{"source_hidden":true}},"id":"30ebd395-7d3a-4ba3-a372-b1cda078de71"},{"cell_type":"code","source":["generate_latest_delta_log(\"/lakehouse/default/Files/meta.db\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"c270a302-ece7-4b10-93c3-763257196ce3","normalized_state":"finished","queued_time":"2025-06-21T13:45:48.6931162Z","session_start_time":null,"execution_start_time":"2025-06-21T13:45:56.0496378Z","execution_finish_time":"2025-06-21T13:46:01.8111787Z","parent_msg_id":"f7e83d04-5cd8-4b48-8035-f9d7a4821994"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ aemo.calendar: Delta table already at latest DuckLake snapshot 2 (Delta version 0), skipping export.\n‚úÖ aemo.mstdatetime: Delta table already at latest DuckLake snapshot 3 (Delta version 0), skipping export.\n‚úÖ aemo.price: Delta log written v3 (DuckLake snapshot: 18)\nüì∏ aemo.price: Checkpoint created at Delta version 3 (DuckLake snapshot: 18)\nüßπ aemo.price: Cleaning up old log and checkpoint files before Delta version 3...\n   Deleted JSON log: 00000000000000000000.json\n   Deleted JSON log: 00000000000000000002.json\n   Deleted JSON log: 00000000000000000001.json\nüßπ aemo.price: Cleanup complete.\n‚úÖ aemo.scada: Delta log written v3 (DuckLake snapshot: 19)\nüì∏ aemo.scada: Checkpoint created at Delta version 3 (DuckLake snapshot: 19)\nüßπ aemo.scada: Cleaning up old log and checkpoint files before Delta version 3...\n   Deleted JSON log: 00000000000000000000.json\n   Deleted JSON log: 00000000000000000002.json\n   Deleted JSON log: 00000000000000000001.json\nüßπ aemo.scada: Cleanup complete.\n‚úÖ aemo.duid: Delta log written v3 (DuckLake snapshot: 20)\nüì∏ aemo.duid: Checkpoint created at Delta version 3 (DuckLake snapshot: 20)\nüßπ aemo.duid: Cleaning up old log and checkpoint files before Delta version 3...\n   Deleted JSON log: 00000000000000000000.json\n   Deleted JSON log: 00000000000000000002.json\n   Deleted JSON log: 00000000000000000001.json\nüßπ aemo.duid: Cleanup complete.\n‚úÖ aemo.summary: Delta log written v3 (DuckLake snapshot: 21)\nüì∏ aemo.summary: Checkpoint created at Delta version 3 (DuckLake snapshot: 21)\nüßπ aemo.summary: Cleaning up old log and checkpoint files before Delta version 3...\n   Deleted JSON log: 00000000000000000000.json\n   Deleted JSON log: 00000000000000000002.json\n   Deleted JSON log: 00000000000000000001.json\nüßπ aemo.summary: Cleanup complete.\n\nüéâ Delta export finished.\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"e845c027-b318-4f45-9900-f2b70c7e1d48"}],"metadata":{"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"9da6ffc8-c2d9-4ee3-9625-185c2c67183f"}],"default_lakehouse":"9da6ffc8-c2d9-4ee3-9625-185c2c67183f","default_lakehouse_name":"lake","default_lakehouse_workspace_id":"b08fb13b-b5d1-4e34-aff6-2fa8a23c8122"}}},"nbformat":4,"nbformat_minor":5}