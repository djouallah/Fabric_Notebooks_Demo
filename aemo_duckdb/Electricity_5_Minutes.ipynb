{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b4fdcaf1-09c8-45b0-a7f9-34ba0422f130",
   "metadata": {
    "executionInfo": {
     "elapsed": 1055,
     "status": "ok",
     "timestamp": 1723507119247,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -600
    },
    "id": "b4fdcaf1-09c8-45b0-a7f9-34ba0422f130",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "Nbr_Files_to_Download  = 400\n",
    "schema                 = 'aemo' \n",
    "ws                     = 'sqlengines'  \n",
    "lh                     = 'power'\n",
    "storage_options = None\n",
    "table_base_url         =  f'abfss://{ws}@onelake.dfs.fabric.microsoft.com/{lh}.Lakehouse/Tables/{schema}/'\n",
    "onelake_file_root_path =  f\"{lh}.Lakehouse/Files/{schema}\"\n",
    "engine = 'pyarrow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e8eb4b30-4605-4e87-b259-cf535b6e22f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16571,
     "status": "ok",
     "timestamp": 1723507148597,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -600
    },
    "id": "e8eb4b30-4605-4e87-b259-cf535b6e22f6",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputId": "058dc7f3-ce7e-4815-8cd3-50efc740f6ee"
   },
   "outputs": [],
   "source": [
    "####### Import Python Libraries ##################################\n",
    "from   deltalake.writer import try_get_deltatable , write_deltalake\n",
    "from   deltalake        import DeltaTable\n",
    "import pyarrow as pa\n",
    "import os\n",
    "import glob\n",
    "from   azure.core.credentials     import AccessToken\n",
    "from   azure.storage.filedatalake import DataLakeServiceClient\n",
    "from   azure.identity             import DefaultAzureCredential\n",
    "import duckdb\n",
    "from   datetime import datetime\n",
    "import time\n",
    "from   psutil import *\n",
    "import re\n",
    "import requests\n",
    "from   shutil import unpack_archive\n",
    "from   urllib.request import urlopen\n",
    "from   concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad59d20",
   "metadata": {},
   "source": [
    "**<mark>Authentification</mark>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be61fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "       os.environ['azure_storage_token'] = notebookutils.credentials.getToken('storage')\n",
    "       print(' all good you are in fabric notebook')\n",
    "except:\n",
    "      from   azure.identity import InteractiveBrowserCredential\n",
    "      os.environ['azure_storage_token'.upper()] = InteractiveBrowserCredential().get_token(\"https://storage.azure.com/.default\").token\n",
    "duckdb.sql(f\"\"\" CREATE or replace SECRET onelake ( TYPE AZURE, PROVIDER ACCESS_TOKEN, ACCESS_TOKEN '{os.getenv('azure_storage_token')}')   \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dad2b9-e69b-4d6f-9dae-f13287669dd8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "<mark>Optimize delta</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7b057e30-6a70-47af-bb8a-24fff65cf609",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def optimize(tables):\n",
    "    results = {}\n",
    "    for tbl in tables:\n",
    "        try:\n",
    "            dt = DeltaTable(table_base_url + tbl)\n",
    "            dt.optimize.compact()\n",
    "            dt.vacuum(retention_hours=0, dry_run=False, enforce_retention_duration=False)\n",
    "            dt.cleanup_metadata()\n",
    "            dt.create_checkpoint()\n",
    "            results[tbl] = \"done\"\n",
    "        except Exception as e:\n",
    "            results[tbl] = f\"error: {str(e)}\"\n",
    "    return results\n",
    "def vacuum(tables):\n",
    "    results = {}\n",
    "    for tbl in tables:\n",
    "        try:\n",
    "            dt = DeltaTable(table_base_url + tbl)\n",
    "            dt.vacuum(retention_hours=0, dry_run=False, enforce_retention_duration=False)\n",
    "            dt.cleanup_metadata()\n",
    "            dt.create_checkpoint()\n",
    "            results[tbl] = \"done\"\n",
    "        except Exception as e:\n",
    "            results[tbl] = f\"error: {str(e)}\"\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d52d4-761a-4e7b-993b-0889fda74a94",
   "metadata": {
    "id": "ee4d52d4-761a-4e7b-993b-0889fda74a94",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**<mark>Download Some Data from the web</mark>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d2701d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory_if_not_exist(fs,abfss_directory) :\n",
    "    directory_client = fs.get_directory_client(abfss_directory)\n",
    "    try:\n",
    "        directory_client.get_directory_properties()\n",
    "        print(f\"Directory '{abfss_directory}' exist already.\")\n",
    "    except :\n",
    "        directory_client.create_directory()\n",
    "        print(f\"Directory '{abfss_directory}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "28416977-0f39-41ca-aa84-9a5d42a6e519",
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1723507149117,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -600
    },
    "id": "28416977-0f39-41ca-aa84-9a5d42a6e519",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def download(url,Path,total_files):\n",
    "    if not os.path.exists('/tmp'+Path):\n",
    "      os.makedirs('/tmp'+Path, exist_ok=True)\n",
    "    \n",
    "\n",
    "    result = urlopen(url).read().decode('utf-8')\n",
    "    pattern = re.compile(r'[\\w.]*.zip')\n",
    "    filelist1 = pattern.findall(result)\n",
    "    filelist_unique = dict.fromkeys(filelist1)\n",
    "    filelist =sorted(filelist_unique, reverse=True)\n",
    "\n",
    "    onelake_folder = onelake_file_root_path + Path\n",
    "    create_directory_if_not_exist(fs,onelake_folder)\n",
    "    paths = fs.get_paths(path=onelake_folder)\n",
    "    current = [os.path.basename(path.name) for path in paths ]\n",
    "\n",
    "    #current =  [os.path.basename(x) for x in glob.glob(Path+'*.zip')]\n",
    "\n",
    "\n",
    "    files_to_upload = list(set(filelist) - set(current))\n",
    "    files_to_upload = list(dict.fromkeys(files_to_upload))[:total_files]\n",
    "    print(str(len(files_to_upload)) + ' New File Downloaded')\n",
    "    if len(files_to_upload) != 0 :\n",
    "      for x in files_to_upload:\n",
    "           with requests.get(url+x, stream=True) as resp:\n",
    "            if resp.ok:\n",
    "              with open(f\"/tmp{Path}{x}\", \"wb\") as f:\n",
    "               for chunk in resp.iter_content(chunk_size=4096):\n",
    "                f.write(chunk)\n",
    "    return \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d20ed8d-8b3f-462b-a866-8736eab45789",
   "metadata": {
    "id": "4d20ed8d-8b3f-462b-a866-8736eab45789",
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**<mark>Unzip</mark>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3b5f3047-8010-41ba-8e30-396e26de0e81",
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1723507158049,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -600
    },
    "id": "3b5f3047-8010-41ba-8e30-396e26de0e81",
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def uncompress(zip_file, source_path, destination_path):\n",
    "    \"\"\"Uncompress a zip file to the destination path.\"\"\"\n",
    "    try:\n",
    "        unpack_archive(os.path.join(source_path, zip_file), destination_path, 'zip')\n",
    "        print(f\"Uncompressed: {zip_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to uncompress {zip_file}: {e}\")\n",
    "\n",
    "def unzip(source, destination):\n",
    "    \"\"\"Unzip all zip files from the source directory to the destination directory.\"\"\"\n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination, exist_ok=True)\n",
    "\n",
    "    # Get list of zip files in the source directory\n",
    "    zip_files = [os.path.basename(x) for x in glob.glob(os.path.join(source, '*.zip'))]\n",
    "\n",
    "    # Get list of already uncompressed files in the destination directory\n",
    "    uncompressed_files = [\n",
    "        os.path.basename(x).replace('.CSV', '.zip')\n",
    "        for x in glob.glob(os.path.join(destination, '*.[Cc][Ss][Vv]')) + \n",
    "                  glob.glob(os.path.join(destination, '*.json'))\n",
    "    ]\n",
    "\n",
    "    # Determine files to uncompress (delta)\n",
    "    files_to_uncompress = list(set(zip_files) - set(uncompressed_files))\n",
    "    files_to_uncompress = list(dict.fromkeys(files_to_uncompress))  # Remove duplicates\n",
    "\n",
    "    print(f\"{len(files_to_uncompress)} new file(s) to uncompress.\")\n",
    "\n",
    "    if files_to_uncompress:\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            # Use lambda to pass additional arguments to uncompress\n",
    "            executor.map(lambda f: uncompress(f, source, destination), files_to_uncompress)\n",
    "        return \"Done\"\n",
    "    else:\n",
    "        return \"Nothing to uncompress\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ab413635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Path(Source,Table):\n",
    " dt =try_get_deltatable(Table,storage_options={\"allow_unsafe_rename\":\"true\"})\n",
    " if dt is not None:\n",
    "    existing_files = duckdb.sql(f\"\"\" select distinct file as file from delta_scan('{Table}') \"\"\").df()['file'].tolist()\n",
    " else:\n",
    "  existing_files=[]\n",
    " print(len(existing_files))\n",
    " list_files_csv=[os.path.basename(x) for x in glob.glob(Source+'*.CSV')]\n",
    " list_files_json=[os.path.basename(x) for x in glob.glob(Source+'*.json')]\n",
    " filelist = list_files_csv + list_files_json\n",
    "\n",
    " files_to_upload = list(set(filelist) - set(existing_files))\n",
    " files_to_upload = list(dict.fromkeys(files_to_upload))\n",
    " files_to_upload_full_Path = [Source + i for i in files_to_upload]\n",
    " return files_to_upload_full_Path[:Nbr_Files_to_Download]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "615e8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onelake_fs(file_system_name):\n",
    "    class CustomTokenCredential:\n",
    "        def __init__(self, token):\n",
    "            self.token = token\n",
    "\n",
    "        def get_token(self, *scopes, **kwargs):\n",
    "            return AccessToken(self.token, expires_on=9999999999)  # Set a far future expiration\n",
    "\n",
    "    credential = CustomTokenCredential(os.getenv('azure_storage_token'))\n",
    "    service_client = DataLakeServiceClient(account_url=f\"https://onelake.dfs.fabric.microsoft.com\", credential= credential)\n",
    "    return service_client.get_file_system_client(file_system_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1ab3c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def List_files_to_upload(fs,local_folder,onelake_folder):\n",
    "    create_directory_if_not_exist(fs,onelake_folder)\n",
    "    paths = fs.get_paths(path=onelake_folder)\n",
    "    onelake_files = [os.path.basename(path.name) for path in paths ]\n",
    "    local_files=[os.path.basename(x) for x in glob.glob(local_folder+'*.*')]\n",
    "    filtes_to_upload  = list(  set(local_files) - set(onelake_files))\n",
    "    filtes_to_upload  = list(dict.fromkeys(filtes_to_upload))\n",
    "    filtes_to_upload  = sorted(filtes_to_upload, reverse=True)\n",
    "    filtes_to_upload = [local_folder+'/' + i for i in filtes_to_upload]\n",
    "    return filtes_to_upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "50c08235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file_system_client, local_file, remote_directory):\n",
    "    \"\"\"\n",
    "    Helper function to upload a single file to a remote directory in ADLS/OneLake.\n",
    "    \n",
    "    Args:\n",
    "        file_system_client: The file system client.\n",
    "        local_file: Local file path to upload.\n",
    "        remote_directory: The remote directory where the file will be uploaded.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract the file name from the local file path\n",
    "        file_name = os.path.basename(local_file)\n",
    "        \n",
    "        # Define the remote file path\n",
    "        remote_file_path = f\"{remote_directory}{file_name}\"\n",
    "        \n",
    "        # Upload the file\n",
    "        print(f\"Uploading {local_file} to {remote_file_path}\")\n",
    "        file_client = file_system_client.get_file_client(remote_file_path)\n",
    "        with open(local_file, \"rb\") as local_file_data:\n",
    "            file_client.upload_data(local_file_data, overwrite=True)\n",
    "        print(f\"Uploaded {local_file} successfully!\")\n",
    "        return local_file, None  # Return the file and no error\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {local_file}: {e}\")\n",
    "        return local_file, str(e)  # Return the file and the error message\n",
    "\n",
    "def upload_files(file_system_client, local_folder, remote_directory, max_workers=5):\n",
    "    \"\"\"\n",
    "    Upload a list of files to a remote directory in ADLS/OneLake using multi-threading.\n",
    "    \n",
    "    Args:\n",
    "        file_system_client: The file system client.\n",
    "        local_files: List of local file paths to upload.\n",
    "        remote_directory: The remote directory where files will be uploaded.\n",
    "        max_workers: Maximum number of threads to use for parallel uploads.\n",
    "    \"\"\"\n",
    "    local_files =     List_files_to_upload(file_system_client,local_folder,remote_directory)\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit upload tasks to the thread pool\n",
    "        futures = [\n",
    "            executor.submit(upload_file, file_system_client, local_file, remote_directory)\n",
    "            for local_file in local_files\n",
    "        ]\n",
    "        \n",
    "        # Wait for all tasks to complete and handle results\n",
    "        for future in as_completed(futures):\n",
    "            local_file, error = future.result()\n",
    "            if error:\n",
    "                print(f\"Error uploading {local_file}: {error}\")\n",
    "            else:\n",
    "                print(f\"Successfully uploaded {local_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HPeQ0W-DtsEI",
   "metadata": {
    "id": "HPeQ0W-DtsEI"
   },
   "source": [
    "**<mark>Price Today </mark>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "BhdYGAod4Vid",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1723507161094,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -600
    },
    "id": "BhdYGAod4Vid",
    "jupyter": {
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def get_price(files_to_upload_full_Path):\n",
    "  raw =duckdb.sql(f\"\"\"from read_csv({files_to_upload_full_Path},\n",
    "  Skip=1,header =0,all_varchar=1,\n",
    "  columns={{\n",
    "  'I' : 'VARCHAR', 'DISPATCH' : 'VARCHAR', 'PRICE' : 'VARCHAR', 'xx' : 'VARCHAR', 'SETTLEMENTDATE' : 'VARCHAR', 'RUNNO' : 'VARCHAR', 'REGIONID' : 'VARCHAR',\n",
    "   'DISPATCHINTERVAL' : 'VARCHAR', 'INTERVENTION' : 'VARCHAR', 'RRP' : 'VARCHAR', 'EEP' : 'VARCHAR', 'ROP' : 'VARCHAR', 'APCFLAG' : 'VARCHAR',\n",
    "    'MARKETSUSPENDEDFLAG' : 'VARCHAR', 'LASTCHANGED' : 'VARCHAR', 'RAISE6SECRRP' : 'VARCHAR', 'RAISE6SECROP' : 'VARCHAR', 'RAISE6SECAPCFLAG' : 'VARCHAR',\n",
    "    'RAISE60SECRRP' : 'VARCHAR', 'RAISE60SECROP' : 'VARCHAR', 'RAISE60SECAPCFLAG' : 'VARCHAR', 'RAISE5MINRRP' : 'VARCHAR', 'RAISE5MINROP' : 'VARCHAR',\n",
    "    'RAISE5MINAPCFLAG' : 'VARCHAR', 'RAISEREGRRP' : 'VARCHAR', 'RAISEREGROP' : 'VARCHAR', 'RAISEREGAPCFLAG' : 'VARCHAR', 'LOWER6SECRRP' : 'VARCHAR',\n",
    "     'LOWER6SECROP' : 'VARCHAR', 'LOWER6SECAPCFLAG' : 'VARCHAR', 'LOWER60SECRRP' : 'VARCHAR', 'LOWER60SECROP' : 'VARCHAR', 'LOWER60SECAPCFLAG' : 'VARCHAR',\n",
    "     'LOWER5MINRRP' : 'VARCHAR', 'LOWER5MINROP' : 'VARCHAR', 'LOWER5MINAPCFLAG' : 'VARCHAR', 'LOWERREGRRP' : 'VARCHAR', 'LOWERREGROP' : 'VARCHAR',\n",
    "      'LOWERREGAPCFLAG' : 'VARCHAR', 'PRICE_STATUS' : 'VARCHAR', 'PRE_AP_ENERGY_PRICE' : 'VARCHAR', 'PRE_AP_RAISE6_PRICE' : 'VARCHAR', 'PRE_AP_RAISE60_PRICE' : 'VARCHAR',\n",
    "       'PRE_AP_RAISE5MIN_PRICE' : 'VARCHAR', 'PRE_AP_RAISEREG_PRICE' : 'VARCHAR', 'PRE_AP_LOWER6_PRICE' : 'VARCHAR', 'PRE_AP_LOWER60_PRICE' : 'VARCHAR',\n",
    "        'PRE_AP_LOWER5MIN_PRICE' : 'VARCHAR', 'PRE_AP_LOWERREG_PRICE' : 'VARCHAR', 'RAISE1SECRRP' : 'VARCHAR', 'RAISE1SECROP' : 'VARCHAR', 'RAISE1SECAPCFLAG' : 'VARCHAR',\n",
    "         'LOWER1SECRRP' : 'VARCHAR', 'LOWER1SECROP' : 'VARCHAR', 'LOWER1SECAPCFLAG' : 'VARCHAR', 'PRE_AP_RAISE1_PRICE' : 'VARCHAR',\n",
    "          'PRE_AP_LOWER1_PRICE' : 'VARCHAR', 'CUMUL_PRE_AP_ENERGY_PRICE' : 'VARCHAR', 'CUMUL_PRE_AP_RAISE6_PRICE' : 'VARCHAR', 'CUMUL_PRE_AP_RAISE60_PRICE' : 'VARCHAR',\n",
    "          'CUMUL_PRE_AP_RAISE5MIN_PRICE' : 'VARCHAR', 'CUMUL_PRE_AP_RAISEREG_PRICE' : 'VARCHAR', 'CUMUL_PRE_AP_LOWER6_PRICE' : 'VARCHAR',\n",
    "          'CUMUL_PRE_AP_LOWER60_PRICE' : 'VARCHAR', 'CUMUL_PRE_AP_LOWER5MIN_PRICE' : 'VARCHAR', 'CUMUL_PRE_AP_LOWERREG_PRICE' : 'VARCHAR',\n",
    "          'CUMUL_PRE_AP_RAISE1_PRICE' : 'VARCHAR', 'CUMUL_PRE_AP_LOWER1_PRICE' : 'VARCHAR', 'OCD_STATUS' : 'VARCHAR', 'MII_STATUS' : 'VARCHAR',\n",
    "  }},\n",
    "  filename =1,null_padding = true,ignore_errors=1,auto_detect=false)\n",
    "  where I='D' and PRICE ='PRICE'\n",
    "\n",
    "                  \"\"\")\n",
    "  columns = list(set(raw.columns) - {'SETTLEMENTDATE','REGIONID','I','PRICE','filename','OCD_STATUS','MII_STATUS','DISPATCH','PRICE_STATUS','LASTCHANGED'})\n",
    "\n",
    "  exprs = [\n",
    "    duckdb.ColumnExpression(x).cast(duckdb.typing.DOUBLE).alias(x)\n",
    "    for x in columns\n",
    "  ]\n",
    "  rel2 = raw.select('SETTLEMENTDATE','REGIONID','I','PRICE','filename','OCD_STATUS','MII_STATUS','DISPATCH','PRICE_STATUS','LASTCHANGED',*exprs)\n",
    "  final=duckdb.sql(\"\"\" select *exclude(SETTLEMENTDATE,I,xx,'PRICE','filename'),\n",
    "  cast (SETTLEMENTDATE as TIMESTAMPTZ) as SETTLEMENTDATE,\n",
    "  cast(SETTLEMENTDATE as date) as date,\n",
    "  parse_filename(filename) as file,\n",
    "  0 as PRIORITY,\n",
    "  isoyear (cast (SETTLEMENTDATE as TIMESTAMPTZ)) as YEAR\n",
    "  from rel2  \"\"\")\n",
    "  return final.arrow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Gg9hRwlEVjGM",
   "metadata": {
    "id": "Gg9hRwlEVjGM"
   },
   "source": [
    "**<mark>SCADA Today </mark>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "U_XTWvrrWByo",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1723507163085,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -600
    },
    "id": "U_XTWvrrWByo",
    "jupyter": {
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def get_scada(files_to_upload_full_Path):\n",
    "  raw =duckdb.sql(f\"\"\"from read_csv({files_to_upload_full_Path},\n",
    "  Skip=1,header =0,all_varchar=1,\n",
    "  columns={{\n",
    "  'I' : 'VARCHAR', 'DISPATCH' : 'VARCHAR', 'UNIT_SCADA' : 'VARCHAR', 'xx' : 'VARCHAR', 'SETTLEMENTDATE' : 'timestamp', 'DUID' : 'VARCHAR', 'SCADAVALUE' : 'double','LASTCHANGED' : 'timestamp'\n",
    "  }},\n",
    "  filename =1,null_padding = true,ignore_errors=1,auto_detect=false)\n",
    "  where I='D' and SCADAVALUE !=0\n",
    "                  \"\"\")\n",
    "  scada=duckdb.sql(\"\"\" select  DUID,SCADAVALUE as INITIALMW, cast(0 as double ) as INTERVENTION,\n",
    "   cast (SETTLEMENTDATE as TIMESTAMPTZ) as SETTLEMENTDATE,\n",
    "   cast(SETTLEMENTDATE as date) as date,\n",
    "   parse_filename(filename) as file,\n",
    "   0 as PRIORITY ,\n",
    "   isoyear (cast (SETTLEMENTDATE as timestamp)) as YEAR\n",
    "   from raw\n",
    "    \"\"\")\n",
    "  return scada.arrow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91da29f-926f-47db-aba4-bfcb3788dbc6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**<mark>DUID </mark>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "aae61997-f447-4096-8d8d-cfcfd2c81746",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_duid(Onelake_table):\n",
    "    DUID_Path = \"/lakehouse/default/Files/0_Source/Dimensions/DUID/\"\n",
    "    import pathlib\n",
    "    import requests\n",
    "    pathlib.Path(DUID_Path).mkdir(parents=True, exist_ok=True)\n",
    "    url = \"https://www.aemo.com.au/-/media/Files/Electricity/NEM/Participant_Information/NEM-Registration-and-Exemption-List.xls\"\n",
    "    s = requests.Session()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36'}\n",
    "    r = s.get(url,headers=headers)\n",
    "    r.content\n",
    "    output = open(DUID_Path+\"NEM-Registration-and-Exemption-List.xls\", 'wb')\n",
    "    output.write(r.content)\n",
    "    output.close()\n",
    "    duckdb.sql(f\"\"\"\n",
    "    INSTALL spatial;\n",
    "    LOAD spatial;\n",
    "    create or replace table DUID as\n",
    "    SELECT Region,DUID,first(\"Fuel Source - Descriptor\") as FuelSourceDescriptor,first(Participant) as Participant\n",
    "    FROM st_read('{DUID_Path}NEM-Registration-and-Exemption-List.xls', layer = 'PU and Scheduled Loads',open_options = ['HEADERS=FORCE'])\n",
    "    group by all\n",
    "    \"\"\")\n",
    "\n",
    "    import requests\n",
    "    dls = \"https://data.wa.aemo.com.au/datafiles/post-facilities/facilities.csv\"\n",
    "    resp = requests.get(dls)\n",
    "    output = open(DUID_Path+\"facilities_WA.csv\", 'wb')\n",
    "    output.write(resp.content)\n",
    "    output.close()\n",
    "\n",
    "    duckdb.sql(f\"\"\" select 'WA1' as Region  , \"Facility Code\" as DUID ,\"Participant Name\" as Participant from read_csv_auto('{DUID_Path}facilities_WA.csv')\"\"\").to_view('x')\n",
    "\n",
    "    duckdb.sql(\"\"\"select x.Region,x.DUID, TECHNOLOGY as FuelSourceDescriptor,Participant from x\n",
    "    left join (select * FROM read_csv_auto('https://github.com/djouallah/aemo_fabric/raw/main/WA_ENERGY.csv',header=1)) as z\n",
    "    on x.duid=z.duid \"\"\").to_view('DUID_WA')\n",
    "\n",
    "    duckdb.sql(\"\"\"\n",
    "    create or replace table states(RegionID varchar, States varchar) ;\n",
    "    insert into states values\n",
    "    ('WA1' , 'Western Australia') ,\n",
    "    ('QLD1' , 'Queensland')  ,\n",
    "    ('NSW1' , 'New South Walles')  ,\n",
    "    ('TAS1' , 'Tasmania')  ,\n",
    "    ('SA1' , 'South Australia')  ,\n",
    "    ('VIC1' , 'Victoria')\n",
    "    \"\"\")\n",
    "\n",
    "    df =duckdb.sql(f\"\"\" with xx as (select * from DUID union BY NAME select * from DUID_WA)\n",
    "                select States,trim(DUID) as DUID,min(Region) as Region, min(FuelSourceDescriptor) as FuelSourceDescriptor,\n",
    "                min(Participant) as Participant\n",
    "                from xx\n",
    "                JOIN states on xx.Region = states.RegionID\n",
    "                where length(trim(DUID)) > 2\n",
    "                group by all\n",
    "                \"\"\").arrow()\n",
    "    write_deltalake(Onelake_table, df,mode=\"overwrite\",engine=engine,storage_options = storage_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6d663c-4389-411c-8414-359c1ea7aceb",
   "metadata": {
    "id": "be6d663c-4389-411c-8414-359c1ea7aceb",
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "_**<mark>Summary</mark>**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fabb4ef-2c81-4013-bc3e-5dd305dbdd8d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "sch = pa.schema([\n",
    "                    (\"date\",        pa.date32()),\n",
    "                    (\"time\",        pa.int16()),\n",
    "                    (\"cutoff\",      pa.timestamp(\"us\",\"UTC\")),\n",
    "                    (\"DUID\",        pa.string()),\n",
    "                    (\"mw\",          pa.decimal128(18, 4)),\n",
    "                    (\"price\",       pa.decimal128(18, 4)),\n",
    "         ])\n",
    "DeltaTable.create(table_base_url+\"summary\", schema=sch,mode=\"ignore\",storage_options = storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "27a393a5-1009-48a4-9697-70320fd12fdb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_max(tbl):\n",
    "    result = duckdb.sql(f\"\"\" SELECT  STRFTIME( max(cutoff), '%Y-%m-%d %H:%M:%S') FROM delta_scan('{tbl}') \"\"\").fetchone()[0]\n",
    "    return result if result is not None else '1900-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "af21959b-a306-4585-85ba-b6b5b32f2167",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def full_refresh(schema):  \n",
    "    return f\"\"\"\n",
    "      CREATE OR REPLACE VIEW {schema}.scada AS SELECT * FROM delta_scan('{table_base_url}scada') where INTERVENTION = 0 and INITIALMW <> 0 ;\n",
    "      CREATE OR REPLACE VIEW {schema}.price AS SELECT * FROM delta_scan('{table_base_url}price') where INTERVENTION = 0;\n",
    "      select\n",
    "        s.date,\n",
    "        cast(strftime(s.SETTLEMENTDATE, '%H%M') AS INT16)                       as time ,\n",
    "        (select max(cast(settlementdate as TIMESTAMPTZ) ) from {schema}.scada)  as cutoff ,\n",
    "        s.DUID,\n",
    "        cast(max(s.INITIALMW) AS DECIMAL(18, 4))                                as mw,\n",
    "        cast(max(p.RRP) AS DECIMAL(18, 4))                                      as price\n",
    "      from  {schema}.scada   s\n",
    "            LEFT JOIN {schema}.duid d    ON s.DUID = d.DUID\n",
    "            LEFT JOIN {schema}.price   p ON s.SETTLEMENTDATE = p.SETTLEMENTDATE AND d.Region = p.REGIONID\n",
    "      where  s.settlementdate >= '2020-01-01'\n",
    "      group by all\n",
    "      order by  s.date, s.DUID, time,price\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "536d901f-7d72-477a-87d3-aa3b98a1f2af",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def Incremental_refresh(schema,max_timestamp):\n",
    "    return f\"\"\"\n",
    "      select\n",
    "        s.date,\n",
    "        cast(strftime(s.SETTLEMENTDATE, '%H%M') AS INT16)                            as time ,\n",
    "        (select max(cast(settlementdate as TIMESTAMPTZ) ) from {schema}.scada_today) as cutoff ,\n",
    "        s.DUID,\n",
    "        CAST(max(s.INITIALMW) AS DECIMAL(18, 4))  as mw,\n",
    "        CAST(max(p.RRP) AS DECIMAL(18, 4))        as price\n",
    "      from\n",
    "        {schema}.scada_today  s\n",
    "        JOIN {schema}.duid d ON s.DUID = d.DUID\n",
    "        JOIN {schema}.price_today        p ON s.SETTLEMENTDATE = p.SETTLEMENTDATE AND d.Region = p.REGIONID\n",
    "      where\n",
    "        s.INTERVENTION = 0\n",
    "        and p.INTERVENTION = 0\n",
    "        and INITIALMW <> 0\n",
    "        and s.settlementdate > '{max_timestamp}' and p.settlementdate > '{max_timestamp}'\n",
    "      group by\n",
    "        all\n",
    "      order by s.date\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XRr_LskEkj9k",
   "metadata": {
    "id": "XRr_LskEkj9k"
   },
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530b7d6-4118-4ca7-9e40-3facd2f444e8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "Onelake_table = table_base_url+'duid'\n",
    "try:\n",
    "    dt =DeltaTable(Onelake_table)\n",
    "    print('duid loaded already')\n",
    "except:\n",
    "    get_duid(Onelake_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fafd822-a9bf-4e05-af90-c8738488dcc3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "Onelake_table = table_base_url +'mstdatetime'\n",
    "try:\n",
    "    dt =DeltaTable(Onelake_table)\n",
    "    print('mstdatetime loaded already')\n",
    "except:\n",
    "    df=duckdb.sql(\"\"\" SELECT cast(unnest(generate_series(cast ('2018-04-01' as date), cast('2024-12-31' as date), interval 5 minute)) as TIMESTAMPTZ) as SETTLEMENTDATE,\n",
    "            strftime(SETTLEMENTDATE, '%I:%M:%S %p') as time,\n",
    "            cast(SETTLEMENTDATE as date ) as date,\n",
    "            EXTRACT(year from date) as year,\n",
    "            EXTRACT(month from date) as month\n",
    "            \"\"\").arrow()\n",
    "    write_deltalake(Onelake_table, df,mode=\"overwrite\",engine='rust',storage_options = storage_options )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a5a9f-3b79-45e0-9354-d9d495ea95b2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "Onelake_table = table_base_url+'calendar'\n",
    "try:\n",
    "    dt =DeltaTable(Onelake_table)\n",
    "    print('calendar loaded already')\n",
    "except:\n",
    "    df=duckdb.sql(\"\"\" SELECT cast(unnest(generate_series(cast ('2018-04-01' as date), cast('2024-12-31' as date), interval 1 day)) as date) as date,\n",
    "            EXTRACT(year from date) as year,\n",
    "            EXTRACT(month from date) as month\n",
    "            \"\"\").arrow()\n",
    "    write_deltalake(Onelake_table, df,mode=\"overwrite\",engine=engine,storage_options = storage_options )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3355b189-4ece-4865-ad97-177755a88c70",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "Onelake_table = table_base_url +'mstime'\n",
    "try:\n",
    "    dt =DeltaTable(Onelake_table)\n",
    "    print('mstime loaded already')\n",
    "except:\n",
    "    xx=duckdb.sql(\"\"\" SELECT cast(unnest(generate_series(cast ('2018-04-01' as date), cast('2018-04-02' as date), interval 5 minute)) as TIMESTAMPTZ) as SETTLEMENTDATE,\n",
    "        strftime(SETTLEMENTDATE, '%I:%M:%S %p') as time,CAST(strftime(SETTLEMENTDATE, '%H%M') AS INT16) AS id  \"\"\")\n",
    "    df = duckdb.sql(\"\"\" select time, min(id) as id from xx group by time \"\"\").arrow()\n",
    "    write_deltalake(Onelake_table, df,mode=\"overwrite\",engine=engine,storage_options = storage_options )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "72d97ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = get_onelake_fs(f\"{ws}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "Web_Path                = \"http://nemweb.com.au/Reports/Current/DispatchIS_Reports/\"\n",
    "Zip_Path                = \"/0_Source/Current/DispatchIS_Reports/\"\n",
    "uncompressed_Path       = \"/1_Transform/CSV/DispatchIS_Reports/\"\n",
    "Onelake_table           = table_base_url+'price_today'\n",
    "#############################################\n",
    "download(Web_Path,Zip_Path,Nbr_Files_to_Download)\n",
    "unzip('/tmp'+Zip_Path,'/tmp'+uncompressed_Path)\n",
    "upload_files(fs, '/tmp'+Zip_Path, onelake_file_root_path + Zip_Path , max_workers= cpu_count())\n",
    "files_to_upload_full_Path = get_Path('/tmp'+uncompressed_Path,Onelake_table)\n",
    "if len(files_to_upload_full_Path) >0 :\n",
    "  df = get_price(files_to_upload_full_Path)\n",
    "  write_deltalake(Onelake_table, df,mode=\"append\",partition_by=['date'],engine=engine, storage_options = storage_options)\n",
    "else:\n",
    "  print('all loaded already')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "Web_Path                = \"http://nemweb.com.au/Reports/Current/Dispatch_SCADA/\"\n",
    "Zip_Path                = \"/0_Source/Current/Dispatch_SCADA/\"\n",
    "uncompressed_Path       = \"/1_Transform/CSV/Dispatch_SCADA/\"\n",
    "Onelake_table           = table_base_url+'scada_today'\n",
    "#############################################\n",
    "download(Web_Path,Zip_Path,Nbr_Files_to_Download)\n",
    "unzip('/tmp'+Zip_Path,'/tmp'+uncompressed_Path)\n",
    "upload_files(fs, '/tmp'+Zip_Path, onelake_file_root_path + Zip_Path , max_workers= cpu_count())\n",
    "files_to_upload_full_Path = get_Path('/tmp'+uncompressed_Path,Onelake_table)\n",
    "if len(files_to_upload_full_Path) >0 :\n",
    "  df = get_scada(files_to_upload_full_Path)\n",
    "  write_deltalake(Onelake_table, df,mode=\"append\",partition_by=['date'],engine=engine, storage_options = storage_options)\n",
    "else:\n",
    "  print('all loaded already')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678dc3b0-1d0b-45a3-afce-91d08dadbb69",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "Summary_table_path = table_base_url+'summary'\n",
    "                 \n",
    "duckdb.sql(f\"create SCHEMA IF NOT EXISTS {schema} \")\n",
    "for tbl in ['scada_today','duid','price_today','summary']:\n",
    "    duckdb.sql(f\"CREATE OR REPLACE VIEW {schema}.{tbl} AS SELECT * FROM delta_scan('{table_base_url}{tbl}');\")\n",
    "try:\n",
    " dt = DeltaTable(f'{table_base_url}scada')\n",
    " current_version = dt.version()\n",
    "except:\n",
    " current_version = -1\n",
    "dt = DeltaTable(Summary_table_path)\n",
    "Previous_version = dt.history()[0].get('scada')\n",
    "Previous_version = int(Previous_version or '0')\n",
    "print(f'scada current  version {current_version}')\n",
    "print(f'scada version used by Summary {Previous_version}')\n",
    "if current_version != Previous_version and current_version !=-1:\n",
    "    print('scada table was updated, trigger full refresh')\n",
    "    get_duid(table_base_url+'duid')\n",
    "    df = duckdb.sql(full_refresh(schema)).arrow()\n",
    "    r = df.num_rows\n",
    "    if r >0 :\n",
    "        RG=8_000_000\n",
    "        write_deltalake(Summary_table_path,\n",
    "        df,\n",
    "        mode=\"overwrite\",\n",
    "        max_rows_per_file = RG , max_rows_per_group = RG, min_rows_per_group = RG,\n",
    "        storage_options = storage_options ,\n",
    "        custom_metadata = {'scada':str(current_version)},\n",
    "        engine=engine)\n",
    "        #### Table maintenance ##############\n",
    "        optimize(['price_today','scada_today'])\n",
    "        print(f' {r} rows fully refreshed')\n",
    "else:\n",
    "    print('scada table was not updated, trigger incremental refresh')\n",
    "    max_timestamp = get_max(Summary_table_path)\n",
    "    df = duckdb.sql(Incremental_refresh(schema,max_timestamp)).arrow()\n",
    "    r = df.num_rows\n",
    "    if r >0 :\n",
    "        write_deltalake(Summary_table_path,\n",
    "         df,\n",
    "         mode=\"append\",\n",
    "         custom_metadata = {'scada':str(current_version)},\n",
    "         storage_options = storage_options,\n",
    "         engine='rust')\n",
    "        print(f'new {r} rows inserted')\n",
    "    else :\n",
    "        print(\"no new data\")  "
   ]
  }
 ],
 "metadata": {
  "a365ComputeOptions": null,
  "colab": {
   "name": "aemo_5_Minutes.ipynb",
   "provenance": []
  },
  "dependencies": {
   "environment": {},
   "lakehouse": {
    "default_lakehouse": "e4a5a1d3-cda8-48ff-b6bc-cfc314f4bcd8",
    "default_lakehouse_name": "data",
    "default_lakehouse_workspace_id": "3eb420b2-9a35-4b43-924d-e599a08c604d",
    "known_lakehouses": [
     {
      "id": "e4a5a1d3-cda8-48ff-b6bc-cfc314f4bcd8"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "notebook_environment": {},
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "save_output": true,
  "sessionKeepAliveTimeout": 0,
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    },
    "enableDebugMode": false
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03a332958e2b4e39bea9aaee681224b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "bar_color": "black",
       "description_width": ""
      }
     },
     "077fa76f9ccb43b5805f42c9ed1738bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "bar_color": "black",
       "description_width": ""
      }
     },
     "0e3fa544c2f14c9e9d34b1eda78ec544": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_c0b5191566be4daf9686e0fccb0e714b",
       "style": "IPY_MODEL_4ff0722a9ff94299aa2c9885c20f9bd9",
       "value": 100
      }
     },
     "47844f1768f1495d820ae28811ac9041": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_66b879559c234c7c808419cc22b54d58",
       "style": "IPY_MODEL_03a332958e2b4e39bea9aaee681224b7",
       "value": 100
      }
     },
     "4ff0722a9ff94299aa2c9885c20f9bd9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "bar_color": "black",
       "description_width": ""
      }
     },
     "5cbdf3c100d2406cac73f3f0e91043af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "bar_color": "black",
       "description_width": ""
      }
     },
     "66b879559c234c7c808419cc22b54d58": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "89cadbefbf1c4cf19986014e19474f29": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "bed5791119bb40888603438b46ec5b86": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_fbb0285be4ca4b4493316b0c076326ec",
       "style": "IPY_MODEL_5cbdf3c100d2406cac73f3f0e91043af"
      }
     },
     "c0b5191566be4daf9686e0fccb0e714b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "c1e4800519f84b639de9c03e9d3dbc37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_e7da1a0d37194c5e8f7fa00861e75a9e",
       "style": "IPY_MODEL_d00e7da9e9c940e89dadada03ffb3ba8",
       "value": 100
      }
     },
     "d00e7da9e9c940e89dadada03ffb3ba8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "bar_color": "black",
       "description_width": ""
      }
     },
     "e5dff4e48b6e4b28b314a3f140edd4ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_89cadbefbf1c4cf19986014e19474f29",
       "style": "IPY_MODEL_077fa76f9ccb43b5805f42c9ed1738bb"
      }
     },
     "e7da1a0d37194c5e8f7fa00861e75a9e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "fbb0285be4ca4b4493316b0c076326ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
