{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac56ca3a-fe0c-4ff9-9b06-63b77aa8232e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\"vCores\": 64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a07a6e4-84d3-4f04-a454-3f14fe563a63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7pHvpu6xED1",
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "outputId": "abb82bfe-8661-465e-a2df-68a87a204c45"
   },
   "outputs": [],
   "source": [
    "!pip install duckdb --pre --upgrade\n",
    "import sys\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8ddbb-41dc-4b66-9524-fad187535e81",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "num_rows =   2_000_000_000\n",
    "ws       =   \"largedata\"\n",
    "lh       =   \"coffee\"\n",
    "db_path  =   '/lakehouse/default/Files/coffe_meta.db'\n",
    "results  =   'abfss://largedata@onelake.dfs.fabric.microsoft.com/coffee.Lakehouse/Tables/dbo/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203d250-7f99-4396-b70f-94119a3ddce4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "from   datetime           import datetime\n",
    "from   deltalake.writer   import write_deltalake\n",
    "import pandas as pd\n",
    "from   psutil import *\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c50215-71a8-4eb4-81e5-02b924380fab",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "core = cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64610541-0c54-4a77-9fe0-e4730041ca4d",
   "metadata": {
    "id": "04s07KCLBAT8"
   },
   "source": [
    "# ***Write Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31ece38-ea15-4be5-9f3b-2529ebc38208",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# adapted from here, converted from pyspark to duckdb sql https://www.linkedin.com/pulse/databricks-vs-snowflake-fabric-test-details-josue-a-bogran-zcpke/\n",
    "if not os.path.exists(f\"/lakehouse/default/Tables/coffee{num_rows}\"):\n",
    "    con = duckdb.connect()\n",
    "    con.sql(f\"\"\"\n",
    "    ATTACH or replace 'ducklake:{db_path}' AS db (DATA_PATH '/lakehouse/default/Tables');\n",
    "    USE db ;\n",
    "    create schema if not exists coffee{num_rows} ;\n",
    "    USE coffee{num_rows} ;\n",
    "    SET preserve_insertion_order = false;\n",
    "    SET temp_directory=   '/lakehouse/default/Files/tmp' ;\n",
    "\n",
    "    -- Step 0: Defined Dimensional Tables\n",
    "    create table if not exists dim_locations as select * from 'https://raw.githubusercontent.com/JosueBogran/coffeeshopdatagenerator/refs/heads/main/Dim_Locations_Table.csv' ;\n",
    "    create table if not exists dim_products  as select * from 'https://raw.githubusercontent.com/JosueBogran/coffeeshopdatagenerator/refs/heads/main/Dim_Products_Table.csv' ;\n",
    "\n",
    "\n",
    "    -- Step 1: Generate base orders\n",
    "    CREATE OR REPLACE view base_orders AS\n",
    "    WITH base AS (\n",
    "        SELECT\n",
    "            id,\n",
    "            md5(CAST(id AS VARCHAR) || '_' || random()) AS Order_ID,\n",
    "            DATE '2023-01-01' + CAST(random() * 730 AS INTEGER) AS Order_Date,\n",
    "            random() AS rand_lines,\n",
    "            random() AS rand_tod,\n",
    "            random() AS rand_loc\n",
    "        FROM generate_series(0, {num_rows} - 1) tbl(id)\n",
    "    )\n",
    "    SELECT\n",
    "        *,\n",
    "        EXTRACT('month' FROM Order_Date) AS Month,\n",
    "        CASE\n",
    "            WHEN EXTRACT('month' FROM Order_Date) IN (12, 1, 2) THEN 'winter'\n",
    "            WHEN EXTRACT('month' FROM Order_Date) IN (3, 4, 5) THEN 'spring'\n",
    "            WHEN EXTRACT('month' FROM Order_Date) IN (6, 7, 8) THEN 'summer'\n",
    "            ELSE 'fall'\n",
    "        END AS Season,\n",
    "        CASE\n",
    "            WHEN rand_lines < 0.60 THEN 1\n",
    "            WHEN rand_lines < 0.90 THEN 2\n",
    "            WHEN rand_lines < 0.95 THEN 3\n",
    "            WHEN rand_lines < 0.96 THEN 4\n",
    "            ELSE 5\n",
    "        END AS Num_Lines,\n",
    "        CASE\n",
    "            WHEN rand_tod < 0.50 THEN 'Morning'\n",
    "            WHEN rand_tod < 0.80 THEN 'Afternoon'\n",
    "            ELSE 'Night'\n",
    "        END AS Time_Of_Day,\n",
    "        CASE\n",
    "            WHEN rand_loc < 0.30 THEN FLOOR(random() * 50) + 1\n",
    "            WHEN rand_loc < 0.80 THEN FLOOR(random() * 150) + 51\n",
    "            WHEN rand_loc < 0.95 THEN FLOOR(random() * 300) + 201\n",
    "            ELSE FLOOR(random() * 500) + 501\n",
    "        END AS Location_ID\n",
    "    FROM base;\n",
    "\n",
    "    ------------------------------------------------------------\n",
    "    -- Step 2: Explode orders by Num_Lines\n",
    "    CREATE OR REPLACE view exploded_orders AS\n",
    "    SELECT\n",
    "        b.*,\n",
    "        s.value AS Line_Val\n",
    "    FROM base_orders b\n",
    "    JOIN LATERAL generate_series(1, b.Num_Lines) s(value) ON TRUE;\n",
    "\n",
    "    -- Step 3: Add line-level randomness\n",
    "    CREATE OR REPLACE view final_data AS\n",
    "    SELECT\n",
    "        *,\n",
    "        Order_ID || '_' || CAST(Line_Val AS VARCHAR) AS Order_Line_ID,\n",
    "\n",
    "        -- Quantity\n",
    "        CASE\n",
    "            WHEN random() < 0.40 THEN 1\n",
    "            WHEN random() < 0.70 THEN 2\n",
    "            WHEN random() < 0.85 THEN 3\n",
    "            WHEN random() < 0.95 THEN 4\n",
    "            ELSE 5\n",
    "        END AS Quantity,\n",
    "\n",
    "        -- Discount Rate\n",
    "        CASE\n",
    "            WHEN random() < 0.80 THEN 0\n",
    "            ELSE FLOOR(random() * 15 + 1)\n",
    "        END AS Discount_Rate,\n",
    "\n",
    "        -- Product_ID distribution by season (cast indexes to BIGINT!)\n",
    "        CASE\n",
    "            WHEN Season = 'summer' THEN\n",
    "                CASE\n",
    "                    WHEN random() < 0.40 THEN (CASE FLOOR(random() * 2) WHEN 0 THEN 5 ELSE 6 END)\n",
    "                    WHEN random() < 0.90 THEN (ARRAY[1,2,3,4,7,8,9,10])[CAST(FLOOR(random() * 8) + 1 AS BIGINT)]\n",
    "                    ELSE (ARRAY[11,12,13])[CAST(FLOOR(random() * 3) + 1 AS BIGINT)]\n",
    "                END\n",
    "            ELSE\n",
    "                CASE\n",
    "                    WHEN random() < 0.70 THEN (ARRAY[1,2,3,4,7,8,9,10])[CAST(FLOOR(random() * 8) + 1 AS BIGINT)]\n",
    "                    WHEN random() < 0.80 THEN (CASE FLOOR(random() * 2) WHEN 0 THEN 5 ELSE 6 END)\n",
    "                    ELSE (ARRAY[11,12,13])[CAST(FLOOR(random() * 3) + 1 AS BIGINT)]\n",
    "                END\n",
    "        END AS Product_ID\n",
    "    FROM exploded_orders;\n",
    "\n",
    "    -- Step 4: final Data\n",
    "    CREATE table if not exists fact_sales as\n",
    "    SELECT\n",
    "    a.Order_ID ,\n",
    "    a.order_line_id ,\n",
    "    a.order_date ,\n",
    "    a.time_Of_day ,\n",
    "    a.season ,\n",
    "    b.location_id ,\n",
    "    c.name AS product_name ,\n",
    "    a.quantity ,\n",
    "    (c.standard_price * ((100-discount_rate)/100)) * a.Quantity AS sales_amount ,\n",
    "    a.discount_rate AS discount_percentage\n",
    "    FROM final_data AS a\n",
    "    LEFT JOIN dim_locations AS b ON (a.Location_ID = b.record_id)\n",
    "    LEFT JOIN dim_products  AS c ON (a.Product_ID = c.product_id AND a.Order_Date BETWEEN c.from_date AND c.to_date) ;\n",
    "    \"\"\")\n",
    "    con.close()\n",
    "    !pip install -q ducklake-delta-exporter\n",
    "    from ducklake_delta_exporter import generate_latest_delta_log\n",
    "    generate_latest_delta_log('/lakehouse/default/Files/coffe_meta.db')\n",
    "else :\n",
    "    print(\"data exists already\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b6de18-3030-4e6b-892d-f3bbec2ad186",
   "metadata": {
    "id": "pg7xmUukBEUi"
   },
   "source": [
    "# ***Query Data***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e8507-4fc6-4d20-b2a1-1dfb7fb62663",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "**<mark>SQL</mark>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f086875-fa27-48ed-9d93-ec2ea90e5867",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "sql =\"\"\"\n",
    "-- 1) Calculate total daily sales for each city and a 7-day rolling average.\n",
    "SELECT\n",
    "    f.order_date,\n",
    "    l.city,\n",
    "    SUM(f.sales_amount) AS total_sales,\n",
    "    AVG(SUM(f.sales_amount)) OVER (\n",
    "        PARTITION BY l.city\n",
    "        ORDER BY f.order_date\n",
    "        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "    ) AS rolling_7day_avg\n",
    "FROM fact_sales f\n",
    "JOIN dim_locations l\n",
    "    ON f.location_id = l.location_id\n",
    "GROUP BY\n",
    "    f.order_date,\n",
    "    l.city\n",
    "ORDER BY\n",
    "    l.city,\n",
    "    f.order_date;\n",
    "\n",
    "\n",
    "\n",
    "-- 2) For each month, rank products by total sales amount, with 1 being the highest.\n",
    "WITH monthly_sales AS (\n",
    "    SELECT\n",
    "        DATE_TRUNC('month', f.order_date) AS sales_month,\n",
    "        f.product_name,\n",
    "        SUM(f.sales_amount) AS total_sales\n",
    "    FROM fact_sales f\n",
    "    GROUP BY\n",
    "        DATE_TRUNC('month', f.order_date),\n",
    "        f.product_name\n",
    ")\n",
    "SELECT\n",
    "    sales_month,\n",
    "    product_name,\n",
    "    total_sales,\n",
    "    RANK() OVER (PARTITION BY sales_month ORDER BY total_sales DESC) AS sales_rank\n",
    "FROM monthly_sales\n",
    "ORDER BY sales_month, sales_rank;\n",
    "\n",
    "\n",
    "\n",
    "-- 3) Find the locations in each season with the highest average discount, limited to top 3.\n",
    "WITH season_discount AS (\n",
    "    SELECT\n",
    "        l.city,\n",
    "        l.state,\n",
    "        f.season,\n",
    "        AVG(f.discount_percentage) AS avg_discount\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_locations l\n",
    "        ON f.location_id = l.location_id\n",
    "    GROUP BY\n",
    "        l.city,\n",
    "        l.state,\n",
    "        f.season\n",
    ")\n",
    "SELECT\n",
    "    city,\n",
    "    state,\n",
    "    season,\n",
    "    avg_discount,\n",
    "    discount_rank\n",
    "FROM (\n",
    "    SELECT\n",
    "        city,\n",
    "        state,\n",
    "        season,\n",
    "        avg_discount,\n",
    "        DENSE_RANK() OVER (PARTITION BY season ORDER BY avg_discount DESC) AS discount_rank\n",
    "    FROM season_discount\n",
    ") t\n",
    "WHERE discount_rank <= 3\n",
    "ORDER BY season, discount_rank;\n",
    "\n",
    "\n",
    "\n",
    "-- 4) Compare actual daily sales to standard_price and standard_cost, to show total margin.\n",
    "--    Join on product_name and date range.\n",
    "SELECT\n",
    "    f.order_date,\n",
    "    f.product_name,\n",
    "    p.standard_price,\n",
    "    p.standard_cost,\n",
    "    SUM(f.quantity) AS total_quantity_sold,\n",
    "    SUM(f.sales_amount) AS total_sales_amount,\n",
    "    (p.standard_price - p.standard_cost) * SUM(f.quantity) AS theoretical_margin\n",
    "FROM fact_sales f\n",
    "JOIN dim_products p\n",
    "    ON f.product_name = p.name\n",
    "    AND f.order_date BETWEEN p.from_date AND p.to_date\n",
    "GROUP BY\n",
    "    f.order_date,\n",
    "    f.product_name,\n",
    "    p.standard_price,\n",
    "    p.standard_cost\n",
    "ORDER BY\n",
    "    f.order_date,\n",
    "    f.product_name;\n",
    "\n",
    "\n",
    "\n",
    "-- 5) Use a window function to calculate a 30-day rolling total quantity sold per city.\n",
    "WITH daily_city_qty AS (\n",
    "    SELECT\n",
    "        f.order_date,\n",
    "        l.city,\n",
    "        SUM(f.quantity) AS daily_qty\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_locations l\n",
    "        ON f.location_id = l.location_id\n",
    "    GROUP BY\n",
    "        f.order_date,\n",
    "        l.city\n",
    ")\n",
    "SELECT\n",
    "    order_date,\n",
    "    city,\n",
    "    daily_qty,\n",
    "    SUM(daily_qty) OVER (\n",
    "        PARTITION BY city\n",
    "        ORDER BY order_date\n",
    "        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW\n",
    "    ) AS rolling_30day_qty\n",
    "FROM daily_city_qty\n",
    "ORDER BY city, order_date;\n",
    "\n",
    "\n",
    "\n",
    "-- 6) Create or replace a table that stores monthly revenue by product category.\n",
    "WITH monthly_cat AS (\n",
    "    SELECT\n",
    "        DATE_TRUNC('month', f.order_date) AS sales_month,\n",
    "        p.category,\n",
    "        SUM(f.sales_amount) AS monthly_revenue\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_products p\n",
    "        ON f.product_name = p.name\n",
    "        AND f.order_date BETWEEN p.from_date AND p.to_date\n",
    "    GROUP BY\n",
    "        DATE_TRUNC('month', f.order_date),\n",
    "        p.category\n",
    ")\n",
    "SELECT\n",
    "    sales_month,\n",
    "    category,\n",
    "    monthly_revenue\n",
    "FROM monthly_cat;\n",
    "\n",
    "\n",
    "\n",
    "-- 7) Compare total sales by location in 2023 vs. 2024.\n",
    "WITH yearly_sales AS (\n",
    "    SELECT\n",
    "        l.location_id,\n",
    "        l.city,\n",
    "        l.state,\n",
    "        YEAR(f.order_date) AS sales_year,\n",
    "        SUM(f.sales_amount) AS total_sales_year\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_locations l\n",
    "        ON f.location_id = l.location_id\n",
    "    GROUP BY\n",
    "        l.location_id,\n",
    "        l.city,\n",
    "        l.state,\n",
    "        YEAR(f.order_date)\n",
    ")\n",
    "SELECT\n",
    "    city,\n",
    "    state,\n",
    "    SUM(CASE WHEN sales_year = 2023 THEN total_sales_year ELSE 0 END) AS sales_2023,\n",
    "    SUM(CASE WHEN sales_year = 2024 THEN total_sales_year ELSE 0 END) AS sales_2024,\n",
    "    (SUM(CASE WHEN sales_year = 2024 THEN total_sales_year ELSE 0 END)\n",
    "     - SUM(CASE WHEN sales_year = 2023 THEN total_sales_year ELSE 0 END)) AS yoy_diff\n",
    "FROM yearly_sales\n",
    "GROUP BY\n",
    "    city,\n",
    "    state\n",
    "ORDER BY\n",
    "    city,\n",
    "    state;\n",
    "\n",
    "\n",
    "\n",
    "-- 8) For each city and quarter, rank subcategories by total sales amount.\n",
    "WITH city_quarter_subcat AS (\n",
    "    SELECT\n",
    "        l.city,\n",
    "        DATE_TRUNC('quarter', f.order_date) AS sales_quarter,\n",
    "        p.subcategory,\n",
    "        SUM(f.sales_amount) AS total_sales\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_locations l\n",
    "        ON f.location_id = l.location_id\n",
    "    JOIN   dim_products p\n",
    "        ON f.product_name = p.name\n",
    "        AND f.order_date BETWEEN p.from_date AND p.to_date\n",
    "    GROUP BY\n",
    "        l.city,\n",
    "        DATE_TRUNC('quarter', f.order_date),\n",
    "        p.subcategory\n",
    ")\n",
    "SELECT\n",
    "    city,\n",
    "    sales_quarter,\n",
    "    subcategory,\n",
    "    total_sales,\n",
    "    RANK() OVER (PARTITION BY city, sales_quarter ORDER BY total_sales DESC) AS subcat_rank\n",
    "FROM city_quarter_subcat\n",
    "ORDER BY city, sales_quarter, subcat_rank;\n",
    "\n",
    "\n",
    "\n",
    "-- 9) Show average discount by day, and a running cumulative average discount per city.\n",
    "WITH daily_discount AS (\n",
    "    SELECT\n",
    "        l.city,\n",
    "        f.order_date,\n",
    "        AVG(f.discount_percentage) AS avg_discount\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_locations l\n",
    "        ON f.location_id = l.location_id\n",
    "    GROUP BY\n",
    "        l.city,\n",
    "        f.order_date\n",
    ")\n",
    "SELECT\n",
    "    city,\n",
    "    order_date,\n",
    "    avg_discount,\n",
    "    AVG(avg_discount) OVER (\n",
    "        PARTITION BY city\n",
    "        ORDER BY order_date\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "    ) AS cumulative_avg_discount\n",
    "FROM daily_discount\n",
    "ORDER BY city, order_date;\n",
    "\n",
    "\n",
    "\n",
    "-- 10) 90-day rolling count of distinct orders in each city.\n",
    "WITH daily_orders AS (\n",
    "    SELECT\n",
    "        f.order_date,\n",
    "        l.city,\n",
    "        COUNT(DISTINCT f.order_id) AS daily_distinct_orders\n",
    "    FROM fact_sales f\n",
    "    JOIN dim_locations l\n",
    "        ON f.location_id = l.location_id\n",
    "    GROUP BY\n",
    "        f.order_date,\n",
    "        l.city\n",
    ")\n",
    "SELECT\n",
    "    order_date,\n",
    "    city,\n",
    "    daily_distinct_orders,\n",
    "    SUM(daily_distinct_orders) OVER (\n",
    "        PARTITION BY city\n",
    "        ORDER BY order_date\n",
    "        ROWS BETWEEN 89 PRECEDING AND CURRENT ROW\n",
    "    ) AS rolling_90d_distinct_orders\n",
    "FROM daily_orders\n",
    "ORDER BY city, order_date;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578a857-6f6e-4b9e-9596-5e1d03c3fe9c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def execute_query(engine, sql_script, exclude_list):\n",
    "    results = []\n",
    "    sql_arr = sql_script.split(\";\")\n",
    "    \n",
    "    for index, value in enumerate(sql_arr, start=1):\n",
    "        if index not in exclude_list:\n",
    "            if len(value.strip()) > 0:\n",
    "                start = time.time()\n",
    "                print('query' + str(index))\n",
    "                try:\n",
    "                    engine.sql(value).show()\n",
    "                    stop = time.time()\n",
    "                    duration = stop - start\n",
    "                except Exception as er:\n",
    "                    print(er)\n",
    "                    duration = float('nan')\n",
    "                print(duration)\n",
    "                results.append({'dur': duration, 'query': index})\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e30f9-3dc1-4e55-868f-67834245c3d0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "con.sql(f\"\"\"\n",
    "    SET temp_directory=   '/lakehouse/default/Files/tmp' ;\n",
    "    CREATE or replace SECRET onelake ( TYPE AZURE, PROVIDER ACCESS_TOKEN, ACCESS_TOKEN '{notebookutils.credentials.getToken('storage')}')   ;\n",
    "    ATTACH or replace 'ducklake:{db_path}' AS db (DATA_PATH 'abfss://{ws}@onelake.dfs.fabric.microsoft.com/{lh}.Lakehouse/Tables');\n",
    "    USE db ;\n",
    "    create schema if not exists coffee{num_rows} ;\n",
    "    USE coffee{num_rows} ;\n",
    "    \"\"\")\n",
    "df = execute_query(con,sql,[])\n",
    "df['Engine']            =  'duckdb'\n",
    "df['time']              =  datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "df['sf']                =  num_rows\n",
    "df['cpu']               =  core\n",
    "df['test']              = 'coffee'\n",
    "write_deltalake(results,df,mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d482e-0afa-4d2c-a33c-c41259b97e0b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "con.sql(\"select count(*) from fact_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16784401-b150-4555-88b9-b11d9ace32ca",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680edc51-648a-4c2a-8c73-081313f5ffef",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65eaa5-1eef-44f1-a7c9-22fdd6980a37",
   "metadata": {
    "jupyter": {
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "duckdb.sql(f\"\"\" with xxx as (\n",
    "     select time,sf,cpu,sum(dur) as duration,list(round(dur,1) order by query) as values, from delta_scan('{results}') where sf =2000000000  group by all)\n",
    "     select time,sf,cpu,round(duration,1) as dur,round((duration/3600) * (cpu/2) * 0.1075,2) as cost,values  from xxx \"\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "97f5a0f5-be6d-4f25-932d-2a0ec29e82dc",
    "default_lakehouse_name": "coffee",
    "default_lakehouse_workspace_id": "7fda7465-a2bb-4f49-9518-87182e59bcf7",
    "known_lakehouses": [
     {
      "id": "97f5a0f5-be6d-4f25-932d-2a0ec29e82dc"
     }
    ]
   }
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "720000"
    }
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "35e453857d614ed0b8ec6a45d70ee501": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "3e19bc4141af468aba83e6f53f012d41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_35e453857d614ed0b8ec6a45d70ee501",
       "style": "IPY_MODEL_c69b1860fdf248119f29746c6e32b060",
       "value": 100
      }
     },
     "c69b1860fdf248119f29746c6e32b060": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "bar_color": "black",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
